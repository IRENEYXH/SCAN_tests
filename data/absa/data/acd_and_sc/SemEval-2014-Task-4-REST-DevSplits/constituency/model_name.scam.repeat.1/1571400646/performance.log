acd_and_sc_train_templates_pytorch.py-88-2021-03-17 22:11:20,197-len_count_list: [[2, 16], [3, 37], [4, 75], [5, 119], [6, 172], [7, 167], [8, 178], [9, 192], [10, 207], [11, 218], [12, 205], [13, 203], [14, 191], [15, 170], [16, 163], [17, 169], [18, 136], [19, 149], [20, 115], [21, 94], [22, 99], [23, 86], [24, 79], [25, 74], [26, 67], [27, 57], [28, 48], [29, 35], [30, 47], [31, 37], [32, 22], [33, 23], [34, 25], [35, 11], [36, 19], [37, 11], [38, 7], [39, 9], [40, 10], [41, 6], [42, 8], [43, 5], [44, 5], [45, 3], [46, 2], [47, 6], [48, 3], [49, 4], [50, 3], [52, 1], [53, 1], [54, 1], [56, 1], [57, 3], [60, 1], [61, 1], [66, 1], [69, 1], [70, 1], [79, 1]]
acd_and_sc_train_templates_pytorch.py-88-2021-03-18 00:06:55,924-len_count_list: [[2, 16], [3, 37], [4, 75], [5, 119], [6, 172], [7, 167], [8, 178], [9, 192], [10, 207], [11, 218], [12, 205], [13, 203], [14, 191], [15, 170], [16, 163], [17, 169], [18, 136], [19, 149], [20, 115], [21, 94], [22, 99], [23, 86], [24, 79], [25, 74], [26, 67], [27, 57], [28, 48], [29, 35], [30, 47], [31, 37], [32, 22], [33, 23], [34, 25], [35, 11], [36, 19], [37, 11], [38, 7], [39, 9], [40, 10], [41, 6], [42, 8], [43, 5], [44, 5], [45, 3], [46, 2], [47, 6], [48, 3], [49, 4], [50, 3], [52, 1], [53, 1], [54, 1], [56, 1], [57, 3], [60, 1], [61, 1], [66, 1], [69, 1], [70, 1], [79, 1]]
acd_and_sc_train_templates_pytorch.py-88-2021-03-18 00:17:39,436-len_count_list: [[2, 16], [3, 37], [4, 75], [5, 119], [6, 172], [7, 167], [8, 178], [9, 192], [10, 207], [11, 218], [12, 205], [13, 203], [14, 191], [15, 170], [16, 163], [17, 169], [18, 136], [19, 149], [20, 115], [21, 94], [22, 99], [23, 86], [24, 79], [25, 74], [26, 67], [27, 57], [28, 48], [29, 35], [30, 47], [31, 37], [32, 22], [33, 23], [34, 25], [35, 11], [36, 19], [37, 11], [38, 7], [39, 9], [40, 10], [41, 6], [42, 8], [43, 5], [44, 5], [45, 3], [46, 2], [47, 6], [48, 3], [49, 4], [50, 3], [52, 1], [53, 1], [54, 1], [56, 1], [57, 3], [60, 1], [61, 1], [66, 1], [69, 1], [70, 1], [79, 1]]
acd_and_sc_train_templates_pytorch.py-191-2021-03-18 00:17:51,069-n_trainable_params: 3160592, n_nontrainable_params: 1507800
acd_and_sc_train_templates_pytorch.py-192-2021-03-18 00:17:51,069-> training arguments:
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,069->>> current_dataset: SemEval-2014-Task-4-REST-DevSplits
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,069->>> hard_test: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,069->>> task_name: acd_and_sc
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,069->>> data_type: constituency
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,069->>> model_name: scam
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,069->>> timestamp: 1571400646
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,069->>> train: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,070->>> evaluate: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,070->>> error_analysis: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,070->>> predict: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,070->>> epochs: 100
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,070->>> batch_size: 32
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,070->>> patience: 10
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,070->>> visualize_attention: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,070->>> embedding_filepath: glove.840B.300d.txt
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,070->>> embed_size: 300
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,070->>> seed: 776
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,070->>> repeat: 1
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,070->>> device: cpu
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,070->>> gpu_id: 0
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,071->>> lstm_layer_category_classifier: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,071->>> position: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,071->>> aspect_position: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,071->>> position_embeddings_dim: 64
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,071->>> only_acd: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,071->>> only_sc: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,071->>> debug: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,071->>> early_stopping_by_batch: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,071->>> share_sentiment_classifier: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,071->>> attention_lamda: 1
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,071->>> sparse_reg: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,071->>> orthogonal_reg: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,071->>> early_stopping: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,071->>> bert: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,072->>> only_bert: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,072->>> concat_cls_vector: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,072->>> pair: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,072->>> fixed: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,072->>> max_len: 128
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,072->>> lstm_layer_num_in_bert: 0
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,072->>> dropout_in_bert: 0.5
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,072->>> learning_rate_in_bert: 2e-05
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,072->>> l2_in_bert: 1e-05
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,072->>> acd_init_weight: 1
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,072->>> bilstm_hidden_size_in_bert: 0
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,072->>> dropout_after_cls: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,072->>> acd_sc_mode: multi-multi
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,072->>> joint_type: joint
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,073->>> acd_warmup: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,073->>> acd_warmup_epochs: 100
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,073->>> acd_warmup_patience: 10
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,073->>> pipeline: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,073->>> pipeline_with_acd: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,073->>> lstm_or_fc_after_embedding_layer: lstm
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,073->>> lstm_layer_num_in_lstm: 3
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,073->>> sentence_encoder_for_sentiment: bilstm
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,073->>> bert_file_path: bert-base-uncased.tar.gz
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,073->>> bert_vocab_file_path: Dvocab.txt
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,073->>> savefig_dir: 
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,073->>> frozen_all_acsc_parameter_while_pretrain_acd: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,073->>> interactive_loss_lamda: 1
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,073->>> attention_warmup_init: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,073->>> attention_warmup: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,074->>> acd_encoder_mode_for_sentiment_attention_warmup: same
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,074->>> gnn_for_sentiment_attention_warmup: gat
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,074->>> acd_sc_encoder_mode: same
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,074->>> acd_encoder_mode: mixed
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,074->>> sentiment_encoder_with_own_gnn: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,074->>> aspect_graph: gat
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,074->>> sentiment_graph: gat
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,074->>> aspect_graph_with_dotted_line: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,074->>> constituency_tree: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,074->>> performance_of_different_lengths: 30,40,50,60,100000
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,074->>> gat_visualization: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,074->>> model_name_complete: model_name.scam.repeat.1
acd_and_sc_train_templates_pytorch.py-580-2021-03-18 00:17:51,076-validation_metric: +accuracy
acd_and_sc_train_templates_pytorch.py-191-2021-03-18 00:17:51,080-n_trainable_params: 3160592, n_nontrainable_params: 1507800
acd_and_sc_train_templates_pytorch.py-192-2021-03-18 00:17:51,080-> training arguments:
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,080->>> current_dataset: SemEval-2014-Task-4-REST-DevSplits
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,080->>> hard_test: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,080->>> task_name: acd_and_sc
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,080->>> data_type: constituency
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,080->>> model_name: scam
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,080->>> timestamp: 1571400646
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,081->>> train: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,081->>> evaluate: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,081->>> error_analysis: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,081->>> predict: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,081->>> epochs: 100
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,081->>> batch_size: 32
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,081->>> patience: 10
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,081->>> visualize_attention: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,081->>> embedding_filepath: glove.840B.300d.txt
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,081->>> embed_size: 300
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,081->>> seed: 776
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,081->>> repeat: 1
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,081->>> device: cpu
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,081->>> gpu_id: 0
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,081->>> lstm_layer_category_classifier: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,081->>> position: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,082->>> aspect_position: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,082->>> position_embeddings_dim: 64
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,082->>> only_acd: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,082->>> only_sc: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,082->>> debug: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,082->>> early_stopping_by_batch: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,082->>> share_sentiment_classifier: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,082->>> attention_lamda: 1
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,082->>> sparse_reg: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,082->>> orthogonal_reg: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,082->>> early_stopping: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,082->>> bert: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,082->>> only_bert: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,082->>> concat_cls_vector: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,082->>> pair: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,082->>> fixed: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,083->>> max_len: 128
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,083->>> lstm_layer_num_in_bert: 0
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,083->>> dropout_in_bert: 0.5
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,083->>> learning_rate_in_bert: 2e-05
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,083->>> l2_in_bert: 1e-05
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,083->>> acd_init_weight: 1
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,083->>> bilstm_hidden_size_in_bert: 0
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,083->>> dropout_after_cls: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,083->>> acd_sc_mode: multi-multi
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,083->>> joint_type: joint
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,083->>> acd_warmup: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,083->>> acd_warmup_epochs: 100
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,083->>> acd_warmup_patience: 10
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,083->>> pipeline: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,083->>> pipeline_with_acd: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,083->>> lstm_or_fc_after_embedding_layer: lstm
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,083->>> lstm_layer_num_in_lstm: 3
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,084->>> sentence_encoder_for_sentiment: bilstm
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,084->>> bert_file_path: bert-base-uncased.tar.gz
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,084->>> bert_vocab_file_path: Dvocab.txt
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,084->>> savefig_dir: 
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,084->>> frozen_all_acsc_parameter_while_pretrain_acd: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,084->>> interactive_loss_lamda: 1
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,084->>> attention_warmup_init: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,084->>> attention_warmup: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,084->>> acd_encoder_mode_for_sentiment_attention_warmup: same
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,084->>> gnn_for_sentiment_attention_warmup: gat
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,084->>> acd_sc_encoder_mode: same
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,084->>> acd_encoder_mode: mixed
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,084->>> sentiment_encoder_with_own_gnn: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,084->>> aspect_graph: gat
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,084->>> sentiment_graph: gat
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,084->>> aspect_graph_with_dotted_line: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,085->>> constituency_tree: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,085->>> performance_of_different_lengths: 30,40,50,60,100000
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,085->>> gat_visualization: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 00:17:51,085->>> model_name_complete: model_name.scam.repeat.1
allennlp_callback.py-39-2021-03-18 00:21:59,853-epoch: 0 data_type: train result: {'sentiment_acc': 0.6656676656676657, 'category_f1': {'precision': 0.738514173998045, 'recall': 0.5031635031635031, 'fscore': 0.5985343632402456}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'anecdotes/miscellaneous': {'f1': 0.6238330587589237, 'precision': 0.64472190692395, 'recall': 0.6042553191489362}, 'food': {'f1': 0.8402309913378248, 'precision': 0.8128491620111732, 'recall': 0.8695219123505976}, 'price': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'service': {'f1': 0.25, 'precision': 0.7865168539325843, 'recall': 0.14861995753715498}}, 'acsc_metrics': {'ambience': {'acc': 0.78419452887538}, 'anecdotes/miscellaneous': {'acc': 0.4776595744680851}, 'food': {'acc': 0.7360557768924303}, 'price': {'acc': 0.7142857142857143}, 'service': {'acc': 0.7813163481953291}}, 'polarity_metrics': {'negative': {'f1': 0.5708812260536399, 'precision': 0.5234192037470726, 'recall': 0.6278089887640449}, 'neutral': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'positive': {'f1': 0.7746443723483903, 'precision': 0.7221963704048394, 'recall': 0.8353067814854682}}, 'merge_micro_f1': 0.3881956823133294}}
allennlp_callback.py-39-2021-03-18 00:22:17,085-epoch: 0 data_type: dev result: {'sentiment_acc': 0.6372745490981964, 'category_f1': {'precision': 0.5771144278606966, 'recall': 0.4649298597194389, 'fscore': 0.5149833518312986}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'anecdotes/miscellaneous': {'f1': 0.5676567656765676, 'precision': 0.5584415584415584, 'recall': 0.5771812080536913}, 'food': {'f1': 0.6997389033942559, 'precision': 0.600896860986547, 'recall': 0.8375}, 'price': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'service': {'f1': 0.20869565217391306, 'precision': 0.48, 'recall': 0.13333333333333333}}, 'acsc_metrics': {'ambience': {'acc': 0.7090909090909091}, 'anecdotes/miscellaneous': {'acc': 0.4899328859060403}, 'food': {'acc': 0.7}, 'price': {'acc': 0.6444444444444445}, 'service': {'acc': 0.7222222222222222}}, 'polarity_metrics': {'negative': {'f1': 0.5376344086021505, 'precision': 0.4934210526315789, 'recall': 0.5905511811023622}, 'neutral': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'positive': {'f1': 0.745398773006135, 'precision': 0.7002881844380403, 'recall': 0.7967213114754098}}, 'merge_micro_f1': 0.3285238623751388}}
allennlp_callback.py-39-2021-03-18 00:22:45,208-epoch: 0 data_type: test result: {'sentiment_acc': 0.7399794450154162, 'category_f1': {'precision': 0.7837837837837838, 'recall': 0.5364850976361768, 'fscore': 0.636973764490543}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.037037037037037035, 'precision': 0.6666666666666666, 'recall': 0.01904761904761905}, 'anecdotes/miscellaneous': {'f1': 0.5788336933045357, 'precision': 0.5491803278688525, 'recall': 0.6118721461187214}, 'food': {'f1': 0.8979591836734694, 'precision': 0.9214659685863874, 'recall': 0.8756218905472637}, 'price': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'service': {'f1': 0.3333333333333333, 'precision': 0.918918918918919, 'recall': 0.20359281437125748}}, 'acsc_metrics': {'ambience': {'acc': 0.8285714285714286}, 'anecdotes/miscellaneous': {'acc': 0.5707762557077626}, 'food': {'acc': 0.7661691542288557}, 'price': {'acc': 0.7875}, 'service': {'acc': 0.8203592814371258}}, 'polarity_metrics': {'negative': {'f1': 0.6163793103448276, 'precision': 0.5909090909090909, 'recall': 0.6441441441441441}, 'neutral': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'positive': {'f1': 0.8314121037463976, 'precision': 0.7893296853625171, 'recall': 0.878234398782344}}, 'merge_micro_f1': 0.4722391702257474}}
allennlp_callback.py-39-2021-03-18 00:22:46,574-epoch: 0 data_type: hard_test result: {'sentiment_acc': 0.5849056603773585, 'category_f1': {'precision': 0.9090909090909091, 'recall': 0.37735849056603776, 'fscore': 0.5333333333333334}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'anecdotes/miscellaneous': {'f1': 0.2222222222222222, 'precision': 0.3333333333333333, 'recall': 0.16666666666666666}, 'food': {'f1': 0.9268292682926829, 'precision': 1.0, 'recall': 0.8636363636363636}, 'price': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'service': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}}, 'acsc_metrics': {'ambience': {'acc': 0.625}, 'anecdotes/miscellaneous': {'acc': 0.3333333333333333}, 'food': {'acc': 0.45454545454545453}, 'price': {'acc': 0.8571428571428571}, 'service': {'acc': 0.8}}, 'polarity_metrics': {'negative': {'f1': 0.6956521739130435, 'precision': 0.6153846153846154, 'recall': 0.8}, 'neutral': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'positive': {'f1': 0.6250000000000001, 'precision': 0.5555555555555556, 'recall': 0.7142857142857143}}, 'merge_micro_f1': 0.24}}
allennlp_callback.py-39-2021-03-18 00:27:26,033-epoch: 1 data_type: train result: {'sentiment_acc': 0.7119547119547119, 'category_f1': {'precision': 0.8274087932647334, 'recall': 0.5890775890775891, 'fscore': 0.6881929585683719}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.11396011396011396, 'precision': 0.9090909090909091, 'recall': 0.060790273556231005}, 'anecdotes/miscellaneous': {'f1': 0.6480304955527318, 'precision': 0.804416403785489, 'recall': 0.5425531914893617}, 'food': {'f1': 0.860019175455417, 'precision': 0.8290203327171903, 'recall': 0.8934262948207171}, 'price': {'f1': 0.4139941690962099, 'precision': 0.8452380952380952, 'recall': 0.27413127413127414}, 'service': {'f1': 0.6886912325285897, 'precision': 0.8575949367088608, 'recall': 0.5753715498938429}}, 'acsc_metrics': {'ambience': {'acc': 0.7659574468085106}, 'anecdotes/miscellaneous': {'acc': 0.574468085106383}, 'food': {'acc': 0.7749003984063745}, 'price': {'acc': 0.749034749034749}, 'service': {'acc': 0.7940552016985138}}, 'polarity_metrics': {'negative': {'f1': 0.5472636815920399, 'precision': 0.6680161943319838, 'recall': 0.46348314606741575}, 'neutral': {'f1': 0.3463855421686747, 'precision': 0.49783549783549785, 'recall': 0.26558891454965355}, 'positive': {'f1': 0.8186653771760155, 'precision': 0.7431957857769974, 'recall': 0.9111948331539289}}, 'merge_micro_f1': 0.48784283213382607}}
allennlp_callback.py-39-2021-03-18 00:27:46,479-epoch: 1 data_type: dev result: {'sentiment_acc': 0.6813627254509018, 'category_f1': {'precision': 0.6232558139534884, 'recall': 0.5370741482965932, 'fscore': 0.5769644779332616}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.09999999999999999, 'precision': 0.6, 'recall': 0.05454545454545454}, 'anecdotes/miscellaneous': {'f1': 0.5905511811023622, 'precision': 0.7142857142857143, 'recall': 0.5033557046979866}, 'food': {'f1': 0.6929133858267716, 'precision': 0.5972850678733032, 'recall': 0.825}, 'price': {'f1': 0.34375, 'precision': 0.5789473684210527, 'recall': 0.24444444444444444}, 'service': {'f1': 0.5529411764705883, 'precision': 0.5875, 'recall': 0.5222222222222223}}, 'acsc_metrics': {'ambience': {'acc': 0.7818181818181819}, 'anecdotes/miscellaneous': {'acc': 0.5503355704697986}, 'food': {'acc': 0.7375}, 'price': {'acc': 0.7333333333333333}, 'service': {'acc': 0.7111111111111111}}, 'polarity_metrics': {'negative': {'f1': 0.514018691588785, 'precision': 0.632183908045977, 'recall': 0.4330708661417323}, 'neutral': {'f1': 0.3018867924528301, 'precision': 0.41025641025641024, 'recall': 0.23880597014925373}, 'positive': {'f1': 0.7935103244837759, 'precision': 0.7211796246648794, 'recall': 0.8819672131147541}}, 'merge_micro_f1': 0.3832077502691066}}
allennlp_callback.py-39-2021-03-18 00:28:19,217-epoch: 1 data_type: test result: {'sentiment_acc': 0.7667009249743063, 'category_f1': {'precision': 0.8928571428571429, 'recall': 0.6423432682425488, 'fscore': 0.7471607890017933}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.15789473684210528, 'precision': 1.0, 'recall': 0.08571428571428572}, 'anecdotes/miscellaneous': {'f1': 0.6103542234332425, 'precision': 0.7567567567567568, 'recall': 0.5114155251141552}, 'food': {'f1': 0.9086357947434293, 'precision': 0.9143576826196473, 'recall': 0.9029850746268657}, 'price': {'f1': 0.4313725490196079, 'precision': 1.0, 'recall': 0.275}, 'service': {'f1': 0.8178694158075601, 'precision': 0.9596774193548387, 'recall': 0.7125748502994012}}, 'acsc_metrics': {'ambience': {'acc': 0.819047619047619}, 'anecdotes/miscellaneous': {'acc': 0.6255707762557078}, 'food': {'acc': 0.7835820895522388}, 'price': {'acc': 0.85}, 'service': {'acc': 0.8383233532934131}}, 'polarity_metrics': {'negative': {'f1': 0.6038781163434902, 'precision': 0.7841726618705036, 'recall': 0.49099099099099097}, 'neutral': {'f1': 0.2702702702702703, 'precision': 0.37037037037037035, 'recall': 0.2127659574468085}, 'positive': {'f1': 0.8587334725121782, 'precision': 0.791025641025641, 'recall': 0.939117199391172}}, 'merge_micro_f1': 0.56664674237896}}
allennlp_callback.py-39-2021-03-18 00:28:20,888-epoch: 1 data_type: hard_test result: {'sentiment_acc': 0.5283018867924528, 'category_f1': {'precision': 0.9375, 'recall': 0.5660377358490566, 'fscore': 0.7058823529411765}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.9523809523809523, 'precision': 1.0, 'recall': 0.9090909090909091}, 'price': {'f1': 0.7272727272727273, 'precision': 1.0, 'recall': 0.5714285714285714}, 'service': {'f1': 0.7058823529411764, 'precision': 0.8571428571428571, 'recall': 0.6}}, 'acsc_metrics': {'ambience': {'acc': 0.375}, 'anecdotes/miscellaneous': {'acc': 0.3333333333333333}, 'food': {'acc': 0.5}, 'price': {'acc': 0.8571428571428571}, 'service': {'acc': 0.6}}, 'polarity_metrics': {'negative': {'f1': 0.5555555555555556, 'precision': 0.625, 'recall': 0.5}, 'neutral': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'positive': {'f1': 0.6206896551724138, 'precision': 0.4864864864864865, 'recall': 0.8571428571428571}}, 'merge_micro_f1': 0.3764705882352941}}
allennlp_callback.py-39-2021-03-18 00:32:35,954-epoch: 2 data_type: train result: {'sentiment_acc': 0.7249417249417249, 'category_f1': {'precision': 0.8494863778472532, 'recall': 0.6333666333666333, 'fscore': 0.7256772224341853}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.6882067851373183, 'precision': 0.7344827586206897, 'recall': 0.6474164133738601}, 'anecdotes/miscellaneous': {'f1': 0.6877770740975301, 'precision': 0.8497652582159625, 'recall': 0.5776595744680851}, 'food': {'f1': 0.8043010752688172, 'precision': 0.8738317757009346, 'recall': 0.7450199203187251}, 'price': {'f1': 0.6115288220551378, 'precision': 0.8714285714285714, 'recall': 0.47104247104247104}, 'service': {'f1': 0.7031847133757961, 'precision': 0.8789808917197452, 'recall': 0.5859872611464968}}, 'acsc_metrics': {'ambience': {'acc': 0.7598784194528876}, 'anecdotes/miscellaneous': {'acc': 0.5861702127659575}, 'food': {'acc': 0.7938247011952191}, 'price': {'acc': 0.7644787644787645}, 'service': {'acc': 0.8089171974522293}}, 'polarity_metrics': {'negative': {'f1': 0.5781758957654723, 'precision': 0.687984496124031, 'recall': 0.49859550561797755}, 'neutral': {'f1': 0.2846975088967971, 'precision': 0.6201550387596899, 'recall': 0.18475750577367206}, 'positive': {'f1': 0.8263757115749525, 'precision': 0.7387616624257846, 'recall': 0.93756727664155}}, 'merge_micro_f1': 0.5345288057993132}}
allennlp_callback.py-39-2021-03-18 00:32:57,998-epoch: 2 data_type: dev result: {'sentiment_acc': 0.7074148296593187, 'category_f1': {'precision': 0.6629955947136564, 'recall': 0.6032064128256514, 'fscore': 0.6316894018887724}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.559322033898305, 'precision': 0.5238095238095238, 'recall': 0.6}, 'anecdotes/miscellaneous': {'f1': 0.6564885496183206, 'precision': 0.7610619469026548, 'recall': 0.5771812080536913}, 'food': {'f1': 0.6976744186046512, 'precision': 0.6521739130434783, 'recall': 0.75}, 'price': {'f1': 0.4507042253521127, 'precision': 0.6153846153846154, 'recall': 0.35555555555555557}, 'service': {'f1': 0.5822784810126582, 'precision': 0.6764705882352942, 'recall': 0.5111111111111111}}, 'acsc_metrics': {'ambience': {'acc': 0.8}, 'anecdotes/miscellaneous': {'acc': 0.5637583892617449}, 'food': {'acc': 0.7625}, 'price': {'acc': 0.8222222222222222}, 'service': {'acc': 0.7333333333333333}}, 'polarity_metrics': {'negative': {'f1': 0.5570776255707763, 'precision': 0.6630434782608695, 'recall': 0.48031496062992124}, 'neutral': {'f1': 0.2528735632183908, 'precision': 0.55, 'recall': 0.16417910447761194}, 'positive': {'f1': 0.8121387283236995, 'precision': 0.7260981912144703, 'recall': 0.921311475409836}}, 'merge_micro_f1': 0.45750262329485836}}
allennlp_callback.py-39-2021-03-18 00:33:27,586-epoch: 2 data_type: test result: {'sentiment_acc': 0.7841726618705036, 'category_f1': {'precision': 0.8980446927374302, 'recall': 0.6608427543679343, 'fscore': 0.7613972764949676}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.75, 'precision': 0.8275862068965517, 'recall': 0.6857142857142857}, 'anecdotes/miscellaneous': {'f1': 0.6301369863013699, 'precision': 0.7876712328767124, 'recall': 0.5251141552511416}, 'food': {'f1': 0.8425414364640883, 'precision': 0.9472049689440993, 'recall': 0.7587064676616916}, 'price': {'f1': 0.603448275862069, 'precision': 0.9722222222222222, 'recall': 0.4375}, 'service': {'f1': 0.7945205479452054, 'precision': 0.928, 'recall': 0.6946107784431138}}, 'acsc_metrics': {'ambience': {'acc': 0.819047619047619}, 'anecdotes/miscellaneous': {'acc': 0.680365296803653}, 'food': {'acc': 0.7985074626865671}, 'price': {'acc': 0.8}, 'service': {'acc': 0.8562874251497006}}, 'polarity_metrics': {'negative': {'f1': 0.6346666666666666, 'precision': 0.7777777777777778, 'recall': 0.536036036036036}, 'neutral': {'f1': 0.27868852459016397, 'precision': 0.6071428571428571, 'recall': 0.18085106382978725}, 'positive': {'f1': 0.865424430641822, 'precision': 0.7916666666666666, 'recall': 0.954337899543379}}, 'merge_micro_f1': 0.606275902901125}}
allennlp_callback.py-39-2021-03-18 00:33:29,578-epoch: 2 data_type: hard_test result: {'sentiment_acc': 0.4528301886792453, 'category_f1': {'precision': 0.9615384615384616, 'recall': 0.4716981132075472, 'fscore': 0.6329113924050632}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.4, 'precision': 1.0, 'recall': 0.25}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.7428571428571429, 'precision': 1.0, 'recall': 0.5909090909090909}, 'price': {'f1': 0.8333333333333333, 'precision': 1.0, 'recall': 0.7142857142857143}, 'service': {'f1': 0.6666666666666666, 'precision': 1.0, 'recall': 0.5}}, 'acsc_metrics': {'ambience': {'acc': 0.375}, 'anecdotes/miscellaneous': {'acc': 0.3333333333333333}, 'food': {'acc': 0.5}, 'price': {'acc': 0.7142857142857143}, 'service': {'acc': 0.3}}, 'polarity_metrics': {'negative': {'f1': 0.37499999999999994, 'precision': 0.5, 'recall': 0.3}, 'neutral': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'positive': {'f1': 0.5806451612903225, 'precision': 0.43902439024390244, 'recall': 0.8571428571428571}}, 'merge_micro_f1': 0.30379746835443033}}
allennlp_callback.py-39-2021-03-18 00:37:55,316-epoch: 3 data_type: train result: {'sentiment_acc': 0.7715617715617715, 'category_f1': {'precision': 0.8229901269393513, 'recall': 0.7772227772227772, 'fscore': 0.7994519609522178}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.6837294332723949, 'precision': 0.8577981651376146, 'recall': 0.5683890577507599}, 'anecdotes/miscellaneous': {'f1': 0.7624434389140272, 'precision': 0.8140096618357487, 'recall': 0.7170212765957447}, 'food': {'f1': 0.881651376146789, 'precision': 0.8171768707482994, 'recall': 0.9571713147410359}, 'price': {'f1': 0.8015122873345935, 'precision': 0.7851851851851852, 'recall': 0.8185328185328186}, 'service': {'f1': 0.7361963190184048, 'precision': 0.872093023255814, 'recall': 0.6369426751592356}}, 'acsc_metrics': {'ambience': {'acc': 0.8632218844984803}, 'anecdotes/miscellaneous': {'acc': 0.6361702127659574}, 'food': {'acc': 0.8316733067729084}, 'price': {'acc': 0.8223938223938224}, 'service': {'acc': 0.821656050955414}}, 'polarity_metrics': {'negative': {'f1': 0.7006211180124223, 'precision': 0.6280623608017817, 'recall': 0.7921348314606742}, 'neutral': {'f1': 0.5137614678899083, 'precision': 0.593939393939394, 'recall': 0.45265588914549654}, 'positive': {'f1': 0.8571428571428572, 'precision': 0.8771830985915493, 'recall': 0.8379978471474704}}, 'merge_micro_f1': 0.6288748073300222}}
allennlp_callback.py-39-2021-03-18 00:38:18,099-epoch: 3 data_type: dev result: {'sentiment_acc': 0.7434869739478958, 'category_f1': {'precision': 0.6202749140893471, 'recall': 0.7234468937875751, 'fscore': 0.6679000925069379}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.48543689320388356, 'precision': 0.5208333333333334, 'recall': 0.45454545454545453}, 'anecdotes/miscellaneous': {'f1': 0.7083333333333334, 'precision': 0.7338129496402878, 'recall': 0.6845637583892618}, 'food': {'f1': 0.7268292682926829, 'precision': 0.596, 'recall': 0.93125}, 'price': {'f1': 0.6415094339622641, 'precision': 0.5573770491803278, 'recall': 0.7555555555555555}, 'service': {'f1': 0.5862068965517241, 'precision': 0.6071428571428571, 'recall': 0.5666666666666667}}, 'acsc_metrics': {'ambience': {'acc': 0.8}, 'anecdotes/miscellaneous': {'acc': 0.610738255033557}, 'food': {'acc': 0.81875}, 'price': {'acc': 0.8222222222222222}, 'service': {'acc': 0.7555555555555555}}, 'polarity_metrics': {'negative': {'f1': 0.7062937062937062, 'precision': 0.6352201257861635, 'recall': 0.7952755905511811}, 'neutral': {'f1': 0.4444444444444445, 'precision': 0.52, 'recall': 0.3880597014925373}, 'positive': {'f1': 0.8201680672268907, 'precision': 0.8413793103448276, 'recall': 0.8}}, 'merge_micro_f1': 0.5161887141535615}}
allennlp_callback.py-39-2021-03-18 00:38:50,526-epoch: 3 data_type: test result: {'sentiment_acc': 0.7862281603288798, 'category_f1': {'precision': 0.893348623853211, 'recall': 0.8006166495375129, 'fscore': 0.8444444444444446}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7011494252873562, 'precision': 0.8840579710144928, 'recall': 0.580952380952381}, 'anecdotes/miscellaneous': {'f1': 0.736318407960199, 'precision': 0.8087431693989071, 'recall': 0.6757990867579908}, 'food': {'f1': 0.9307411907654921, 'precision': 0.9097387173396675, 'recall': 0.9527363184079602}, 'price': {'f1': 0.8194444444444445, 'precision': 0.921875, 'recall': 0.7375}, 'service': {'f1': 0.847682119205298, 'precision': 0.9481481481481482, 'recall': 0.7664670658682635}}, 'acsc_metrics': {'ambience': {'acc': 0.8380952380952381}, 'anecdotes/miscellaneous': {'acc': 0.6621004566210046}, 'food': {'acc': 0.8084577114427861}, 'price': {'acc': 0.8125}, 'service': {'acc': 0.8502994011976048}}, 'polarity_metrics': {'negative': {'f1': 0.71900826446281, 'precision': 0.6641221374045801, 'recall': 0.7837837837837838}, 'neutral': {'f1': 0.4171779141104295, 'precision': 0.4927536231884058, 'recall': 0.3617021276595745}, 'positive': {'f1': 0.8575827559661279, 'precision': 0.867601246105919, 'recall': 0.84779299847793}}, 'merge_micro_f1': 0.6753387533875338}}
allennlp_callback.py-39-2021-03-18 00:38:51,979-epoch: 3 data_type: hard_test result: {'sentiment_acc': 0.6037735849056604, 'category_f1': {'precision': 0.9411764705882353, 'recall': 0.6037735849056604, 'fscore': 0.735632183908046}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.2222222222222222, 'precision': 1.0, 'recall': 0.125}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.9523809523809523, 'precision': 1.0, 'recall': 0.9090909090909091}, 'price': {'f1': 0.923076923076923, 'precision': 1.0, 'recall': 0.8571428571428571}, 'service': {'f1': 0.625, 'precision': 0.8333333333333334, 'recall': 0.5}}, 'acsc_metrics': {'ambience': {'acc': 0.75}, 'anecdotes/miscellaneous': {'acc': 0.16666666666666666}, 'food': {'acc': 0.45454545454545453}, 'price': {'acc': 0.8571428571428571}, 'service': {'acc': 0.9}}, 'polarity_metrics': {'negative': {'f1': 0.7083333333333333, 'precision': 0.6071428571428571, 'recall': 0.85}, 'neutral': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'positive': {'f1': 0.6521739130434783, 'precision': 0.6, 'recall': 0.7142857142857143}}, 'merge_micro_f1': 0.4597701149425288}}
allennlp_callback.py-39-2021-03-18 00:43:11,146-epoch: 4 data_type: train result: {'sentiment_acc': 0.8025308025308026, 'category_f1': {'precision': 0.8738567073170732, 'recall': 0.7635697635697636, 'fscore': 0.8149991114270483}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7339130434782608, 'precision': 0.8577235772357723, 'recall': 0.6413373860182371}, 'anecdotes/miscellaneous': {'f1': 0.7431023911710608, 'precision': 0.8769898697539797, 'recall': 0.6446808510638298}, 'food': {'f1': 0.8960396039603961, 'precision': 0.890748031496063, 'recall': 0.901394422310757}, 'price': {'f1': 0.8142292490118577, 'precision': 0.8340080971659919, 'recall': 0.7953667953667953}, 'service': {'f1': 0.8156424581005587, 'precision': 0.8608490566037735, 'recall': 0.7749469214437368}}, 'acsc_metrics': {'ambience': {'acc': 0.8966565349544073}, 'anecdotes/miscellaneous': {'acc': 0.6776595744680851}, 'food': {'acc': 0.8426294820717132}, 'price': {'acc': 0.8571428571428571}, 'service': {'acc': 0.8704883227176221}}, 'polarity_metrics': {'negative': {'f1': 0.7410832232496696, 'precision': 0.699501246882793, 'recall': 0.7879213483146067}, 'neutral': {'f1': 0.4984894259818731, 'precision': 0.7205240174672489, 'recall': 0.3810623556581986}, 'positive': {'f1': 0.8793733681462141, 'precision': 0.8539553752535497, 'recall': 0.9063509149623251}}, 'merge_micro_f1': 0.6724720099520171}}
allennlp_callback.py-39-2021-03-18 00:43:31,602-epoch: 4 data_type: dev result: {'sentiment_acc': 0.751503006012024, 'category_f1': {'precision': 0.6501831501831502, 'recall': 0.7114228456913828, 'fscore': 0.6794258373205742}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.509090909090909, 'precision': 0.509090909090909, 'recall': 0.509090909090909}, 'anecdotes/miscellaneous': {'f1': 0.7063197026022304, 'precision': 0.7916666666666666, 'recall': 0.6375838926174496}, 'food': {'f1': 0.7345844504021447, 'precision': 0.6431924882629108, 'recall': 0.85625}, 'price': {'f1': 0.6470588235294117, 'precision': 0.5789473684210527, 'recall': 0.7333333333333333}, 'service': {'f1': 0.649214659685864, 'precision': 0.6138613861386139, 'recall': 0.6888888888888889}}, 'acsc_metrics': {'ambience': {'acc': 0.8}, 'anecdotes/miscellaneous': {'acc': 0.6040268456375839}, 'food': {'acc': 0.825}, 'price': {'acc': 0.8222222222222222}, 'service': {'acc': 0.8}}, 'polarity_metrics': {'negative': {'f1': 0.6966292134831461, 'precision': 0.6642857142857143, 'recall': 0.7322834645669292}, 'neutral': {'f1': 0.411214953271028, 'precision': 0.55, 'recall': 0.3283582089552239}, 'positive': {'f1': 0.8333333333333334, 'precision': 0.8150470219435737, 'recall': 0.8524590163934426}}, 'merge_micro_f1': 0.5301435406698565}}
allennlp_callback.py-39-2021-03-18 00:44:02,689-epoch: 4 data_type: test result: {'sentiment_acc': 0.8139773895169579, 'category_f1': {'precision': 0.9062870699881376, 'recall': 0.7852004110996916, 'fscore': 0.8414096916299559}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7486631016042781, 'precision': 0.8536585365853658, 'recall': 0.6666666666666666}, 'anecdotes/miscellaneous': {'f1': 0.7113402061855669, 'precision': 0.8165680473372781, 'recall': 0.6301369863013698}, 'food': {'f1': 0.9142125480153649, 'precision': 0.941952506596306, 'recall': 0.8880597014925373}, 'price': {'f1': 0.8368794326241135, 'precision': 0.9672131147540983, 'recall': 0.7375}, 'service': {'f1': 0.877742946708464, 'precision': 0.9210526315789473, 'recall': 0.8383233532934131}}, 'acsc_metrics': {'ambience': {'acc': 0.8571428571428571}, 'anecdotes/miscellaneous': {'acc': 0.7031963470319634}, 'food': {'acc': 0.818407960199005}, 'price': {'acc': 0.85}, 'service': {'acc': 0.9041916167664671}}, 'polarity_metrics': {'negative': {'f1': 0.7500000000000001, 'precision': 0.7433628318584071, 'recall': 0.7567567567567568}, 'neutral': {'f1': 0.3857142857142858, 'precision': 0.5869565217391305, 'recall': 0.2872340425531915}, 'positive': {'f1': 0.8792341678939618, 'precision': 0.8516405135520685, 'recall': 0.908675799086758}}, 'merge_micro_f1': 0.7015418502202643}}
allennlp_callback.py-39-2021-03-18 00:44:04,954-epoch: 4 data_type: hard_test result: {'sentiment_acc': 0.660377358490566, 'category_f1': {'precision': 0.9393939393939394, 'recall': 0.5849056603773585, 'fscore': 0.7209302325581395}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.5454545454545454, 'precision': 1.0, 'recall': 0.375}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.8421052631578948, 'precision': 1.0, 'recall': 0.7272727272727273}, 'price': {'f1': 0.923076923076923, 'precision': 1.0, 'recall': 0.8571428571428571}, 'service': {'f1': 0.7058823529411764, 'precision': 0.8571428571428571, 'recall': 0.6}}, 'acsc_metrics': {'ambience': {'acc': 0.75}, 'anecdotes/miscellaneous': {'acc': 0.3333333333333333}, 'food': {'acc': 0.5}, 'price': {'acc': 1.0}, 'service': {'acc': 0.9}}, 'polarity_metrics': {'negative': {'f1': 0.75, 'precision': 0.6428571428571429, 'recall': 0.9}, 'neutral': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'positive': {'f1': 0.7391304347826089, 'precision': 0.68, 'recall': 0.8095238095238095}}, 'merge_micro_f1': 0.5348837209302327}}
allennlp_callback.py-39-2021-03-18 00:48:21,065-epoch: 5 data_type: train result: {'sentiment_acc': 0.8195138195138195, 'category_f1': {'precision': 0.8696483180428135, 'recall': 0.7575757575757576, 'fscore': 0.8097526250222461}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7993579454253611, 'precision': 0.8469387755102041, 'recall': 0.756838905775076}, 'anecdotes/miscellaneous': {'f1': 0.7332908975175048, 'precision': 0.9128367670364501, 'recall': 0.6127659574468085}, 'food': {'f1': 0.9091769157994324, 'precision': 0.8657657657657658, 'recall': 0.9571713147410359}, 'price': {'f1': 0.5425531914893617, 'precision': 0.8717948717948718, 'recall': 0.3938223938223938}, 'service': {'f1': 0.8278074866310161, 'precision': 0.834051724137931, 'recall': 0.821656050955414}}, 'acsc_metrics': {'ambience': {'acc': 0.878419452887538}, 'anecdotes/miscellaneous': {'acc': 0.7021276595744681}, 'food': {'acc': 0.848605577689243}, 'price': {'acc': 0.9073359073359073}, 'service': {'acc': 0.9023354564755839}}, 'polarity_metrics': {'negative': {'f1': 0.7576894223555889, 'precision': 0.8132045088566827, 'recall': 0.7092696629213483}, 'neutral': {'f1': 0.590344827586207, 'precision': 0.7328767123287672, 'recall': 0.4942263279445728}, 'positive': {'f1': 0.8824721377912867, 'precision': 0.8334928229665072, 'recall': 0.93756727664155}}, 'merge_micro_f1': 0.6801922050186866}}
allennlp_callback.py-39-2021-03-18 00:48:41,419-epoch: 5 data_type: dev result: {'sentiment_acc': 0.7454909819639278, 'category_f1': {'precision': 0.6366906474820144, 'recall': 0.7094188376753507, 'fscore': 0.671090047393365}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.6190476190476191, 'precision': 0.5492957746478874, 'recall': 0.7090909090909091}, 'anecdotes/miscellaneous': {'f1': 0.6796875000000001, 'precision': 0.8130841121495327, 'recall': 0.5838926174496645}, 'food': {'f1': 0.7189873417721518, 'precision': 0.6042553191489362, 'recall': 0.8875}, 'price': {'f1': 0.4864864864864865, 'precision': 0.6206896551724138, 'recall': 0.4}, 'service': {'f1': 0.6666666666666666, 'precision': 0.5964912280701754, 'recall': 0.7555555555555555}}, 'acsc_metrics': {'ambience': {'acc': 0.8}, 'anecdotes/miscellaneous': {'acc': 0.6375838926174496}, 'food': {'acc': 0.80625}, 'price': {'acc': 0.8}, 'service': {'acc': 0.7555555555555555}}, 'polarity_metrics': {'negative': {'f1': 0.6527196652719666, 'precision': 0.6964285714285714, 'recall': 0.6141732283464567}, 'neutral': {'f1': 0.4210526315789473, 'precision': 0.5106382978723404, 'recall': 0.3582089552238806}, 'positive': {'f1': 0.8372093023255813, 'precision': 0.7941176470588235, 'recall': 0.8852459016393442}}, 'merge_micro_f1': 0.5194312796208531}}
allennlp_callback.py-39-2021-03-18 00:49:11,211-epoch: 5 data_type: test result: {'sentiment_acc': 0.8047276464542652, 'category_f1': {'precision': 0.9081015719467956, 'recall': 0.7718396711202467, 'fscore': 0.8344444444444443}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.8, 'precision': 0.8421052631578947, 'recall': 0.7619047619047619}, 'anecdotes/miscellaneous': {'f1': 0.6799999999999999, 'precision': 0.9083969465648855, 'recall': 0.54337899543379}, 'food': {'f1': 0.9369592088998764, 'precision': 0.9312039312039312, 'recall': 0.9427860696517413}, 'price': {'f1': 0.4716981132075471, 'precision': 0.9615384615384616, 'recall': 0.3125}, 'service': {'f1': 0.8835820895522388, 'precision': 0.8809523809523809, 'recall': 0.8862275449101796}}, 'acsc_metrics': {'ambience': {'acc': 0.8380952380952381}, 'anecdotes/miscellaneous': {'acc': 0.7031963470319634}, 'food': {'acc': 0.8208955223880597}, 'price': {'acc': 0.85}, 'service': {'acc': 0.8562874251497006}}, 'polarity_metrics': {'negative': {'f1': 0.6984924623115578, 'precision': 0.7897727272727273, 'recall': 0.6261261261261262}, 'neutral': {'f1': 0.3835616438356164, 'precision': 0.5384615384615384, 'recall': 0.2978723404255319}, 'positive': {'f1': 0.8787446504992866, 'precision': 0.8268456375838926, 'recall': 0.9375951293759512}}, 'merge_micro_f1': 0.6877777777777778}}
allennlp_callback.py-39-2021-03-18 00:49:12,770-epoch: 5 data_type: hard_test result: {'sentiment_acc': 0.660377358490566, 'category_f1': {'precision': 0.9375, 'recall': 0.5660377358490566, 'fscore': 0.7058823529411765}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.5454545454545454, 'precision': 1.0, 'recall': 0.375}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.9268292682926829, 'precision': 1.0, 'recall': 0.8636363636363636}, 'price': {'f1': 0.4444444444444445, 'precision': 1.0, 'recall': 0.2857142857142857}, 'service': {'f1': 0.6666666666666665, 'precision': 0.75, 'recall': 0.6}}, 'acsc_metrics': {'ambience': {'acc': 0.75}, 'anecdotes/miscellaneous': {'acc': 0.3333333333333333}, 'food': {'acc': 0.5454545454545454}, 'price': {'acc': 1.0}, 'service': {'acc': 0.8}}, 'polarity_metrics': {'negative': {'f1': 0.7272727272727272, 'precision': 0.6666666666666666, 'recall': 0.8}, 'neutral': {'f1': 0.15384615384615385, 'precision': 1.0, 'recall': 0.08333333333333333}, 'positive': {'f1': 0.7346938775510204, 'precision': 0.6428571428571429, 'recall': 0.8571428571428571}}, 'merge_micro_f1': 0.49411764705882355}}
allennlp_callback.py-39-2021-03-18 00:53:40,653-epoch: 6 data_type: train result: {'sentiment_acc': 0.8404928404928405, 'category_f1': {'precision': 0.8507014028056112, 'recall': 0.8481518481518482, 'fscore': 0.8494247123561781}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.8088467614533964, 'precision': 0.8421052631578947, 'recall': 0.7781155015197568}, 'anecdotes/miscellaneous': {'f1': 0.8133764832793959, 'precision': 0.824945295404814, 'recall': 0.8021276595744681}, 'food': {'f1': 0.9145823501651722, 'precision': 0.8690582959641255, 'recall': 0.9651394422310757}, 'price': {'f1': 0.8048780487804877, 'precision': 0.8497854077253219, 'recall': 0.7644787644787645}, 'service': {'f1': 0.8231368186874305, 'precision': 0.8644859813084113, 'recall': 0.7855626326963907}}, 'acsc_metrics': {'ambience': {'acc': 0.9513677811550152}, 'anecdotes/miscellaneous': {'acc': 0.7244680851063829}, 'food': {'acc': 0.8635458167330677}, 'price': {'acc': 0.9034749034749034}, 'service': {'acc': 0.910828025477707}}, 'polarity_metrics': {'negative': {'f1': 0.7873684210526316, 'precision': 0.7868162692847125, 'recall': 0.7879213483146067}, 'neutral': {'f1': 0.6722488038277511, 'precision': 0.6972704714640199, 'recall': 0.648960739030023}, 'positive': {'f1': 0.89826435246996, 'precision': 0.8913619501854796, 'recall': 0.9052744886975242}}, 'merge_micro_f1': 0.71935967983992}}
allennlp_callback.py-39-2021-03-18 00:54:02,018-epoch: 6 data_type: dev result: {'sentiment_acc': 0.7555110220440882, 'category_f1': {'precision': 0.6060606060606061, 'recall': 0.7615230460921844, 'fscore': 0.6749555950266429}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.528, 'precision': 0.4714285714285714, 'recall': 0.6}, 'anecdotes/miscellaneous': {'f1': 0.7215189873417721, 'precision': 0.6826347305389222, 'recall': 0.7651006711409396}, 'food': {'f1': 0.7189873417721518, 'precision': 0.6042553191489362, 'recall': 0.8875}, 'price': {'f1': 0.625, 'precision': 0.5882352941176471, 'recall': 0.6666666666666666}, 'service': {'f1': 0.6288659793814434, 'precision': 0.5865384615384616, 'recall': 0.6777777777777778}}, 'acsc_metrics': {'ambience': {'acc': 0.8363636363636363}, 'anecdotes/miscellaneous': {'acc': 0.6375838926174496}, 'food': {'acc': 0.825}, 'price': {'acc': 0.8222222222222222}, 'service': {'acc': 0.7444444444444445}}, 'polarity_metrics': {'negative': {'f1': 0.669260700389105, 'precision': 0.6615384615384615, 'recall': 0.6771653543307087}, 'neutral': {'f1': 0.5038167938931298, 'precision': 0.515625, 'recall': 0.4925373134328358}, 'positive': {'f1': 0.8459016393442624, 'precision': 0.8459016393442623, 'recall': 0.8459016393442623}}, 'merge_micro_f1': 0.5204262877442274}}
allennlp_callback.py-39-2021-03-18 00:54:34,396-epoch: 6 data_type: test result: {'sentiment_acc': 0.7995889003083247, 'category_f1': {'precision': 0.864406779661017, 'recall': 0.8386433710174718, 'fscore': 0.8513302034428796}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7524752475247525, 'precision': 0.7835051546391752, 'recall': 0.7238095238095238}, 'anecdotes/miscellaneous': {'f1': 0.7528868360277137, 'precision': 0.7616822429906542, 'recall': 0.7442922374429224}, 'food': {'f1': 0.9333333333333335, 'precision': 0.9264705882352942, 'recall': 0.9402985074626866}, 'price': {'f1': 0.7887323943661972, 'precision': 0.9032258064516129, 'recall': 0.7}, 'service': {'f1': 0.8666666666666667, 'precision': 0.8773006134969326, 'recall': 0.8562874251497006}}, 'acsc_metrics': {'ambience': {'acc': 0.8285714285714286}, 'anecdotes/miscellaneous': {'acc': 0.6757990867579908}, 'food': {'acc': 0.8084577114427861}, 'price': {'acc': 0.8375}, 'service': {'acc': 0.9041916167664671}}, 'polarity_metrics': {'negative': {'f1': 0.728538283062645, 'precision': 0.7511961722488039, 'recall': 0.7072072072072072}, 'neutral': {'f1': 0.430939226519337, 'precision': 0.4482758620689655, 'recall': 0.4148936170212766}, 'positive': {'f1': 0.8725637181409294, 'precision': 0.8596750369276218, 'recall': 0.8858447488584474}}, 'merge_micro_f1': 0.6958789775691184}}
allennlp_callback.py-39-2021-03-18 00:54:36,466-epoch: 6 data_type: hard_test result: {'sentiment_acc': 0.660377358490566, 'category_f1': {'precision': 0.8717948717948718, 'recall': 0.6415094339622641, 'fscore': 0.7391304347826088}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.4, 'precision': 1.0, 'recall': 0.25}, 'anecdotes/miscellaneous': {'f1': 0.25, 'precision': 0.5, 'recall': 0.16666666666666666}, 'food': {'f1': 0.9268292682926829, 'precision': 1.0, 'recall': 0.8636363636363636}, 'price': {'f1': 0.8571428571428571, 'precision': 0.8571428571428571, 'recall': 0.8571428571428571}, 'service': {'f1': 0.631578947368421, 'precision': 0.6666666666666666, 'recall': 0.6}}, 'acsc_metrics': {'ambience': {'acc': 0.75}, 'anecdotes/miscellaneous': {'acc': 0.3333333333333333}, 'food': {'acc': 0.5}, 'price': {'acc': 1.0}, 'service': {'acc': 0.9}}, 'polarity_metrics': {'negative': {'f1': 0.7555555555555556, 'precision': 0.68, 'recall': 0.85}, 'neutral': {'f1': 0.15384615384615385, 'precision': 1.0, 'recall': 0.08333333333333333}, 'positive': {'f1': 0.7083333333333334, 'precision': 0.6296296296296297, 'recall': 0.8095238095238095}}, 'merge_micro_f1': 0.5217391304347827}}
allennlp_callback.py-39-2021-03-18 00:59:02,058-epoch: 7 data_type: train result: {'sentiment_acc': 0.8484848484848485, 'category_f1': {'precision': 0.8678001387925052, 'recall': 0.8328338328338328, 'fscore': 0.8499575191163977}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7928221859706363, 'precision': 0.8556338028169014, 'recall': 0.7386018237082067}, 'anecdotes/miscellaneous': {'f1': 0.833764888658726, 'precision': 0.8123107971745711, 'recall': 0.8563829787234043}, 'food': {'f1': 0.9034907597535935, 'precision': 0.9322033898305084, 'recall': 0.8764940239043825}, 'price': {'f1': 0.8130081300813008, 'precision': 0.8583690987124464, 'recall': 0.7722007722007722}, 'service': {'f1': 0.827968923418424, 'precision': 0.8674418604651163, 'recall': 0.7919320594479831}}, 'acsc_metrics': {'ambience': {'acc': 0.9544072948328267}, 'anecdotes/miscellaneous': {'acc': 0.7148936170212766}, 'food': {'acc': 0.8834661354581673}, 'price': {'acc': 0.9343629343629344}, 'service': {'acc': 0.9193205944798302}}, 'polarity_metrics': {'negative': {'f1': 0.7994907702100572, 'precision': 0.7310826542491269, 'recall': 0.8820224719101124}, 'neutral': {'f1': 0.60431654676259, 'precision': 0.8015267175572519, 'recall': 0.48498845265588914}, 'positive': {'f1': 0.9144385026737968, 'precision': 0.9086078639744952, 'recall': 0.9203444564047363}}, 'merge_micro_f1': 0.7337298215802889}}
allennlp_callback.py-39-2021-03-18 00:59:23,606-epoch: 7 data_type: dev result: {'sentiment_acc': 0.7474949899799599, 'category_f1': {'precision': 0.6153846153846154, 'recall': 0.7214428857715431, 'fscore': 0.6642066420664207}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.5289256198347108, 'precision': 0.48484848484848486, 'recall': 0.5818181818181818}, 'anecdotes/miscellaneous': {'f1': 0.7062499999999999, 'precision': 0.6608187134502924, 'recall': 0.7583892617449665}, 'food': {'f1': 0.6916426512968301, 'precision': 0.6417112299465241, 'recall': 0.75}, 'price': {'f1': 0.6666666666666666, 'precision': 0.5964912280701754, 'recall': 0.7555555555555555}, 'service': {'f1': 0.6288659793814434, 'precision': 0.5865384615384616, 'recall': 0.6777777777777778}}, 'acsc_metrics': {'ambience': {'acc': 0.8}, 'anecdotes/miscellaneous': {'acc': 0.6174496644295302}, 'food': {'acc': 0.80625}, 'price': {'acc': 0.8444444444444444}, 'service': {'acc': 0.7777777777777778}}, 'polarity_metrics': {'negative': {'f1': 0.703448275862069, 'precision': 0.6257668711656442, 'recall': 0.8031496062992126}, 'neutral': {'f1': 0.32, 'precision': 0.48484848484848486, 'recall': 0.23880597014925373}, 'positive': {'f1': 0.8388157894736842, 'precision': 0.8415841584158416, 'recall': 0.8360655737704918}}, 'merge_micro_f1': 0.518450184501845}}
allennlp_callback.py-39-2021-03-18 00:59:54,698-epoch: 7 data_type: test result: {'sentiment_acc': 0.8119218910585817, 'category_f1': {'precision': 0.8663646659116648, 'recall': 0.7862281603288798, 'fscore': 0.8243534482758621}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7411167512690355, 'precision': 0.7934782608695652, 'recall': 0.6952380952380952}, 'anecdotes/miscellaneous': {'f1': 0.7578475336322871, 'precision': 0.7444933920704846, 'recall': 0.771689497716895}, 'food': {'f1': 0.8874172185430464, 'precision': 0.9490084985835694, 'recall': 0.8333333333333334}, 'price': {'f1': 0.7555555555555555, 'precision': 0.9272727272727272, 'recall': 0.6375}, 'service': {'f1': 0.8482972136222909, 'precision': 0.8782051282051282, 'recall': 0.8203592814371258}}, 'acsc_metrics': {'ambience': {'acc': 0.8095238095238095}, 'anecdotes/miscellaneous': {'acc': 0.7214611872146118}, 'food': {'acc': 0.818407960199005}, 'price': {'acc': 0.85}, 'service': {'acc': 0.8982035928143712}}, 'polarity_metrics': {'negative': {'f1': 0.7242105263157894, 'precision': 0.6798418972332015, 'recall': 0.7747747747747747}, 'neutral': {'f1': 0.4028776978417266, 'precision': 0.6222222222222222, 'recall': 0.2978723404255319}, 'positive': {'f1': 0.8858858858858859, 'precision': 0.8740740740740741, 'recall': 0.898021308980213}}, 'merge_micro_f1': 0.689655172413793}}
allennlp_callback.py-39-2021-03-18 00:59:56,589-epoch: 7 data_type: hard_test result: {'sentiment_acc': 0.660377358490566, 'category_f1': {'precision': 0.9615384615384616, 'recall': 0.4716981132075472, 'fscore': 0.6329113924050632}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.2222222222222222, 'precision': 1.0, 'recall': 0.125}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.7777777777777778, 'precision': 1.0, 'recall': 0.6363636363636364}, 'price': {'f1': 0.7272727272727273, 'precision': 1.0, 'recall': 0.5714285714285714}, 'service': {'f1': 0.7058823529411764, 'precision': 0.8571428571428571, 'recall': 0.6}}, 'acsc_metrics': {'ambience': {'acc': 0.75}, 'anecdotes/miscellaneous': {'acc': 0.3333333333333333}, 'food': {'acc': 0.5}, 'price': {'acc': 1.0}, 'service': {'acc': 0.9}}, 'polarity_metrics': {'negative': {'f1': 0.7200000000000001, 'precision': 0.6, 'recall': 0.9}, 'neutral': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'positive': {'f1': 0.7906976744186046, 'precision': 0.7727272727272727, 'recall': 0.8095238095238095}}, 'merge_micro_f1': 0.4556962025316456}}
allennlp_callback.py-39-2021-03-18 01:04:13,943-epoch: 8 data_type: train result: {'sentiment_acc': 0.8871128871128872, 'category_f1': {'precision': 0.8942003514938489, 'recall': 0.8471528471528471, 'fscore': 0.8700410396716826}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.8341543513957308, 'precision': 0.9071428571428571, 'recall': 0.7720364741641338}, 'anecdotes/miscellaneous': {'f1': 0.8461538461538463, 'precision': 0.875, 'recall': 0.8191489361702128}, 'food': {'f1': 0.9394387001477105, 'precision': 0.928919182083739, 'recall': 0.950199203187251}, 'price': {'f1': 0.7420814479638008, 'precision': 0.8961748633879781, 'recall': 0.6332046332046332}, 'service': {'f1': 0.849894291754757, 'precision': 0.8463157894736842, 'recall': 0.8535031847133758}}, 'acsc_metrics': {'ambience': {'acc': 0.9726443768996961}, 'anecdotes/miscellaneous': {'acc': 0.798936170212766}, 'food': {'acc': 0.9103585657370518}, 'price': {'acc': 0.9613899613899614}, 'service': {'acc': 0.9129511677282378}}, 'polarity_metrics': {'negative': {'f1': 0.8477653631284917, 'precision': 0.8430555555555556, 'recall': 0.8525280898876404}, 'neutral': {'f1': 0.7475, 'precision': 0.8147138964577657, 'recall': 0.6905311778290993}, 'positive': {'f1': 0.9316375198728141, 'precision': 0.9175365344467641, 'recall': 0.946178686759957}}, 'merge_micro_f1': 0.774281805745554}}
allennlp_callback.py-39-2021-03-18 01:04:33,482-epoch: 8 data_type: dev result: {'sentiment_acc': 0.7835671342685371, 'category_f1': {'precision': 0.6156583629893239, 'recall': 0.6933867735470942, 'fscore': 0.6522148916116871}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.5, 'precision': 0.49122807017543857, 'recall': 0.509090909090909}, 'anecdotes/miscellaneous': {'f1': 0.7210884353741497, 'precision': 0.7310344827586207, 'recall': 0.7114093959731543}, 'food': {'f1': 0.6847826086956521, 'precision': 0.6057692307692307, 'recall': 0.7875}, 'price': {'f1': 0.5542168674698796, 'precision': 0.6052631578947368, 'recall': 0.5111111111111111}, 'service': {'f1': 0.6176470588235295, 'precision': 0.5526315789473685, 'recall': 0.7}}, 'acsc_metrics': {'ambience': {'acc': 0.8363636363636363}, 'anecdotes/miscellaneous': {'acc': 0.6912751677852349}, 'food': {'acc': 0.80625}, 'price': {'acc': 0.8888888888888888}, 'service': {'acc': 0.8111111111111111}}, 'polarity_metrics': {'negative': {'f1': 0.7265625, 'precision': 0.7209302325581395, 'recall': 0.7322834645669292}, 'neutral': {'f1': 0.528, 'precision': 0.5689655172413793, 'recall': 0.4925373134328358}, 'positive': {'f1': 0.8589951377633711, 'precision': 0.8493589743589743, 'recall': 0.8688524590163934}}, 'merge_micro_f1': 0.5146088595664468}}
allennlp_callback.py-39-2021-03-18 01:05:16,716-epoch: 8 data_type: test result: {'sentiment_acc': 0.8078108941418294, 'category_f1': {'precision': 0.9, 'recall': 0.7954779033915724, 'fscore': 0.8445171849427168}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.745945945945946, 'precision': 0.8625, 'recall': 0.6571428571428571}, 'anecdotes/miscellaneous': {'f1': 0.7707317073170731, 'precision': 0.8272251308900523, 'recall': 0.7214611872146118}, 'food': {'f1': 0.9213197969543148, 'precision': 0.9404145077720207, 'recall': 0.9029850746268657}, 'price': {'f1': 0.6666666666666666, 'precision': 1.0, 'recall': 0.5}, 'service': {'f1': 0.8727272727272728, 'precision': 0.8834355828220859, 'recall': 0.8622754491017964}}, 'acsc_metrics': {'ambience': {'acc': 0.8285714285714286}, 'anecdotes/miscellaneous': {'acc': 0.726027397260274}, 'food': {'acc': 0.8084577114427861}, 'price': {'acc': 0.8625}, 'service': {'acc': 0.874251497005988}}, 'polarity_metrics': {'negative': {'f1': 0.7328605200945626, 'precision': 0.7711442786069652, 'recall': 0.6981981981981982}, 'neutral': {'f1': 0.4432432432432433, 'precision': 0.45054945054945056, 'recall': 0.43617021276595747}, 'positive': {'f1': 0.881913303437967, 'precision': 0.8663729809104258, 'recall': 0.898021308980213}}, 'merge_micro_f1': 0.690671031096563}}
allennlp_callback.py-39-2021-03-18 01:05:19,578-epoch: 8 data_type: hard_test result: {'sentiment_acc': 0.660377358490566, 'category_f1': {'precision': 0.90625, 'recall': 0.5471698113207547, 'fscore': 0.6823529411764705}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.2222222222222222, 'precision': 1.0, 'recall': 0.125}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.8717948717948718, 'precision': 1.0, 'recall': 0.7727272727272727}, 'price': {'f1': 0.8333333333333333, 'precision': 1.0, 'recall': 0.7142857142857143}, 'service': {'f1': 0.631578947368421, 'precision': 0.6666666666666666, 'recall': 0.6}}, 'acsc_metrics': {'ambience': {'acc': 0.75}, 'anecdotes/miscellaneous': {'acc': 0.16666666666666666}, 'food': {'acc': 0.5909090909090909}, 'price': {'acc': 1.0}, 'service': {'acc': 0.8}}, 'polarity_metrics': {'negative': {'f1': 0.7555555555555556, 'precision': 0.68, 'recall': 0.85}, 'neutral': {'f1': 0.35294117647058826, 'precision': 0.6, 'recall': 0.25}, 'positive': {'f1': 0.6818181818181819, 'precision': 0.6521739130434783, 'recall': 0.7142857142857143}}, 'merge_micro_f1': 0.49411764705882355}}
allennlp_callback.py-39-2021-03-18 01:09:47,062-epoch: 9 data_type: train result: {'sentiment_acc': 0.8611388611388612, 'category_f1': {'precision': 0.8924731182795699, 'recall': 0.8568098568098568, 'fscore': 0.874277947672443}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.8344370860927154, 'precision': 0.9163636363636364, 'recall': 0.7659574468085106}, 'anecdotes/miscellaneous': {'f1': 0.8361320208453966, 'precision': 0.9174078780177891, 'recall': 0.7680851063829788}, 'food': {'f1': 0.9389946315275745, 'precision': 0.9205741626794258, 'recall': 0.9581673306772909}, 'price': {'f1': 0.8292682926829268, 'precision': 0.8755364806866953, 'recall': 0.7876447876447876}, 'service': {'f1': 0.8540433925049311, 'precision': 0.7974217311233885, 'recall': 0.9193205944798302}}, 'acsc_metrics': {'ambience': {'acc': 0.9604863221884499}, 'anecdotes/miscellaneous': {'acc': 0.7670212765957447}, 'food': {'acc': 0.9073705179282868}, 'price': {'acc': 0.9073359073359073}, 'service': {'acc': 0.8556263269639066}}, 'polarity_metrics': {'negative': {'f1': 0.8337912087912088, 'precision': 0.8158602150537635, 'recall': 0.8525280898876404}, 'neutral': {'f1': 0.7481751824817519, 'precision': 0.6184012066365008, 'recall': 0.9468822170900693}, 'positive': {'f1': 0.9085118702953099, 'precision': 0.9830827067669173, 'recall': 0.8444564047362756}}, 'merge_micro_f1': 0.763506625891947}}
allennlp_callback.py-39-2021-03-18 01:10:07,183-epoch: 9 data_type: dev result: {'sentiment_acc': 0.7094188376753507, 'category_f1': {'precision': 0.6013628620102215, 'recall': 0.7074148296593187, 'fscore': 0.6500920810313077}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.45454545454545453, 'precision': 0.45454545454545453, 'recall': 0.45454545454545453}, 'anecdotes/miscellaneous': {'f1': 0.6877192982456141, 'precision': 0.7205882352941176, 'recall': 0.6577181208053692}, 'food': {'f1': 0.6986666666666667, 'precision': 0.6093023255813953, 'recall': 0.81875}, 'price': {'f1': 0.6199999999999999, 'precision': 0.5636363636363636, 'recall': 0.6888888888888889}, 'service': {'f1': 0.6296296296296297, 'precision': 0.5396825396825397, 'recall': 0.7555555555555555}}, 'acsc_metrics': {'ambience': {'acc': 0.8}, 'anecdotes/miscellaneous': {'acc': 0.5704697986577181}, 'food': {'acc': 0.7875}, 'price': {'acc': 0.7333333333333333}, 'service': {'acc': 0.7333333333333333}}, 'polarity_metrics': {'negative': {'f1': 0.6641509433962265, 'precision': 0.6376811594202898, 'recall': 0.6929133858267716}, 'neutral': {'f1': 0.46560846560846564, 'precision': 0.36065573770491804, 'recall': 0.6567164179104478}, 'positive': {'f1': 0.8161764705882353, 'precision': 0.9288702928870293, 'recall': 0.7278688524590164}}, 'merge_micro_f1': 0.48618784530386744}}
allennlp_callback.py-39-2021-03-18 01:10:36,499-epoch: 9 data_type: test result: {'sentiment_acc': 0.7533401849948612, 'category_f1': {'precision': 0.8740157480314961, 'recall': 0.7985611510791367, 'fscore': 0.8345864661654135}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7204301075268816, 'precision': 0.8271604938271605, 'recall': 0.638095238095238}, 'anecdotes/miscellaneous': {'f1': 0.727735368956743, 'precision': 0.8218390804597702, 'recall': 0.6529680365296804}, 'food': {'f1': 0.9178255372945638, 'precision': 0.9331619537275064, 'recall': 0.9029850746268657}, 'price': {'f1': 0.7794117647058824, 'precision': 0.9464285714285714, 'recall': 0.6625}, 'service': {'f1': 0.848314606741573, 'precision': 0.798941798941799, 'recall': 0.9041916167664671}}, 'acsc_metrics': {'ambience': {'acc': 0.819047619047619}, 'anecdotes/miscellaneous': {'acc': 0.6255707762557078}, 'food': {'acc': 0.763681592039801}, 'price': {'acc': 0.8125}, 'service': {'acc': 0.8263473053892215}}, 'polarity_metrics': {'negative': {'f1': 0.7184035476718403, 'precision': 0.7074235807860262, 'recall': 0.7297297297297297}, 'neutral': {'f1': 0.4071428571428572, 'precision': 0.3064516129032258, 'recall': 0.6063829787234043}, 'positive': {'f1': 0.8460905349794238, 'precision': 0.921146953405018, 'recall': 0.7823439878234398}}, 'merge_micro_f1': 0.6509129967776586}}
allennlp_callback.py-39-2021-03-18 01:10:38,079-epoch: 9 data_type: hard_test result: {'sentiment_acc': 0.7169811320754716, 'category_f1': {'precision': 0.8857142857142857, 'recall': 0.5849056603773585, 'fscore': 0.7045454545454545}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.2222222222222222, 'precision': 1.0, 'recall': 0.125}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.8717948717948718, 'precision': 1.0, 'recall': 0.7727272727272727}, 'price': {'f1': 0.923076923076923, 'precision': 1.0, 'recall': 0.8571428571428571}, 'service': {'f1': 0.7, 'precision': 0.7, 'recall': 0.7}}, 'acsc_metrics': {'ambience': {'acc': 0.625}, 'anecdotes/miscellaneous': {'acc': 0.3333333333333333}, 'food': {'acc': 0.6363636363636364}, 'price': {'acc': 1.0}, 'service': {'acc': 1.0}}, 'polarity_metrics': {'negative': {'f1': 0.761904761904762, 'precision': 0.7272727272727273, 'recall': 0.8}, 'neutral': {'f1': 0.6428571428571429, 'precision': 0.5625, 'recall': 0.75}, 'positive': {'f1': 0.7222222222222222, 'precision': 0.8666666666666667, 'recall': 0.6190476190476191}}, 'merge_micro_f1': 0.5909090909090909}}
allennlp_callback.py-39-2021-03-18 01:14:21,734-epoch: 10 data_type: train result: {'sentiment_acc': 0.8951048951048951, 'category_f1': {'precision': 0.854940213971051, 'recall': 0.9047619047619048, 'fscore': 0.8791457692929947}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.8304405874499333, 'precision': 0.7404761904761905, 'recall': 0.9452887537993921}, 'anecdotes/miscellaneous': {'f1': 0.8634271099744245, 'precision': 0.8315270935960591, 'recall': 0.8978723404255319}, 'food': {'f1': 0.9196474857439089, 'precision': 0.9589189189189189, 'recall': 0.8834661354581673}, 'price': {'f1': 0.8864468864468864, 'precision': 0.8432055749128919, 'recall': 0.9343629343629344}, 'service': {'f1': 0.8642714570858284, 'precision': 0.815442561205273, 'recall': 0.9193205944798302}}, 'acsc_metrics': {'ambience': {'acc': 0.9635258358662614}, 'anecdotes/miscellaneous': {'acc': 0.7734042553191489}, 'food': {'acc': 0.9432270916334662}, 'price': {'acc': 0.972972972972973}, 'service': {'acc': 0.9447983014861996}}, 'polarity_metrics': {'negative': {'f1': 0.8670095518001468, 'precision': 0.9090909090909091, 'recall': 0.8286516853932584}, 'neutral': {'f1': 0.7741935483870966, 'precision': 0.6711864406779661, 'recall': 0.9145496535796767}, 'positive': {'f1': 0.939812258420762, 'precision': 0.9648526077097506, 'recall': 0.9160387513455328}}, 'merge_micro_f1': 0.7878983983174244}}
allennlp_callback.py-39-2021-03-18 01:14:38,682-epoch: 10 data_type: dev result: {'sentiment_acc': 0.7374749498997996, 'category_f1': {'precision': 0.5699088145896657, 'recall': 0.751503006012024, 'fscore': 0.648228176318064}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.5696969696969697, 'precision': 0.42727272727272725, 'recall': 0.8545454545454545}, 'anecdotes/miscellaneous': {'f1': 0.7138461538461539, 'precision': 0.6590909090909091, 'recall': 0.7785234899328859}, 'food': {'f1': 0.6409495548961426, 'precision': 0.6101694915254238, 'recall': 0.675}, 'price': {'f1': 0.6017699115044248, 'precision': 0.5, 'recall': 0.7555555555555555}, 'service': {'f1': 0.6451612903225806, 'precision': 0.5511811023622047, 'recall': 0.7777777777777778}}, 'acsc_metrics': {'ambience': {'acc': 0.7636363636363637}, 'anecdotes/miscellaneous': {'acc': 0.6241610738255033}, 'food': {'acc': 0.7875}, 'price': {'acc': 0.8444444444444444}, 'service': {'acc': 0.7666666666666667}}, 'polarity_metrics': {'negative': {'f1': 0.6363636363636365, 'precision': 0.6695652173913044, 'recall': 0.6062992125984252}, 'neutral': {'f1': 0.5119047619047619, 'precision': 0.42574257425742573, 'recall': 0.6417910447761194}, 'positive': {'f1': 0.8435374149659863, 'precision': 0.8763250883392226, 'recall': 0.8131147540983606}}, 'merge_micro_f1': 0.49265341400172863}}
allennlp_callback.py-39-2021-03-18 01:15:09,700-epoch: 10 data_type: test result: {'sentiment_acc': 0.775950668036999, 'category_f1': {'precision': 0.8255578093306288, 'recall': 0.8365878725590956, 'fscore': 0.8310362429811129}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7351778656126482, 'precision': 0.6283783783783784, 'recall': 0.8857142857142857}, 'anecdotes/miscellaneous': {'f1': 0.7675438596491229, 'precision': 0.7383966244725738, 'recall': 0.7990867579908676}, 'food': {'f1': 0.8793565683646113, 'precision': 0.9534883720930233, 'recall': 0.8159203980099502}, 'price': {'f1': 0.8533333333333333, 'precision': 0.9142857142857143, 'recall': 0.8}, 'service': {'f1': 0.8700564971751411, 'precision': 0.8235294117647058, 'recall': 0.9221556886227545}}, 'acsc_metrics': {'ambience': {'acc': 0.780952380952381}, 'anecdotes/miscellaneous': {'acc': 0.6073059360730594}, 'food': {'acc': 0.8109452736318408}, 'price': {'acc': 0.8125}, 'service': {'acc': 0.8922155688622755}}, 'polarity_metrics': {'negative': {'f1': 0.7058823529411765, 'precision': 0.7389162561576355, 'recall': 0.6756756756756757}, 'neutral': {'f1': 0.3983050847457627, 'precision': 0.33098591549295775, 'recall': 0.5}, 'positive': {'f1': 0.8684824902723736, 'precision': 0.8885350318471338, 'recall': 0.8493150684931506}}, 'merge_micro_f1': 0.664624808575804}}
allennlp_callback.py-39-2021-03-18 01:15:11,260-epoch: 10 data_type: hard_test result: {'sentiment_acc': 0.6981132075471698, 'category_f1': {'precision': 0.8095238095238095, 'recall': 0.6415094339622641, 'fscore': 0.7157894736842104}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7999999999999999, 'precision': 0.8571428571428571, 'recall': 0.75}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.7428571428571429, 'precision': 1.0, 'recall': 0.5909090909090909}, 'price': {'f1': 0.9333333333333333, 'precision': 0.875, 'recall': 1.0}, 'service': {'f1': 0.761904761904762, 'precision': 0.7272727272727273, 'recall': 0.8}}, 'acsc_metrics': {'ambience': {'acc': 0.75}, 'anecdotes/miscellaneous': {'acc': 0.16666666666666666}, 'food': {'acc': 0.6363636363636364}, 'price': {'acc': 1.0}, 'service': {'acc': 0.9}}, 'polarity_metrics': {'negative': {'f1': 0.7826086956521738, 'precision': 0.6923076923076923, 'recall': 0.9}, 'neutral': {'f1': 0.4, 'precision': 0.5, 'recall': 0.3333333333333333}, 'positive': {'f1': 0.7500000000000001, 'precision': 0.7894736842105263, 'recall': 0.7142857142857143}}, 'merge_micro_f1': 0.5894736842105263}}
allennlp_callback.py-39-2021-03-18 01:19:29,272-epoch: 11 data_type: train result: {'sentiment_acc': 0.9324009324009324, 'category_f1': {'precision': 0.8844974446337308, 'recall': 0.8644688644688645, 'fscore': 0.8743684742337487}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7478260869565218, 'precision': 0.8739837398373984, 'recall': 0.6534954407294833}, 'anecdotes/miscellaneous': {'f1': 0.8444193912063134, 'precision': 0.8980815347721822, 'recall': 0.7968085106382978}, 'food': {'f1': 0.941910705712914, 'precision': 0.9091751621872104, 'recall': 0.9770916334661355}, 'price': {'f1': 0.8819188191881919, 'precision': 0.8445229681978799, 'recall': 0.9227799227799228}, 'service': {'f1': 0.8547717842323652, 'precision': 0.8356997971602435, 'recall': 0.8747346072186837}}, 'acsc_metrics': {'ambience': {'acc': 0.9665653495440729}, 'anecdotes/miscellaneous': {'acc': 0.8659574468085106}, 'food': {'acc': 0.954183266932271}, 'price': {'acc': 0.9884169884169884}, 'service': {'acc': 0.9639065817409767}}, 'polarity_metrics': {'negative': {'f1': 0.9079584775086504, 'precision': 0.8949522510231923, 'recall': 0.9213483146067416}, 'neutral': {'f1': 0.8689655172413794, 'precision': 0.8649885583524027, 'recall': 0.8729792147806005}, 'positive': {'f1': 0.9569222432945002, 'precision': 0.9634478996181124, 'recall': 0.9504843918191603}}, 'merge_micro_f1': 0.8174469518356348}}
allennlp_callback.py-39-2021-03-18 01:19:45,698-epoch: 11 data_type: dev result: {'sentiment_acc': 0.7575150300601202, 'category_f1': {'precision': 0.5883333333333334, 'recall': 0.7074148296593187, 'fscore': 0.6424021838034578}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.37999999999999995, 'precision': 0.4222222222222222, 'recall': 0.34545454545454546}, 'anecdotes/miscellaneous': {'f1': 0.7074829931972788, 'precision': 0.7172413793103448, 'recall': 0.697986577181208}, 'food': {'f1': 0.6666666666666666, 'precision': 0.5682819383259912, 'recall': 0.80625}, 'price': {'f1': 0.6140350877192984, 'precision': 0.5072463768115942, 'recall': 0.7777777777777778}, 'service': {'f1': 0.6470588235294117, 'precision': 0.5789473684210527, 'recall': 0.7333333333333333}}, 'acsc_metrics': {'ambience': {'acc': 0.7272727272727273}, 'anecdotes/miscellaneous': {'acc': 0.6510067114093959}, 'food': {'acc': 0.80625}, 'price': {'acc': 0.8666666666666667}, 'service': {'acc': 0.8111111111111111}}, 'polarity_metrics': {'negative': {'f1': 0.7011070110701106, 'precision': 0.6597222222222222, 'recall': 0.7480314960629921}, 'neutral': {'f1': 0.5072463768115941, 'precision': 0.49295774647887325, 'recall': 0.5223880597014925}, 'positive': {'f1': 0.8421052631578947, 'precision': 0.8732394366197183, 'recall': 0.8131147540983606}}, 'merge_micro_f1': 0.5040946314831666}}
allennlp_callback.py-39-2021-03-18 01:20:15,701-epoch: 11 data_type: test result: {'sentiment_acc': 0.8006166495375129, 'category_f1': {'precision': 0.8696132596685083, 'recall': 0.8088386433710175, 'fscore': 0.838125665601704}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.6850828729281768, 'precision': 0.8157894736842105, 'recall': 0.5904761904761905}, 'anecdotes/miscellaneous': {'f1': 0.7167919799498746, 'precision': 0.7944444444444444, 'recall': 0.6529680365296804}, 'food': {'f1': 0.913151364764268, 'precision': 0.9108910891089109, 'recall': 0.9154228855721394}, 'price': {'f1': 0.8609271523178808, 'precision': 0.9154929577464789, 'recall': 0.8125}, 'service': {'f1': 0.873900293255132, 'precision': 0.8563218390804598, 'recall': 0.8922155688622755}}, 'acsc_metrics': {'ambience': {'acc': 0.819047619047619}, 'anecdotes/miscellaneous': {'acc': 0.6621004566210046}, 'food': {'acc': 0.8407960199004975}, 'price': {'acc': 0.7875}, 'service': {'acc': 0.8802395209580839}}, 'polarity_metrics': {'negative': {'f1': 0.7146067415730336, 'precision': 0.7130044843049327, 'recall': 0.7162162162162162}, 'neutral': {'f1': 0.43243243243243246, 'precision': 0.43956043956043955, 'recall': 0.425531914893617}, 'positive': {'f1': 0.8814589665653494, 'precision': 0.8801213960546282, 'recall': 0.882800608828006}}, 'merge_micro_f1': 0.6911608093716719}}
allennlp_callback.py-39-2021-03-18 01:20:17,178-epoch: 11 data_type: hard_test result: {'sentiment_acc': 0.6981132075471698, 'category_f1': {'precision': 0.9459459459459459, 'recall': 0.660377358490566, 'fscore': 0.7777777777777778}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.4, 'precision': 1.0, 'recall': 0.25}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.9523809523809523, 'precision': 1.0, 'recall': 0.9090909090909091}, 'price': {'f1': 1.0, 'precision': 1.0, 'recall': 1.0}, 'service': {'f1': 0.6666666666666665, 'precision': 0.75, 'recall': 0.6}}, 'acsc_metrics': {'ambience': {'acc': 0.625}, 'anecdotes/miscellaneous': {'acc': 0.3333333333333333}, 'food': {'acc': 0.6363636363636364}, 'price': {'acc': 1.0}, 'service': {'acc': 0.9}}, 'polarity_metrics': {'negative': {'f1': 0.7916666666666667, 'precision': 0.6785714285714286, 'recall': 0.95}, 'neutral': {'f1': 0.4210526315789474, 'precision': 0.5714285714285714, 'recall': 0.3333333333333333}, 'positive': {'f1': 0.717948717948718, 'precision': 0.7777777777777778, 'recall': 0.6666666666666666}}, 'merge_micro_f1': 0.5777777777777777}}
allennlp_callback.py-39-2021-03-18 01:24:00,945-epoch: 12 data_type: train result: {'sentiment_acc': 0.9557109557109557, 'category_f1': {'precision': 0.9071547420965058, 'recall': 0.9077589077589078, 'fscore': 0.9074567243675099}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.8576104746317513, 'precision': 0.9290780141843972, 'recall': 0.7963525835866262}, 'anecdotes/miscellaneous': {'f1': 0.8887739265390584, 'precision': 0.865055387713998, 'recall': 0.9138297872340425}, 'food': {'f1': 0.9593810444874274, 'precision': 0.9323308270676691, 'recall': 0.9880478087649402}, 'price': {'f1': 0.8952380952380952, 'precision': 0.8834586466165414, 'recall': 0.9073359073359073}, 'service': {'f1': 0.8679678530424798, 'precision': 0.945, 'recall': 0.802547770700637}}, 'acsc_metrics': {'ambience': {'acc': 0.9908814589665653}, 'anecdotes/miscellaneous': {'acc': 0.9031914893617021}, 'food': {'acc': 0.9731075697211156}, 'price': {'acc': 0.9961389961389961}, 'service': {'acc': 0.9766454352441614}}, 'polarity_metrics': {'negative': {'f1': 0.9388335704125179, 'precision': 0.9510086455331412, 'recall': 0.9269662921348315}, 'neutral': {'f1': 0.9124854142357061, 'precision': 0.9221698113207547, 'recall': 0.9030023094688222}, 'positive': {'f1': 0.971947635586428, 'precision': 0.9649867374005305, 'recall': 0.9790096878363832}}, 'merge_micro_f1': 0.8671770972037285}}
allennlp_callback.py-39-2021-03-18 01:24:17,796-epoch: 12 data_type: dev result: {'sentiment_acc': 0.7394789579158316, 'category_f1': {'precision': 0.5785953177257525, 'recall': 0.6933867735470942, 'fscore': 0.6308113035551505}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.4521739130434783, 'precision': 0.43333333333333335, 'recall': 0.4727272727272727}, 'anecdotes/miscellaneous': {'f1': 0.736842105263158, 'precision': 0.6839080459770115, 'recall': 0.7986577181208053}, 'food': {'f1': 0.6701570680628272, 'precision': 0.5765765765765766, 'recall': 0.8}, 'price': {'f1': 0.5607476635514019, 'precision': 0.4838709677419355, 'recall': 0.6666666666666666}, 'service': {'f1': 0.5058823529411766, 'precision': 0.5375, 'recall': 0.4777777777777778}}, 'acsc_metrics': {'ambience': {'acc': 0.8}, 'anecdotes/miscellaneous': {'acc': 0.6375838926174496}, 'food': {'acc': 0.78125}, 'price': {'acc': 0.8666666666666667}, 'service': {'acc': 0.7333333333333333}}, 'polarity_metrics': {'negative': {'f1': 0.6816479400749064, 'precision': 0.65, 'recall': 0.7165354330708661}, 'neutral': {'f1': 0.5070422535211268, 'precision': 0.48, 'recall': 0.5373134328358209}, 'positive': {'f1': 0.8217317487266554, 'precision': 0.852112676056338, 'recall': 0.7934426229508197}}, 'merge_micro_f1': 0.47948951686417496}}
allennlp_callback.py-39-2021-03-18 01:24:43,421-epoch: 12 data_type: test result: {'sentiment_acc': 0.7821171634121274, 'category_f1': {'precision': 0.8869565217391304, 'recall': 0.8386433710174718, 'fscore': 0.8621236133122028}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7897435897435897, 'precision': 0.8555555555555555, 'recall': 0.7333333333333333}, 'anecdotes/miscellaneous': {'f1': 0.7900677200902935, 'precision': 0.78125, 'recall': 0.7990867579908676}, 'food': {'f1': 0.9366459627329192, 'precision': 0.9354838709677419, 'recall': 0.9378109452736318}, 'price': {'f1': 0.8275862068965517, 'precision': 0.9230769230769231, 'recall': 0.75}, 'service': {'f1': 0.8327868852459017, 'precision': 0.9202898550724637, 'recall': 0.7604790419161677}}, 'acsc_metrics': {'ambience': {'acc': 0.8095238095238095}, 'anecdotes/miscellaneous': {'acc': 0.680365296803653}, 'food': {'acc': 0.7885572139303483}, 'price': {'acc': 0.8125}, 'service': {'acc': 0.8682634730538922}}, 'polarity_metrics': {'negative': {'f1': 0.7149532710280373, 'precision': 0.7427184466019418, 'recall': 0.6891891891891891}, 'neutral': {'f1': 0.35532994923857864, 'precision': 0.33980582524271846, 'recall': 0.3723404255319149}, 'positive': {'f1': 0.8675246025738076, 'precision': 0.8629518072289156, 'recall': 0.8721461187214612}}, 'merge_micro_f1': 0.6814580031695722}}
allennlp_callback.py-39-2021-03-18 01:24:44,574-epoch: 12 data_type: hard_test result: {'sentiment_acc': 0.6981132075471698, 'category_f1': {'precision': 0.9722222222222222, 'recall': 0.660377358490566, 'fscore': 0.7865168539325842}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.4, 'precision': 1.0, 'recall': 0.25}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.9523809523809523, 'precision': 1.0, 'recall': 0.9090909090909091}, 'price': {'f1': 1.0, 'precision': 1.0, 'recall': 1.0}, 'service': {'f1': 0.7058823529411764, 'precision': 0.8571428571428571, 'recall': 0.6}}, 'acsc_metrics': {'ambience': {'acc': 0.75}, 'anecdotes/miscellaneous': {'acc': 0.16666666666666666}, 'food': {'acc': 0.6818181818181818}, 'price': {'acc': 1.0}, 'service': {'acc': 0.8}}, 'polarity_metrics': {'negative': {'f1': 0.7999999999999999, 'precision': 0.72, 'recall': 0.9}, 'neutral': {'f1': 0.4210526315789474, 'precision': 0.5714285714285714, 'recall': 0.3333333333333333}, 'positive': {'f1': 0.7142857142857143, 'precision': 0.7142857142857143, 'recall': 0.7142857142857143}}, 'merge_micro_f1': 0.6067415730337078}}
allennlp_callback.py-39-2021-03-18 01:28:16,267-epoch: 13 data_type: train result: {'sentiment_acc': 0.938061938061938, 'category_f1': {'precision': 0.9376351838500361, 'recall': 0.8661338661338661, 'fscore': 0.9004673706075818}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.8896, 'precision': 0.9391891891891891, 'recall': 0.8449848024316109}, 'anecdotes/miscellaneous': {'f1': 0.8259010384850336, 'precision': 0.96987087517934, 'recall': 0.7191489361702128}, 'food': {'f1': 0.9610645638245441, 'precision': 0.9512195121951219, 'recall': 0.9711155378486056}, 'price': {'f1': 0.9166666666666667, 'precision': 0.8996282527881041, 'recall': 0.9343629343629344}, 'service': {'f1': 0.8977035490605427, 'precision': 0.8829568788501027, 'recall': 0.9129511677282378}}, 'acsc_metrics': {'ambience': {'acc': 0.9756838905775076}, 'anecdotes/miscellaneous': {'acc': 0.8553191489361702}, 'food': {'acc': 0.9770916334661355}, 'price': {'acc': 0.9806949806949807}, 'service': {'acc': 0.970276008492569}}, 'polarity_metrics': {'negative': {'f1': 0.926829268292683, 'precision': 0.9781591263650546, 'recall': 0.8806179775280899}, 'neutral': {'f1': 0.8716707021791767, 'precision': 0.916030534351145, 'recall': 0.8314087759815243}, 'positive': {'f1': 0.9563626861771622, 'precision': 0.9294057897409853, 'recall': 0.984930032292788}}, 'merge_micro_f1': 0.8474987017483123}}
allennlp_callback.py-39-2021-03-18 01:28:33,156-epoch: 13 data_type: dev result: {'sentiment_acc': 0.7715430861723447, 'category_f1': {'precision': 0.5873015873015873, 'recall': 0.6673346693386774, 'fscore': 0.6247654784240151}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.43697478991596644, 'precision': 0.40625, 'recall': 0.4727272727272727}, 'anecdotes/miscellaneous': {'f1': 0.704119850187266, 'precision': 0.7966101694915254, 'recall': 0.6308724832214765}, 'food': {'f1': 0.6720430107526882, 'precision': 0.589622641509434, 'recall': 0.78125}, 'price': {'f1': 0.5714285714285715, 'precision': 0.5, 'recall': 0.6666666666666666}, 'service': {'f1': 0.5714285714285715, 'precision': 0.5132743362831859, 'recall': 0.6444444444444445}}, 'acsc_metrics': {'ambience': {'acc': 0.8363636363636363}, 'anecdotes/miscellaneous': {'acc': 0.6442953020134228}, 'food': {'acc': 0.80625}, 'price': {'acc': 0.8666666666666667}, 'service': {'acc': 0.8333333333333334}}, 'polarity_metrics': {'negative': {'f1': 0.6842105263157895, 'precision': 0.7722772277227723, 'recall': 0.6141732283464567}, 'neutral': {'f1': 0.4878048780487805, 'precision': 0.5357142857142857, 'recall': 0.44776119402985076}, 'positive': {'f1': 0.856259659969088, 'precision': 0.8099415204678363, 'recall': 0.9081967213114754}}, 'merge_micro_f1': 0.4934333958724202}}
allennlp_callback.py-39-2021-03-18 01:28:56,993-epoch: 13 data_type: test result: {'sentiment_acc': 0.802672147995889, 'category_f1': {'precision': 0.8857142857142857, 'recall': 0.7965056526207606, 'fscore': 0.8387445887445887}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7487179487179487, 'precision': 0.8111111111111111, 'recall': 0.6952380952380952}, 'anecdotes/miscellaneous': {'f1': 0.7267904509283821, 'precision': 0.8670886075949367, 'recall': 0.6255707762557078}, 'food': {'f1': 0.9152970922882427, 'precision': 0.9305912596401028, 'recall': 0.900497512437811}, 'price': {'f1': 0.7887323943661972, 'precision': 0.9032258064516129, 'recall': 0.7}, 'service': {'f1': 0.8571428571428571, 'precision': 0.8352272727272727, 'recall': 0.8802395209580839}}, 'acsc_metrics': {'ambience': {'acc': 0.8095238095238095}, 'anecdotes/miscellaneous': {'acc': 0.730593607305936}, 'food': {'acc': 0.8283582089552238}, 'price': {'acc': 0.7875}, 'service': {'acc': 0.8383233532934131}}, 'polarity_metrics': {'negative': {'f1': 0.6752577319587628, 'precision': 0.7891566265060241, 'recall': 0.5900900900900901}, 'neutral': {'f1': 0.43589743589743596, 'precision': 0.5483870967741935, 'recall': 0.3617021276595745}, 'positive': {'f1': 0.8787446504992866, 'precision': 0.8268456375838926, 'recall': 0.9375951293759512}}, 'merge_micro_f1': 0.6904761904761905}}
allennlp_callback.py-39-2021-03-18 01:28:58,253-epoch: 13 data_type: hard_test result: {'sentiment_acc': 0.660377358490566, 'category_f1': {'precision': 0.9142857142857143, 'recall': 0.6037735849056604, 'fscore': 0.7272727272727272}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.4, 'precision': 1.0, 'recall': 0.25}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.8717948717948718, 'precision': 1.0, 'recall': 0.7727272727272727}, 'price': {'f1': 0.923076923076923, 'precision': 1.0, 'recall': 0.8571428571428571}, 'service': {'f1': 0.7, 'precision': 0.7, 'recall': 0.7}}, 'acsc_metrics': {'ambience': {'acc': 0.75}, 'anecdotes/miscellaneous': {'acc': 0.16666666666666666}, 'food': {'acc': 0.7272727272727273}, 'price': {'acc': 0.8571428571428571}, 'service': {'acc': 0.6}}, 'polarity_metrics': {'negative': {'f1': 0.7142857142857143, 'precision': 0.6818181818181818, 'recall': 0.75}, 'neutral': {'f1': 0.375, 'precision': 0.75, 'recall': 0.25}, 'positive': {'f1': 0.7083333333333334, 'precision': 0.6296296296296297, 'recall': 0.8095238095238095}}, 'merge_micro_f1': 0.5227272727272727}}
allennlp_callback.py-39-2021-03-18 01:32:26,954-epoch: 14 data_type: train result: {'sentiment_acc': 0.9723609723609724, 'category_f1': {'precision': 0.9532163742690059, 'recall': 0.9227439227439227, 'fscore': 0.9377326565143824}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.9415384615384615, 'precision': 0.9532710280373832, 'recall': 0.9300911854103343}, 'anecdotes/miscellaneous': {'f1': 0.9020270270270271, 'precision': 0.9581339712918661, 'recall': 0.8521276595744681}, 'food': {'f1': 0.9707486365889936, 'precision': 0.9664363277393879, 'recall': 0.9750996015936255}, 'price': {'f1': 0.9373814041745732, 'precision': 0.9216417910447762, 'recall': 0.9536679536679536}, 'service': {'f1': 0.9319148936170213, 'precision': 0.9339019189765458, 'recall': 0.9299363057324841}}, 'acsc_metrics': {'ambience': {'acc': 1.0}, 'anecdotes/miscellaneous': {'acc': 0.9287234042553192}, 'food': {'acc': 0.9850597609561753}, 'price': {'acc': 1.0}, 'service': {'acc': 0.9978768577494692}}, 'polarity_metrics': {'negative': {'f1': 0.9654686398872446, 'precision': 0.9688826025459689, 'recall': 0.9620786516853933}, 'neutral': {'f1': 0.9452380952380952, 'precision': 0.9754299754299754, 'recall': 0.9168591224018475}, 'positive': {'f1': 0.9810515078729651, 'precision': 0.973001588141874, 'recall': 0.9892357373519914}}, 'merge_micro_f1': 0.9120135363790186}}
allennlp_callback.py-39-2021-03-18 01:32:44,804-epoch: 14 data_type: dev result: {'sentiment_acc': 0.7775551102204409, 'category_f1': {'precision': 0.5652173913043478, 'recall': 0.6513026052104208, 'fscore': 0.6052141527001862}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.432, 'precision': 0.38571428571428573, 'recall': 0.4909090909090909}, 'anecdotes/miscellaneous': {'f1': 0.7094594594594594, 'precision': 0.7142857142857143, 'recall': 0.7046979865771812}, 'food': {'f1': 0.6444444444444445, 'precision': 0.58, 'recall': 0.725}, 'price': {'f1': 0.5384615384615384, 'precision': 0.4745762711864407, 'recall': 0.6222222222222222}, 'service': {'f1': 0.5185185185185185, 'precision': 0.494949494949495, 'recall': 0.5444444444444444}}, 'acsc_metrics': {'ambience': {'acc': 0.8363636363636363}, 'anecdotes/miscellaneous': {'acc': 0.6577181208053692}, 'food': {'acc': 0.825}, 'price': {'acc': 0.9111111111111111}, 'service': {'acc': 0.7888888888888889}}, 'polarity_metrics': {'negative': {'f1': 0.7159533073929961, 'precision': 0.7076923076923077, 'recall': 0.7244094488188977}, 'neutral': {'f1': 0.5151515151515151, 'precision': 0.5230769230769231, 'recall': 0.5074626865671642}, 'positive': {'f1': 0.8604269293924466, 'precision': 0.8618421052631579, 'recall': 0.8590163934426229}}, 'merge_micro_f1': 0.4729981378026071}}
allennlp_callback.py-39-2021-03-18 01:33:11,987-epoch: 14 data_type: test result: {'sentiment_acc': 0.8036998972250771, 'category_f1': {'precision': 0.8832199546485261, 'recall': 0.8006166495375129, 'fscore': 0.8398921832884098}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7524752475247525, 'precision': 0.7835051546391752, 'recall': 0.7238095238095238}, 'anecdotes/miscellaneous': {'f1': 0.7543424317617865, 'precision': 0.8260869565217391, 'recall': 0.6940639269406392}, 'food': {'f1': 0.9107142857142858, 'precision': 0.9345549738219895, 'recall': 0.8880597014925373}, 'price': {'f1': 0.7769784172661871, 'precision': 0.9152542372881356, 'recall': 0.675}, 'service': {'f1': 0.8562691131498471, 'precision': 0.875, 'recall': 0.8383233532934131}}, 'acsc_metrics': {'ambience': {'acc': 0.819047619047619}, 'anecdotes/miscellaneous': {'acc': 0.726027397260274}, 'food': {'acc': 0.8159203980099502}, 'price': {'acc': 0.7875}, 'service': {'acc': 0.874251497005988}}, 'polarity_metrics': {'negative': {'f1': 0.7226107226107226, 'precision': 0.748792270531401, 'recall': 0.6981981981981982}, 'neutral': {'f1': 0.40449438202247195, 'precision': 0.42857142857142855, 'recall': 0.3829787234042553}, 'positive': {'f1': 0.8827483196415236, 'precision': 0.8665689149560117, 'recall': 0.8995433789954338}}, 'merge_micro_f1': 0.7008086253369271}}
allennlp_callback.py-39-2021-03-18 01:33:13,367-epoch: 14 data_type: hard_test result: {'sentiment_acc': 0.6415094339622641, 'category_f1': {'precision': 0.9444444444444444, 'recall': 0.6415094339622641, 'fscore': 0.7640449438202246}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.5454545454545454, 'precision': 1.0, 'recall': 0.375}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.9, 'precision': 1.0, 'recall': 0.8181818181818182}, 'price': {'f1': 0.923076923076923, 'precision': 1.0, 'recall': 0.8571428571428571}, 'service': {'f1': 0.7368421052631577, 'precision': 0.7777777777777778, 'recall': 0.7}}, 'acsc_metrics': {'ambience': {'acc': 0.75}, 'anecdotes/miscellaneous': {'acc': 0.16666666666666666}, 'food': {'acc': 0.6363636363636364}, 'price': {'acc': 0.8571428571428571}, 'service': {'acc': 0.7}}, 'polarity_metrics': {'negative': {'f1': 0.6976744186046512, 'precision': 0.6521739130434783, 'recall': 0.75}, 'neutral': {'f1': 0.4444444444444444, 'precision': 0.6666666666666666, 'recall': 0.3333333333333333}, 'positive': {'f1': 0.6666666666666666, 'precision': 0.625, 'recall': 0.7142857142857143}}, 'merge_micro_f1': 0.5393258426966292}}
allennlp_callback.py-39-2021-03-18 01:36:46,455-epoch: 15 data_type: train result: {'sentiment_acc': 0.9820179820179821, 'category_f1': {'precision': 0.9420289855072463, 'recall': 0.9523809523809523, 'fscore': 0.9471766848816029}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.9526813880126183, 'precision': 0.9901639344262295, 'recall': 0.9179331306990881}, 'anecdotes/miscellaneous': {'f1': 0.9335548172757476, 'precision': 0.9734411085450346, 'recall': 0.8968085106382979}, 'food': {'f1': 0.9542857142857143, 'precision': 0.9142335766423357, 'recall': 0.99800796812749}, 'price': {'f1': 0.9534450651769089, 'precision': 0.920863309352518, 'recall': 0.9884169884169884}, 'service': {'f1': 0.9501039501039501, 'precision': 0.9307535641547862, 'recall': 0.970276008492569}}, 'acsc_metrics': {'ambience': {'acc': 1.0}, 'anecdotes/miscellaneous': {'acc': 0.9563829787234043}, 'food': {'acc': 0.9870517928286853}, 'price': {'acc': 1.0}, 'service': {'acc': 1.0}}, 'polarity_metrics': {'negative': {'f1': 0.9777158774373259, 'precision': 0.9696132596685083, 'recall': 0.9859550561797753}, 'neutral': {'f1': 0.9645390070921985, 'precision': 0.9878934624697336, 'recall': 0.9422632794457275}, 'positive': {'f1': 0.9876476906552094, 'precision': 0.9855305466237942, 'recall': 0.9897739504843919}}, 'merge_micro_f1': 0.9302864712700778}}
allennlp_callback.py-39-2021-03-18 01:37:04,901-epoch: 15 data_type: dev result: {'sentiment_acc': 0.779559118236473, 'category_f1': {'precision': 0.5530179445350734, 'recall': 0.6793587174348698, 'fscore': 0.6097122302158272}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.4000000000000001, 'precision': 0.4, 'recall': 0.4}, 'anecdotes/miscellaneous': {'f1': 0.7066666666666666, 'precision': 0.7019867549668874, 'recall': 0.7114093959731543}, 'food': {'f1': 0.654911838790932, 'precision': 0.5485232067510548, 'recall': 0.8125}, 'price': {'f1': 0.5185185185185185, 'precision': 0.4444444444444444, 'recall': 0.6222222222222222}, 'service': {'f1': 0.5380710659898477, 'precision': 0.4953271028037383, 'recall': 0.5888888888888889}}, 'acsc_metrics': {'ambience': {'acc': 0.8}, 'anecdotes/miscellaneous': {'acc': 0.6644295302013423}, 'food': {'acc': 0.8375}, 'price': {'acc': 0.8666666666666667}, 'service': {'acc': 0.8111111111111111}}, 'polarity_metrics': {'negative': {'f1': 0.7218045112781956, 'precision': 0.6906474820143885, 'recall': 0.7559055118110236}, 'neutral': {'f1': 0.5426356589147286, 'precision': 0.5645161290322581, 'recall': 0.5223880597014925}, 'positive': {'f1': 0.8557213930348259, 'precision': 0.8657718120805369, 'recall': 0.8459016393442623}}, 'merge_micro_f1': 0.48201438848920863}}
allennlp_callback.py-39-2021-03-18 01:37:30,801-epoch: 15 data_type: test result: {'sentiment_acc': 0.802672147995889, 'category_f1': {'precision': 0.8778378378378379, 'recall': 0.8345323741007195, 'fscore': 0.8556375131717597}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7643979057591622, 'precision': 0.8488372093023255, 'recall': 0.6952380952380952}, 'anecdotes/miscellaneous': {'f1': 0.7623762376237624, 'precision': 0.8324324324324325, 'recall': 0.7031963470319634}, 'food': {'f1': 0.925700365408039, 'precision': 0.9069212410501193, 'recall': 0.945273631840796}, 'price': {'f1': 0.7972027972027972, 'precision': 0.9047619047619048, 'recall': 0.7125}, 'service': {'f1': 0.8731563421828908, 'precision': 0.8604651162790697, 'recall': 0.8862275449101796}}, 'acsc_metrics': {'ambience': {'acc': 0.8285714285714286}, 'anecdotes/miscellaneous': {'acc': 0.6757990867579908}, 'food': {'acc': 0.8308457711442786}, 'price': {'acc': 0.825}, 'service': {'acc': 0.874251497005988}}, 'polarity_metrics': {'negative': {'f1': 0.730593607305936, 'precision': 0.7407407407407407, 'recall': 0.7207207207207207}, 'neutral': {'f1': 0.3977272727272727, 'precision': 0.4268292682926829, 'recall': 0.3723404255319149}, 'positive': {'f1': 0.8798798798798798, 'precision': 0.8681481481481481, 'recall': 0.8919330289193302}}, 'merge_micro_f1': 0.7070600632244468}}
allennlp_callback.py-39-2021-03-18 01:37:31,989-epoch: 15 data_type: hard_test result: {'sentiment_acc': 0.7169811320754716, 'category_f1': {'precision': 0.9473684210526315, 'recall': 0.6792452830188679, 'fscore': 0.7912087912087911}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.5454545454545454, 'precision': 1.0, 'recall': 0.375}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.9523809523809523, 'precision': 1.0, 'recall': 0.9090909090909091}, 'price': {'f1': 0.8571428571428571, 'precision': 0.8571428571428571, 'recall': 0.8571428571428571}, 'service': {'f1': 0.7777777777777777, 'precision': 0.875, 'recall': 0.7}}, 'acsc_metrics': {'ambience': {'acc': 0.75}, 'anecdotes/miscellaneous': {'acc': 0.16666666666666666}, 'food': {'acc': 0.7272727272727273}, 'price': {'acc': 1.0}, 'service': {'acc': 0.8}}, 'polarity_metrics': {'negative': {'f1': 0.7999999999999999, 'precision': 0.72, 'recall': 0.9}, 'neutral': {'f1': 0.4444444444444444, 'precision': 0.6666666666666666, 'recall': 0.3333333333333333}, 'positive': {'f1': 0.7441860465116279, 'precision': 0.7272727272727273, 'recall': 0.7619047619047619}}, 'merge_micro_f1': 0.6373626373626373}}
allennlp_callback.py-39-2021-03-18 01:41:06,959-epoch: 16 data_type: train result: {'sentiment_acc': 0.9776889776889777, 'category_f1': {'precision': 0.9253098499673842, 'recall': 0.9447219447219447, 'fscore': 0.9349151425275992}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.9475262368815592, 'precision': 0.9349112426035503, 'recall': 0.9604863221884499}, 'anecdotes/miscellaneous': {'f1': 0.9318181818181818, 'precision': 0.948237885462555, 'recall': 0.9159574468085107}, 'food': {'f1': 0.9637462235649547, 'precision': 0.9745417515274949, 'recall': 0.953187250996016}, 'price': {'f1': 0.9445506692160611, 'precision': 0.9356060606060606, 'recall': 0.9536679536679536}, 'service': {'f1': 0.8727272727272728, 'precision': 0.794425087108014, 'recall': 0.9681528662420382}}, 'acsc_metrics': {'ambience': {'acc': 1.0}, 'anecdotes/miscellaneous': {'acc': 0.9531914893617022}, 'food': {'acc': 0.9880478087649402}, 'price': {'acc': 1.0}, 'service': {'acc': 0.9766454352441614}}, 'polarity_metrics': {'negative': {'f1': 0.969993021632938, 'precision': 0.9639389736477115, 'recall': 0.976123595505618}, 'neutral': {'f1': 0.9585798816568047, 'precision': 0.9830097087378641, 'recall': 0.9353348729792148}, 'positive': {'f1': 0.9849785407725322, 'precision': 0.9818181818181818, 'recall': 0.9881593110871906}}, 'merge_micro_f1': 0.9131652661064427}}
allennlp_callback.py-39-2021-03-18 01:41:25,209-epoch: 16 data_type: dev result: {'sentiment_acc': 0.7915831663326653, 'category_f1': {'precision': 0.5441176470588235, 'recall': 0.6673346693386774, 'fscore': 0.5994599459945994}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.45454545454545453, 'precision': 0.38961038961038963, 'recall': 0.5454545454545454}, 'anecdotes/miscellaneous': {'f1': 0.6996699669966997, 'precision': 0.6883116883116883, 'recall': 0.7114093959731543}, 'food': {'f1': 0.6081871345029239, 'precision': 0.5714285714285714, 'recall': 0.65}, 'price': {'f1': 0.5242718446601942, 'precision': 0.46551724137931033, 'recall': 0.6}, 'service': {'f1': 0.5714285714285714, 'precision': 0.46808510638297873, 'recall': 0.7333333333333333}}, 'acsc_metrics': {'ambience': {'acc': 0.8363636363636363}, 'anecdotes/miscellaneous': {'acc': 0.7046979865771812}, 'food': {'acc': 0.8375}, 'price': {'acc': 0.8888888888888888}, 'service': {'acc': 0.7777777777777778}}, 'polarity_metrics': {'negative': {'f1': 0.7279411764705882, 'precision': 0.6827586206896552, 'recall': 0.7795275590551181}, 'neutral': {'f1': 0.5925925925925926, 'precision': 0.5882352941176471, 'recall': 0.5970149253731343}, 'positive': {'f1': 0.8663282571912013, 'precision': 0.8951048951048951, 'recall': 0.839344262295082}}, 'merge_micro_f1': 0.48244824482448245}}
allennlp_callback.py-39-2021-03-18 01:41:51,713-epoch: 16 data_type: test result: {'sentiment_acc': 0.8078108941418294, 'category_f1': {'precision': 0.8606741573033708, 'recall': 0.7872559095580678, 'fscore': 0.8223295759527643}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7464114832535885, 'precision': 0.75, 'recall': 0.7428571428571429}, 'anecdotes/miscellaneous': {'f1': 0.7639902676399026, 'precision': 0.8177083333333334, 'recall': 0.7168949771689498}, 'food': {'f1': 0.8778523489932886, 'precision': 0.9533527696793003, 'recall': 0.8134328358208955}, 'price': {'f1': 0.7769784172661871, 'precision': 0.9152542372881356, 'recall': 0.675}, 'service': {'f1': 0.8356545961002785, 'precision': 0.78125, 'recall': 0.8982035928143712}}, 'acsc_metrics': {'ambience': {'acc': 0.8476190476190476}, 'anecdotes/miscellaneous': {'acc': 0.7031963470319634}, 'food': {'acc': 0.8208955223880597}, 'price': {'acc': 0.85}, 'service': {'acc': 0.8682634730538922}}, 'polarity_metrics': {'negative': {'f1': 0.7438752783964365, 'precision': 0.73568281938326, 'recall': 0.7522522522522522}, 'neutral': {'f1': 0.4161849710982659, 'precision': 0.45569620253164556, 'recall': 0.3829787234042553}, 'positive': {'f1': 0.8806646525679759, 'precision': 0.8740629685157422, 'recall': 0.8873668188736682}}, 'merge_micro_f1': 0.6859903381642513}}
allennlp_callback.py-39-2021-03-18 01:41:52,873-epoch: 16 data_type: hard_test result: {'sentiment_acc': 0.660377358490566, 'category_f1': {'precision': 0.9142857142857143, 'recall': 0.6037735849056604, 'fscore': 0.7272727272727272}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.5454545454545454, 'precision': 1.0, 'recall': 0.375}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.8108108108108109, 'precision': 1.0, 'recall': 0.6818181818181818}, 'price': {'f1': 0.8333333333333333, 'precision': 1.0, 'recall': 0.7142857142857143}, 'service': {'f1': 0.8571428571428572, 'precision': 0.8181818181818182, 'recall': 0.9}}, 'acsc_metrics': {'ambience': {'acc': 0.75}, 'anecdotes/miscellaneous': {'acc': 0.16666666666666666}, 'food': {'acc': 0.5909090909090909}, 'price': {'acc': 1.0}, 'service': {'acc': 0.8}}, 'polarity_metrics': {'negative': {'f1': 0.7391304347826088, 'precision': 0.6538461538461539, 'recall': 0.85}, 'neutral': {'f1': 0.375, 'precision': 0.75, 'recall': 0.25}, 'positive': {'f1': 0.6818181818181819, 'precision': 0.6521739130434783, 'recall': 0.7142857142857143}}, 'merge_micro_f1': 0.5681818181818181}}
allennlp_callback.py-39-2021-03-18 01:45:26,545-epoch: 17 data_type: train result: {'sentiment_acc': 0.9693639693639694, 'category_f1': {'precision': 0.8939766441303012, 'recall': 0.9686979686979686, 'fscore': 0.9298385807895158}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.9300998573466477, 'precision': 0.8763440860215054, 'recall': 0.9908814589665653}, 'anecdotes/miscellaneous': {'f1': 0.9139203148057059, 'precision': 0.8499542543458372, 'recall': 0.9882978723404255}, 'food': {'f1': 0.9526686807653575, 'precision': 0.9633401221995926, 'recall': 0.9422310756972112}, 'price': {'f1': 0.9183303085299456, 'precision': 0.8664383561643836, 'recall': 0.9768339768339769}, 'service': {'f1': 0.922920892494929, 'precision': 0.883495145631068, 'recall': 0.9660297239915074}}, 'acsc_metrics': {'ambience': {'acc': 0.9969604863221885}, 'anecdotes/miscellaneous': {'acc': 0.951063829787234}, 'food': {'acc': 0.9641434262948207}, 'price': {'acc': 0.9806949806949807}, 'service': {'acc': 0.9915074309978769}}, 'polarity_metrics': {'negative': {'f1': 0.9592123769338959, 'precision': 0.9605633802816902, 'recall': 0.9578651685393258}, 'neutral': {'f1': 0.9501133786848073, 'precision': 0.933184855233853, 'recall': 0.9676674364896074}, 'positive': {'f1': 0.9778498109130199, 'precision': 0.9815618221258134, 'recall': 0.9741657696447793}}, 'merge_micro_f1': 0.9017100847051303}}
allennlp_callback.py-39-2021-03-18 01:45:43,796-epoch: 17 data_type: dev result: {'sentiment_acc': 0.7474949899799599, 'category_f1': {'precision': 0.5438066465256798, 'recall': 0.7214428857715431, 'fscore': 0.6201550387596899}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.4827586206896552, 'precision': 0.3888888888888889, 'recall': 0.6363636363636364}, 'anecdotes/miscellaneous': {'f1': 0.6857142857142857, 'precision': 0.5970149253731343, 'recall': 0.8053691275167785}, 'food': {'f1': 0.6512968299711815, 'precision': 0.6042780748663101, 'recall': 0.70625}, 'price': {'f1': 0.5486725663716815, 'precision': 0.45588235294117646, 'recall': 0.6888888888888889}, 'service': {'f1': 0.5922330097087379, 'precision': 0.5258620689655172, 'recall': 0.6777777777777778}}, 'acsc_metrics': {'ambience': {'acc': 0.8727272727272727}, 'anecdotes/miscellaneous': {'acc': 0.6510067114093959}, 'food': {'acc': 0.775}, 'price': {'acc': 0.8}, 'service': {'acc': 0.7555555555555555}}, 'polarity_metrics': {'negative': {'f1': 0.6562500000000001, 'precision': 0.6511627906976745, 'recall': 0.6614173228346457}, 'neutral': {'f1': 0.5503355704697986, 'precision': 0.5, 'recall': 0.6119402985074627}, 'positive': {'f1': 0.836424957841484, 'precision': 0.8611111111111112, 'recall': 0.8131147540983606}}, 'merge_micro_f1': 0.4823428079242033}}
allennlp_callback.py-39-2021-03-18 01:46:09,196-epoch: 17 data_type: test result: {'sentiment_acc': 0.7841726618705036, 'category_f1': {'precision': 0.8244897959183674, 'recall': 0.8304213771839671, 'fscore': 0.8274449564772145}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7807017543859649, 'precision': 0.7235772357723578, 'recall': 0.8476190476190476}, 'anecdotes/miscellaneous': {'f1': 0.732919254658385, 'precision': 0.6704545454545454, 'recall': 0.8082191780821918}, 'food': {'f1': 0.8716577540106952, 'precision': 0.9421965317919075, 'recall': 0.8109452736318408}, 'price': {'f1': 0.8461538461538461, 'precision': 0.868421052631579, 'recall': 0.825}, 'service': {'f1': 0.8875739644970414, 'precision': 0.8771929824561403, 'recall': 0.8982035928143712}}, 'acsc_metrics': {'ambience': {'acc': 0.8285714285714286}, 'anecdotes/miscellaneous': {'acc': 0.680365296803653}, 'food': {'acc': 0.7960199004975125}, 'price': {'acc': 0.825}, 'service': {'acc': 0.844311377245509}}, 'polarity_metrics': {'negative': {'f1': 0.7276995305164319, 'precision': 0.7598039215686274, 'recall': 0.6981981981981982}, 'neutral': {'f1': 0.38914027149321273, 'precision': 0.33858267716535434, 'recall': 0.4574468085106383}, 'positive': {'f1': 0.8698999230177059, 'precision': 0.8800623052959502, 'recall': 0.8599695585996956}}, 'merge_micro_f1': 0.6707629288274449}}
allennlp_callback.py-39-2021-03-18 01:46:10,780-epoch: 17 data_type: hard_test result: {'sentiment_acc': 0.6226415094339622, 'category_f1': {'precision': 0.8108108108108109, 'recall': 0.5660377358490566, 'fscore': 0.6666666666666667}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.5454545454545454, 'precision': 1.0, 'recall': 0.375}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.7428571428571429, 'precision': 1.0, 'recall': 0.5909090909090909}, 'price': {'f1': 0.9333333333333333, 'precision': 0.875, 'recall': 1.0}, 'service': {'f1': 0.7, 'precision': 0.7, 'recall': 0.7}}, 'acsc_metrics': {'ambience': {'acc': 0.75}, 'anecdotes/miscellaneous': {'acc': 0.16666666666666666}, 'food': {'acc': 0.5909090909090909}, 'price': {'acc': 1.0}, 'service': {'acc': 0.6}}, 'polarity_metrics': {'negative': {'f1': 0.75, 'precision': 0.75, 'recall': 0.75}, 'neutral': {'f1': 0.3157894736842105, 'precision': 0.42857142857142855, 'recall': 0.25}, 'positive': {'f1': 0.6382978723404256, 'precision': 0.5769230769230769, 'recall': 0.7142857142857143}}, 'merge_micro_f1': 0.48888888888888893}}
allennlp_callback.py-39-2021-03-18 01:49:49,558-epoch: 18 data_type: train result: {'sentiment_acc': 0.9766899766899767, 'category_f1': {'precision': 0.9277504105090312, 'recall': 0.9407259407259407, 'fscore': 0.9341931216931216}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.9104258443465492, 'precision': 0.8806818181818182, 'recall': 0.9422492401215805}, 'anecdotes/miscellaneous': {'f1': 0.9291338582677167, 'precision': 0.917098445595855, 'recall': 0.9414893617021277}, 'food': {'f1': 0.9732142857142857, 'precision': 0.9693675889328063, 'recall': 0.9770916334661355}, 'price': {'f1': 0.899641577060932, 'precision': 0.8394648829431438, 'recall': 0.9691119691119691}, 'service': {'f1': 0.8963963963963963, 'precision': 0.9544364508393285, 'recall': 0.8450106157112527}}, 'acsc_metrics': {'ambience': {'acc': 0.9908814589665653}, 'anecdotes/miscellaneous': {'acc': 0.9563829787234043}, 'food': {'acc': 0.9820717131474104}, 'price': {'acc': 0.9884169884169884}, 'service': {'acc': 0.9893842887473461}}, 'polarity_metrics': {'negative': {'f1': 0.9680170575692965, 'precision': 0.9798561151079137, 'recall': 0.9564606741573034}, 'neutral': {'f1': 0.9606674612634089, 'precision': 0.9926108374384236, 'recall': 0.930715935334873}, 'positive': {'f1': 0.9835106382978724, 'precision': 0.9721345951629863, 'recall': 0.9951560818083961}}, 'merge_micro_f1': 0.9123677248677248}}
allennlp_callback.py-39-2021-03-18 01:50:06,339-epoch: 18 data_type: dev result: {'sentiment_acc': 0.7635270541082164, 'category_f1': {'precision': 0.5420098846787479, 'recall': 0.6593186372745491, 'fscore': 0.5949367088607594}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.4427480916030534, 'precision': 0.3815789473684211, 'recall': 0.5272727272727272}, 'anecdotes/miscellaneous': {'f1': 0.7129909365558913, 'precision': 0.6483516483516484, 'recall': 0.7919463087248322}, 'food': {'f1': 0.6271186440677966, 'precision': 0.5721649484536082, 'recall': 0.69375}, 'price': {'f1': 0.5378151260504201, 'precision': 0.43243243243243246, 'recall': 0.7111111111111111}, 'service': {'f1': 0.456140350877193, 'precision': 0.48148148148148145, 'recall': 0.43333333333333335}}, 'acsc_metrics': {'ambience': {'acc': 0.7818181818181819}, 'anecdotes/miscellaneous': {'acc': 0.6845637583892618}, 'food': {'acc': 0.80625}, 'price': {'acc': 0.8222222222222222}, 'service': {'acc': 0.7777777777777778}}, 'polarity_metrics': {'negative': {'f1': 0.6854838709677419, 'precision': 0.7024793388429752, 'recall': 0.6692913385826772}, 'neutral': {'f1': 0.5333333333333332, 'precision': 0.5294117647058824, 'recall': 0.5373134328358209}, 'positive': {'f1': 0.8455284552845529, 'precision': 0.8387096774193549, 'recall': 0.8524590163934426}}, 'merge_micro_f1': 0.46112115732368897}}
allennlp_callback.py-39-2021-03-18 01:50:31,585-epoch: 18 data_type: test result: {'sentiment_acc': 0.8129496402877698, 'category_f1': {'precision': 0.8524590163934426, 'recall': 0.8016443987667009, 'fscore': 0.826271186440678}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7281105990783412, 'precision': 0.7053571428571429, 'recall': 0.7523809523809524}, 'anecdotes/miscellaneous': {'f1': 0.7640449438202246, 'precision': 0.7522123893805309, 'recall': 0.776255707762557}, 'food': {'f1': 0.8897637795275589, 'precision': 0.9416666666666667, 'recall': 0.8432835820895522}, 'price': {'f1': 0.8662420382165604, 'precision': 0.8831168831168831, 'recall': 0.85}, 'service': {'f1': 0.8078175895765471, 'precision': 0.8857142857142857, 'recall': 0.7425149700598802}}, 'acsc_metrics': {'ambience': {'acc': 0.819047619047619}, 'anecdotes/miscellaneous': {'acc': 0.7351598173515982}, 'food': {'acc': 0.8333333333333334}, 'price': {'acc': 0.8125}, 'service': {'acc': 0.8622754491017964}}, 'polarity_metrics': {'negative': {'f1': 0.721153846153846, 'precision': 0.7731958762886598, 'recall': 0.6756756756756757}, 'neutral': {'f1': 0.41916167664670656, 'precision': 0.4794520547945205, 'recall': 0.3723404255319149}, 'positive': {'f1': 0.8892149669845928, 'precision': 0.8583569405099151, 'recall': 0.9223744292237442}}, 'merge_micro_f1': 0.6800847457627119}}
allennlp_callback.py-39-2021-03-18 01:50:32,866-epoch: 18 data_type: hard_test result: {'sentiment_acc': 0.7358490566037735, 'category_f1': {'precision': 0.8823529411764706, 'recall': 0.5660377358490566, 'fscore': 0.6896551724137931}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.4, 'precision': 1.0, 'recall': 0.25}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.8421052631578948, 'precision': 1.0, 'recall': 0.7272727272727273}, 'price': {'f1': 0.9333333333333333, 'precision': 0.875, 'recall': 1.0}, 'service': {'f1': 0.625, 'precision': 0.8333333333333334, 'recall': 0.5}}, 'acsc_metrics': {'ambience': {'acc': 0.75}, 'anecdotes/miscellaneous': {'acc': 0.16666666666666666}, 'food': {'acc': 0.7272727272727273}, 'price': {'acc': 1.0}, 'service': {'acc': 0.9}}, 'polarity_metrics': {'negative': {'f1': 0.8260869565217392, 'precision': 0.7307692307692307, 'recall': 0.95}, 'neutral': {'f1': 0.375, 'precision': 0.75, 'recall': 0.25}, 'positive': {'f1': 0.7727272727272727, 'precision': 0.7391304347826086, 'recall': 0.8095238095238095}}, 'merge_micro_f1': 0.5747126436781609}}
allennlp_callback.py-39-2021-03-18 01:54:10,864-epoch: 19 data_type: train result: {'sentiment_acc': 0.9846819846819846, 'category_f1': {'precision': 0.9480645161290323, 'recall': 0.9786879786879786, 'fscore': 0.9631328854661642}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.9634146341463415, 'precision': 0.9663608562691132, 'recall': 0.9604863221884499}, 'anecdotes/miscellaneous': {'f1': 0.9510056730273337, 'precision': 0.9229229229229229, 'recall': 0.9808510638297873}, 'food': {'f1': 0.9788905252822779, 'precision': 0.9651500484027106, 'recall': 0.9930278884462151}, 'price': {'f1': 0.9481765834932822, 'precision': 0.9427480916030534, 'recall': 0.9536679536679536}, 'service': {'f1': 0.9621052631578948, 'precision': 0.954070981210856, 'recall': 0.970276008492569}}, 'acsc_metrics': {'ambience': {'acc': 0.9969604863221885}, 'anecdotes/miscellaneous': {'acc': 0.9617021276595744}, 'food': {'acc': 0.9930278884462151}, 'price': {'acc': 0.9961389961389961}, 'service': {'acc': 0.9978768577494692}}, 'polarity_metrics': {'negative': {'f1': 0.9845288326300984, 'precision': 0.9859154929577465, 'recall': 0.9831460674157303}, 'neutral': {'f1': 0.9645714285714286, 'precision': 0.9547511312217195, 'recall': 0.9745958429561201}, 'positive': {'f1': 0.989485036397951, 'precision': 0.9913560237709347, 'recall': 0.9876210979547901}}, 'merge_micro_f1': 0.9480583319678847}}
allennlp_callback.py-39-2021-03-18 01:54:26,998-epoch: 19 data_type: dev result: {'sentiment_acc': 0.7354709418837675, 'category_f1': {'precision': 0.5411954765751211, 'recall': 0.6713426853707415, 'fscore': 0.5992844364937387}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.3937007874015748, 'precision': 0.3472222222222222, 'recall': 0.45454545454545453}, 'anecdotes/miscellaneous': {'f1': 0.7051671732522796, 'precision': 0.6444444444444445, 'recall': 0.7785234899328859}, 'food': {'f1': 0.6594594594594595, 'precision': 0.580952380952381, 'recall': 0.7625}, 'price': {'f1': 0.47058823529411764, 'precision': 0.42105263157894735, 'recall': 0.5333333333333333}, 'service': {'f1': 0.505263157894737, 'precision': 0.48, 'recall': 0.5333333333333333}}, 'acsc_metrics': {'ambience': {'acc': 0.8}, 'anecdotes/miscellaneous': {'acc': 0.6174496644295302}, 'food': {'acc': 0.79375}, 'price': {'acc': 0.8222222222222222}, 'service': {'acc': 0.7444444444444445}}, 'polarity_metrics': {'negative': {'f1': 0.6818181818181818, 'precision': 0.656934306569343, 'recall': 0.7086614173228346}, 'neutral': {'f1': 0.4938271604938272, 'precision': 0.42105263157894735, 'recall': 0.5970149253731343}, 'positive': {'f1': 0.8286713286713288, 'precision': 0.8876404494382022, 'recall': 0.7770491803278688}}, 'merge_micro_f1': 0.4472271914132379}}
allennlp_callback.py-39-2021-03-18 01:54:51,416-epoch: 19 data_type: test result: {'sentiment_acc': 0.7882836587872559, 'category_f1': {'precision': 0.861522198731501, 'recall': 0.8376156217882836, 'fscore': 0.8494007295466388}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7596153846153846, 'precision': 0.7669902912621359, 'recall': 0.7523809523809524}, 'anecdotes/miscellaneous': {'f1': 0.7973273942093542, 'precision': 0.7782608695652173, 'recall': 0.817351598173516}, 'food': {'f1': 0.9141414141414141, 'precision': 0.9282051282051282, 'recall': 0.900497512437811}, 'price': {'f1': 0.7832167832167831, 'precision': 0.8888888888888888, 'recall': 0.7}, 'service': {'f1': 0.8501529051987767, 'precision': 0.86875, 'recall': 0.8323353293413174}}, 'acsc_metrics': {'ambience': {'acc': 0.8285714285714286}, 'anecdotes/miscellaneous': {'acc': 0.6757990867579908}, 'food': {'acc': 0.8009950248756219}, 'price': {'acc': 0.8}, 'service': {'acc': 0.874251497005988}}, 'polarity_metrics': {'negative': {'f1': 0.7346938775510203, 'precision': 0.7397260273972602, 'recall': 0.7297297297297297}, 'neutral': {'f1': 0.41441441441441446, 'precision': 0.359375, 'recall': 0.48936170212765956}, 'positive': {'f1': 0.8713951675759938, 'precision': 0.8929712460063898, 'recall': 0.8508371385083714}}, 'merge_micro_f1': 0.6826472120896301}}
allennlp_callback.py-39-2021-03-18 01:54:52,682-epoch: 19 data_type: hard_test result: {'sentiment_acc': 0.7169811320754716, 'category_f1': {'precision': 0.9166666666666666, 'recall': 0.6226415094339622, 'fscore': 0.7415730337078651}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.5454545454545454, 'precision': 1.0, 'recall': 0.375}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.9268292682926829, 'precision': 1.0, 'recall': 0.8636363636363636}, 'price': {'f1': 0.8333333333333333, 'precision': 1.0, 'recall': 0.7142857142857143}, 'service': {'f1': 0.7058823529411764, 'precision': 0.8571428571428571, 'recall': 0.6}}, 'acsc_metrics': {'ambience': {'acc': 0.75}, 'anecdotes/miscellaneous': {'acc': 0.16666666666666666}, 'food': {'acc': 0.6818181818181818}, 'price': {'acc': 1.0}, 'service': {'acc': 0.9}}, 'polarity_metrics': {'negative': {'f1': 0.8181818181818182, 'precision': 0.75, 'recall': 0.9}, 'neutral': {'f1': 0.4761904761904762, 'precision': 0.5555555555555556, 'recall': 0.4166666666666667}, 'positive': {'f1': 0.7317073170731706, 'precision': 0.75, 'recall': 0.7142857142857143}}, 'merge_micro_f1': 0.5842696629213483}}
allennlp_callback.py-39-2021-03-18 01:58:14,164-epoch: 20 data_type: train result: {'sentiment_acc': 0.9896769896769897, 'category_f1': {'precision': 0.9626939584021129, 'recall': 0.971028971028971, 'fscore': 0.9668435013262598}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.9585798816568047, 'precision': 0.9337175792507204, 'recall': 0.9848024316109423}, 'anecdotes/miscellaneous': {'f1': 0.9621621621621622, 'precision': 0.978021978021978, 'recall': 0.9468085106382979}, 'food': {'f1': 0.9838155958803336, 'precision': 0.9690821256038648, 'recall': 0.999003984063745}, 'price': {'f1': 0.9565217391304347, 'precision': 0.937037037037037, 'recall': 0.9768339768339769}, 'service': {'f1': 0.9509594882729212, 'precision': 0.9550321199143469, 'recall': 0.9469214437367304}}, 'acsc_metrics': {'ambience': {'acc': 1.0}, 'anecdotes/miscellaneous': {'acc': 0.973404255319149}, 'food': {'acc': 0.9940239043824701}, 'price': {'acc': 1.0}, 'service': {'acc': 1.0}}, 'polarity_metrics': {'negative': {'f1': 0.9895031490552835, 'precision': 0.9860529986052998, 'recall': 0.9929775280898876}, 'neutral': {'f1': 0.9753810082063306, 'precision': 0.9904761904761905, 'recall': 0.9607390300230947}, 'positive': {'f1': 0.9930182599355531, 'precision': 0.9908896034297964, 'recall': 0.9951560818083961}}, 'merge_micro_f1': 0.9565649867374006}}
allennlp_callback.py-39-2021-03-18 01:58:30,486-epoch: 20 data_type: dev result: {'sentiment_acc': 0.7695390781563126, 'category_f1': {'precision': 0.5324459234608985, 'recall': 0.6412825651302605, 'fscore': 0.5818181818181818}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.4492753623188406, 'precision': 0.37349397590361444, 'recall': 0.5636363636363636}, 'anecdotes/miscellaneous': {'f1': 0.6948051948051949, 'precision': 0.6729559748427673, 'recall': 0.7181208053691275}, 'food': {'f1': 0.6270270270270271, 'precision': 0.5523809523809524, 'recall': 0.725}, 'price': {'f1': 0.49056603773584895, 'precision': 0.4262295081967213, 'recall': 0.5777777777777777}, 'service': {'f1': 0.44943820224719094, 'precision': 0.45454545454545453, 'recall': 0.4444444444444444}}, 'acsc_metrics': {'ambience': {'acc': 0.8363636363636363}, 'anecdotes/miscellaneous': {'acc': 0.6577181208053692}, 'food': {'acc': 0.8375}, 'price': {'acc': 0.8}, 'service': {'acc': 0.7777777777777778}}, 'polarity_metrics': {'negative': {'f1': 0.7028985507246376, 'precision': 0.6510067114093959, 'recall': 0.7637795275590551}, 'neutral': {'f1': 0.5384615384615385, 'precision': 0.5555555555555556, 'recall': 0.5223880597014925}, 'positive': {'f1': 0.8513513513513514, 'precision': 0.8780487804878049, 'recall': 0.8262295081967214}}, 'merge_micro_f1': 0.4490909090909091}}
allennlp_callback.py-39-2021-03-18 01:58:54,511-epoch: 20 data_type: test result: {'sentiment_acc': 0.8078108941418294, 'category_f1': {'precision': 0.8614393125671321, 'recall': 0.8242548818088387, 'fscore': 0.842436974789916}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.743119266055046, 'precision': 0.7168141592920354, 'recall': 0.7714285714285715}, 'anecdotes/miscellaneous': {'f1': 0.7662650602409639, 'precision': 0.8112244897959183, 'recall': 0.726027397260274}, 'food': {'f1': 0.9172932330827067, 'precision': 0.9242424242424242, 'recall': 0.9104477611940298}, 'price': {'f1': 0.7862068965517242, 'precision': 0.8769230769230769, 'recall': 0.7125}, 'service': {'f1': 0.8475609756097561, 'precision': 0.8633540372670807, 'recall': 0.8323353293413174}}, 'acsc_metrics': {'ambience': {'acc': 0.819047619047619}, 'anecdotes/miscellaneous': {'acc': 0.726027397260274}, 'food': {'acc': 0.8034825870646766}, 'price': {'acc': 0.825}, 'service': {'acc': 0.9101796407185628}}, 'polarity_metrics': {'negative': {'f1': 0.7445887445887446, 'precision': 0.7166666666666667, 'recall': 0.7747747747747747}, 'neutral': {'f1': 0.4022346368715084, 'precision': 0.4235294117647059, 'recall': 0.3829787234042553}, 'positive': {'f1': 0.885823754789272, 'precision': 0.8919753086419753, 'recall': 0.8797564687975646}}, 'merge_micro_f1': 0.7058823529411764}}
allennlp_callback.py-39-2021-03-18 01:58:55,684-epoch: 20 data_type: hard_test result: {'sentiment_acc': 0.6981132075471698, 'category_f1': {'precision': 0.9722222222222222, 'recall': 0.660377358490566, 'fscore': 0.7865168539325842}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.5454545454545454, 'precision': 1.0, 'recall': 0.375}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.9268292682926829, 'precision': 1.0, 'recall': 0.8636363636363636}, 'price': {'f1': 0.8333333333333333, 'precision': 1.0, 'recall': 0.7142857142857143}, 'service': {'f1': 0.8421052631578948, 'precision': 0.8888888888888888, 'recall': 0.8}}, 'acsc_metrics': {'ambience': {'acc': 0.75}, 'anecdotes/miscellaneous': {'acc': 0.16666666666666666}, 'food': {'acc': 0.6363636363636364}, 'price': {'acc': 1.0}, 'service': {'acc': 0.9}}, 'polarity_metrics': {'negative': {'f1': 0.7659574468085106, 'precision': 0.6666666666666666, 'recall': 0.9}, 'neutral': {'f1': 0.35294117647058826, 'precision': 0.6, 'recall': 0.25}, 'positive': {'f1': 0.7619047619047619, 'precision': 0.7619047619047619, 'recall': 0.7619047619047619}}, 'merge_micro_f1': 0.651685393258427}}
allennlp_callback.py-39-2021-03-18 02:02:27,270-epoch: 21 data_type: train result: {'sentiment_acc': 0.983016983016983, 'category_f1': {'precision': 0.952240758913968, 'recall': 0.9693639693639694, 'fscore': 0.9607260726072607}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.9785276073619633, 'precision': 0.9876160990712074, 'recall': 0.9696048632218845}, 'anecdotes/miscellaneous': {'f1': 0.9309090909090909, 'precision': 0.9096446700507614, 'recall': 0.9531914893617022}, 'food': {'f1': 0.9833333333333333, 'precision': 0.9681467181467182, 'recall': 0.999003984063745}, 'price': {'f1': 0.9484126984126985, 'precision': 0.9755102040816327, 'recall': 0.9227799227799228}, 'service': {'f1': 0.9669861554845581, 'precision': 0.9700854700854701, 'recall': 0.9639065817409767}}, 'acsc_metrics': {'ambience': {'acc': 1.0}, 'anecdotes/miscellaneous': {'acc': 0.9574468085106383}, 'food': {'acc': 0.9900398406374502}, 'price': {'acc': 1.0}, 'service': {'acc': 0.9978768577494692}}, 'polarity_metrics': {'negative': {'f1': 0.9796776454099508, 'precision': 0.9776223776223776, 'recall': 0.9817415730337079}, 'neutral': {'f1': 0.9643268124280782, 'precision': 0.9610091743119266, 'recall': 0.9676674364896074}, 'positive': {'f1': 0.9886792452830189, 'precision': 0.990280777537797, 'recall': 0.9870828848223897}}, 'merge_micro_f1': 0.9442244224422442}}
allennlp_callback.py-39-2021-03-18 02:02:45,580-epoch: 21 data_type: dev result: {'sentiment_acc': 0.751503006012024, 'category_f1': {'precision': 0.5260416666666666, 'recall': 0.6072144288577155, 'fscore': 0.5637209302325582}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.35897435897435903, 'precision': 0.3387096774193548, 'recall': 0.38181818181818183}, 'anecdotes/miscellaneous': {'f1': 0.7069486404833838, 'precision': 0.6428571428571429, 'recall': 0.785234899328859}, 'food': {'f1': 0.6225895316804407, 'precision': 0.5566502463054187, 'recall': 0.70625}, 'price': {'f1': 0.3695652173913044, 'precision': 0.3617021276595745, 'recall': 0.37777777777777777}, 'service': {'f1': 0.40697674418604646, 'precision': 0.4268292682926829, 'recall': 0.3888888888888889}}, 'acsc_metrics': {'ambience': {'acc': 0.8181818181818182}, 'anecdotes/miscellaneous': {'acc': 0.6577181208053692}, 'food': {'acc': 0.79375}, 'price': {'acc': 0.8222222222222222}, 'service': {'acc': 0.7555555555555555}}, 'polarity_metrics': {'negative': {'f1': 0.6886446886446888, 'precision': 0.6438356164383562, 'recall': 0.7401574803149606}, 'neutral': {'f1': 0.5771812080536913, 'precision': 0.524390243902439, 'recall': 0.6417910447761194}, 'positive': {'f1': 0.8263888888888888, 'precision': 0.8782287822878229, 'recall': 0.780327868852459}}, 'merge_micro_f1': 0.4223255813953488}}
allennlp_callback.py-39-2021-03-18 02:03:14,683-epoch: 21 data_type: test result: {'sentiment_acc': 0.8057553956834532, 'category_f1': {'precision': 0.8492407809110629, 'recall': 0.8047276464542652, 'fscore': 0.8263852242744064}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7227722772277226, 'precision': 0.7525773195876289, 'recall': 0.6952380952380952}, 'anecdotes/miscellaneous': {'f1': 0.752783964365256, 'precision': 0.7347826086956522, 'recall': 0.771689497716895}, 'food': {'f1': 0.9084065244667503, 'precision': 0.9164556962025316, 'recall': 0.900497512437811}, 'price': {'f1': 0.703125, 'precision': 0.9375, 'recall': 0.5625}, 'service': {'f1': 0.8401253918495297, 'precision': 0.881578947368421, 'recall': 0.8023952095808383}}, 'acsc_metrics': {'ambience': {'acc': 0.8380952380952381}, 'anecdotes/miscellaneous': {'acc': 0.6986301369863014}, 'food': {'acc': 0.8283582089552238}, 'price': {'acc': 0.8125}, 'service': {'acc': 0.8682634730538922}}, 'polarity_metrics': {'negative': {'f1': 0.734966592427617, 'precision': 0.7268722466960352, 'recall': 0.7432432432432432}, 'neutral': {'f1': 0.4321608040201005, 'precision': 0.4095238095238095, 'recall': 0.4574468085106383}, 'positive': {'f1': 0.8875192604006163, 'precision': 0.8985959438377535, 'recall': 0.8767123287671232}}, 'merge_micro_f1': 0.6754617414248021}}
allennlp_callback.py-39-2021-03-18 02:03:16,478-epoch: 21 data_type: hard_test result: {'sentiment_acc': 0.7358490566037735, 'category_f1': {'precision': 0.9444444444444444, 'recall': 0.6415094339622641, 'fscore': 0.7640449438202246}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.5454545454545454, 'precision': 1.0, 'recall': 0.375}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.9268292682926829, 'precision': 1.0, 'recall': 0.8636363636363636}, 'price': {'f1': 0.8333333333333333, 'precision': 1.0, 'recall': 0.7142857142857143}, 'service': {'f1': 0.7777777777777777, 'precision': 0.875, 'recall': 0.7}}, 'acsc_metrics': {'ambience': {'acc': 0.75}, 'anecdotes/miscellaneous': {'acc': 0.3333333333333333}, 'food': {'acc': 0.7272727272727273}, 'price': {'acc': 0.8571428571428571}, 'service': {'acc': 0.9}}, 'polarity_metrics': {'negative': {'f1': 0.7727272727272727, 'precision': 0.7083333333333334, 'recall': 0.85}, 'neutral': {'f1': 0.5454545454545454, 'precision': 0.6, 'recall': 0.5}, 'positive': {'f1': 0.8, 'precision': 0.8421052631578947, 'recall': 0.7619047619047619}}, 'merge_micro_f1': 0.6067415730337078}}
allennlp_callback.py-39-2021-03-18 02:07:12,245-epoch: 22 data_type: train result: {'sentiment_acc': 0.995004995004995, 'category_f1': {'precision': 0.9904955872369314, 'recall': 0.9716949716949717, 'fscore': 0.9810052109598252}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.9877300613496933, 'precision': 0.9969040247678018, 'recall': 0.9787234042553191}, 'anecdotes/miscellaneous': {'f1': 0.985042735042735, 'precision': 0.9892703862660944, 'recall': 0.9808510638297873}, 'food': {'f1': 0.9880358923230309, 'precision': 0.9890219560878244, 'recall': 0.9870517928286853}, 'price': {'f1': 0.9664694280078895, 'precision': 0.9879032258064516, 'recall': 0.9459459459459459}, 'service': {'f1': 0.9605263157894738, 'precision': 0.9931972789115646, 'recall': 0.9299363057324841}}, 'acsc_metrics': {'ambience': {'acc': 1.0}, 'anecdotes/miscellaneous': {'acc': 0.9925531914893617}, 'food': {'acc': 0.9920318725099602}, 'price': {'acc': 1.0}, 'service': {'acc': 1.0}}, 'polarity_metrics': {'negative': {'f1': 0.9971870604781997, 'precision': 0.9985915492957746, 'recall': 0.9957865168539326}, 'neutral': {'f1': 0.9859484777517564, 'precision': 1.0, 'recall': 0.9722863741339491}, 'positive': {'f1': 0.9962466487935657, 'precision': 0.9925213675213675, 'recall': 1.0}}, 'merge_micro_f1': 0.9759623466128762}}
allennlp_callback.py-39-2021-03-18 02:07:30,304-epoch: 22 data_type: dev result: {'sentiment_acc': 0.7855711422845691, 'category_f1': {'precision': 0.5300751879699248, 'recall': 0.5651302605210421, 'fscore': 0.5470417070805043}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.3448275862068965, 'precision': 0.32786885245901637, 'recall': 0.36363636363636365}, 'anecdotes/miscellaneous': {'f1': 0.7215189873417721, 'precision': 0.6826347305389222, 'recall': 0.7651006711409396}, 'food': {'f1': 0.5964912280701755, 'precision': 0.5604395604395604, 'recall': 0.6375}, 'price': {'f1': 0.3695652173913044, 'precision': 0.3617021276595745, 'recall': 0.37777777777777777}, 'service': {'f1': 0.35151515151515156, 'precision': 0.38666666666666666, 'recall': 0.32222222222222224}}, 'acsc_metrics': {'ambience': {'acc': 0.8181818181818182}, 'anecdotes/miscellaneous': {'acc': 0.6778523489932886}, 'food': {'acc': 0.84375}, 'price': {'acc': 0.8444444444444444}, 'service': {'acc': 0.8111111111111111}}, 'polarity_metrics': {'negative': {'f1': 0.7294117647058824, 'precision': 0.7265625, 'recall': 0.7322834645669292}, 'neutral': {'f1': 0.5606060606060606, 'precision': 0.5692307692307692, 'recall': 0.5522388059701493}, 'positive': {'f1': 0.8576104746317511, 'precision': 0.8562091503267973, 'recall': 0.8590163934426229}}, 'merge_micro_f1': 0.43452958292919497}}
allennlp_callback.py-39-2021-03-18 02:07:58,574-epoch: 22 data_type: test result: {'sentiment_acc': 0.8191161356628982, 'category_f1': {'precision': 0.8853658536585366, 'recall': 0.7461459403905447, 'fscore': 0.8098159509202454}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.73015873015873, 'precision': 0.8214285714285714, 'recall': 0.6571428571428571}, 'anecdotes/miscellaneous': {'f1': 0.7952941176470588, 'precision': 0.8203883495145631, 'recall': 0.771689497716895}, 'food': {'f1': 0.8716577540106952, 'precision': 0.9421965317919075, 'recall': 0.8109452736318408}, 'price': {'f1': 0.6559999999999999, 'precision': 0.9111111111111111, 'recall': 0.5125}, 'service': {'f1': 0.7908496732026143, 'precision': 0.8705035971223022, 'recall': 0.7245508982035929}}, 'acsc_metrics': {'ambience': {'acc': 0.8476190476190476}, 'anecdotes/miscellaneous': {'acc': 0.7214611872146118}, 'food': {'acc': 0.8333333333333334}, 'price': {'acc': 0.825}, 'service': {'acc': 0.8922155688622755}}, 'polarity_metrics': {'negative': {'f1': 0.7378190255220418, 'precision': 0.7607655502392344, 'recall': 0.7162162162162162}, 'neutral': {'f1': 0.4363636363636364, 'precision': 0.5070422535211268, 'recall': 0.3829787234042553}, 'positive': {'f1': 0.891851851851852, 'precision': 0.8686868686868687, 'recall': 0.9162861491628614}}, 'merge_micro_f1': 0.6793084216397098}}
allennlp_callback.py-39-2021-03-18 02:08:00,273-epoch: 22 data_type: hard_test result: {'sentiment_acc': 0.6981132075471698, 'category_f1': {'precision': 0.9375, 'recall': 0.5660377358490566, 'fscore': 0.7058823529411765}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.4, 'precision': 1.0, 'recall': 0.25}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.8717948717948718, 'precision': 1.0, 'recall': 0.7727272727272727}, 'price': {'f1': 0.8333333333333333, 'precision': 1.0, 'recall': 0.7142857142857143}, 'service': {'f1': 0.6666666666666665, 'precision': 0.75, 'recall': 0.6}}, 'acsc_metrics': {'ambience': {'acc': 0.75}, 'anecdotes/miscellaneous': {'acc': 0.16666666666666666}, 'food': {'acc': 0.6363636363636364}, 'price': {'acc': 1.0}, 'service': {'acc': 0.9}}, 'polarity_metrics': {'negative': {'f1': 0.7826086956521738, 'precision': 0.6923076923076923, 'recall': 0.9}, 'neutral': {'f1': 0.26666666666666666, 'precision': 0.6666666666666666, 'recall': 0.16666666666666666}, 'positive': {'f1': 0.7555555555555556, 'precision': 0.7083333333333334, 'recall': 0.8095238095238095}}, 'merge_micro_f1': 0.5647058823529413}}
allennlp_callback.py-39-2021-03-18 02:12:44,366-epoch: 23 data_type: train result: {'sentiment_acc': 0.9983349983349983, 'category_f1': {'precision': 0.994634473507713, 'recall': 0.9876789876789877, 'fscore': 0.9911445279866332}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.9969604863221885, 'precision': 0.9969604863221885, 'recall': 0.9969604863221885}, 'anecdotes/miscellaneous': {'f1': 0.9946695095948828, 'precision': 0.9967948717948718, 'recall': 0.9925531914893617}, 'food': {'f1': 0.9940357852882704, 'precision': 0.9920634920634921, 'recall': 0.9960159362549801}, 'price': {'f1': 0.9600000000000001, 'precision': 0.995850622406639, 'recall': 0.9266409266409267}, 'service': {'f1': 0.9904153354632588, 'precision': 0.9935897435897436, 'recall': 0.9872611464968153}}, 'acsc_metrics': {'ambience': {'acc': 1.0}, 'anecdotes/miscellaneous': {'acc': 0.9957446808510638}, 'food': {'acc': 0.999003984063745}, 'price': {'acc': 1.0}, 'service': {'acc': 1.0}}, 'polarity_metrics': {'negative': {'f1': 0.9978888106966924, 'precision': 1.0, 'recall': 0.9957865168539326}, 'neutral': {'f1': 0.9976905311778291, 'precision': 0.9976905311778291, 'recall': 0.9976905311778291}, 'positive': {'f1': 0.9986555525678945, 'precision': 0.9978506179473401, 'recall': 0.9994617868675996}}, 'merge_micro_f1': 0.9894736842105264}}
allennlp_callback.py-39-2021-03-18 02:13:11,920-epoch: 23 data_type: dev result: {'sentiment_acc': 0.7775551102204409, 'category_f1': {'precision': 0.5216572504708098, 'recall': 0.5551102204408818, 'fscore': 0.5378640776699029}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.3559322033898305, 'precision': 0.3333333333333333, 'recall': 0.38181818181818183}, 'anecdotes/miscellaneous': {'f1': 0.7096774193548386, 'precision': 0.6832298136645962, 'recall': 0.738255033557047}, 'food': {'f1': 0.584070796460177, 'precision': 0.553072625698324, 'recall': 0.61875}, 'price': {'f1': 0.3478260869565218, 'precision': 0.3404255319148936, 'recall': 0.35555555555555557}, 'service': {'f1': 0.3625730994152046, 'precision': 0.38271604938271603, 'recall': 0.34444444444444444}}, 'acsc_metrics': {'ambience': {'acc': 0.8181818181818182}, 'anecdotes/miscellaneous': {'acc': 0.6577181208053692}, 'food': {'acc': 0.8375}, 'price': {'acc': 0.8444444444444444}, 'service': {'acc': 0.8111111111111111}}, 'polarity_metrics': {'negative': {'f1': 0.714859437751004, 'precision': 0.7295081967213115, 'recall': 0.7007874015748031}, 'neutral': {'f1': 0.5416666666666666, 'precision': 0.5064935064935064, 'recall': 0.582089552238806}, 'positive': {'f1': 0.8595041322314049, 'precision': 0.8666666666666667, 'recall': 0.8524590163934426}}, 'merge_micro_f1': 0.42524271844660194}}
allennlp_callback.py-39-2021-03-18 02:13:48,200-epoch: 23 data_type: test result: {'sentiment_acc': 0.8047276464542652, 'category_f1': {'precision': 0.8741092636579573, 'recall': 0.7564234326824255, 'fscore': 0.8110192837465565}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7586206896551724, 'precision': 0.7857142857142857, 'recall': 0.7333333333333333}, 'anecdotes/miscellaneous': {'f1': 0.7706855791962175, 'precision': 0.7990196078431373, 'recall': 0.7442922374429224}, 'food': {'f1': 0.8788282290279628, 'precision': 0.9455587392550143, 'recall': 0.8208955223880597}, 'price': {'f1': 0.6229508196721311, 'precision': 0.9047619047619048, 'recall': 0.475}, 'service': {'f1': 0.810126582278481, 'precision': 0.8590604026845637, 'recall': 0.7664670658682635}}, 'acsc_metrics': {'ambience': {'acc': 0.8380952380952381}, 'anecdotes/miscellaneous': {'acc': 0.7123287671232876}, 'food': {'acc': 0.8159203980099502}, 'price': {'acc': 0.8125}, 'service': {'acc': 0.874251497005988}}, 'polarity_metrics': {'negative': {'f1': 0.7190476190476192, 'precision': 0.7626262626262627, 'recall': 0.6801801801801802}, 'neutral': {'f1': 0.44329896907216493, 'precision': 0.43, 'recall': 0.4574468085106383}, 'positive': {'f1': 0.8843843843843844, 'precision': 0.8725925925925926, 'recall': 0.8964992389649924}}, 'merge_micro_f1': 0.6633608815426998}}
allennlp_callback.py-39-2021-03-18 02:13:50,372-epoch: 23 data_type: hard_test result: {'sentiment_acc': 0.7169811320754716, 'category_f1': {'precision': 0.9354838709677419, 'recall': 0.5471698113207547, 'fscore': 0.6904761904761905}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.5454545454545454, 'precision': 1.0, 'recall': 0.375}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.8108108108108109, 'precision': 1.0, 'recall': 0.6818181818181818}, 'price': {'f1': 0.8333333333333333, 'precision': 1.0, 'recall': 0.7142857142857143}, 'service': {'f1': 0.6666666666666665, 'precision': 0.75, 'recall': 0.6}}, 'acsc_metrics': {'ambience': {'acc': 0.75}, 'anecdotes/miscellaneous': {'acc': 0.16666666666666666}, 'food': {'acc': 0.6818181818181818}, 'price': {'acc': 1.0}, 'service': {'acc': 0.9}}, 'polarity_metrics': {'negative': {'f1': 0.7826086956521738, 'precision': 0.6923076923076923, 'recall': 0.9}, 'neutral': {'f1': 0.4210526315789474, 'precision': 0.5714285714285714, 'recall': 0.3333333333333333}, 'positive': {'f1': 0.7804878048780488, 'precision': 0.8, 'recall': 0.7619047619047619}}, 'merge_micro_f1': 0.5476190476190477}}
allennlp_callback.py-39-2021-03-18 02:18:46,254-epoch: 24 data_type: train result: {'sentiment_acc': 0.993006993006993, 'category_f1': {'precision': 0.993238674780257, 'recall': 0.9783549783549783, 'fscore': 0.9857406475423588}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.9969604863221885, 'precision': 0.9969604863221885, 'recall': 0.9969604863221885}, 'anecdotes/miscellaneous': {'f1': 0.9755567626290059, 'precision': 0.9966703662597114, 'recall': 0.9553191489361702}, 'food': {'f1': 0.9914872308462694, 'precision': 0.9969788519637462, 'recall': 0.9860557768924303}, 'price': {'f1': 0.9865125240847784, 'precision': 0.9846153846153847, 'recall': 0.9884169884169884}, 'service': {'f1': 0.9852008456659619, 'precision': 0.9810526315789474, 'recall': 0.9893842887473461}}, 'acsc_metrics': {'ambience': {'acc': 1.0}, 'anecdotes/miscellaneous': {'acc': 0.9861702127659574}, 'food': {'acc': 0.9920318725099602}, 'price': {'acc': 1.0}, 'service': {'acc': 1.0}}, 'polarity_metrics': {'negative': {'f1': 0.9909281228192602, 'precision': 0.984743411927878, 'recall': 0.9971910112359551}, 'neutral': {'f1': 0.9811764705882352, 'precision': 1.0, 'recall': 0.9630484988452656}, 'positive': {'f1': 0.9965081923180231, 'precision': 0.9946380697050938, 'recall': 0.9983853606027987}}, 'merge_micro_f1': 0.9790303640328802}}
allennlp_callback.py-39-2021-03-18 02:19:11,683-epoch: 24 data_type: dev result: {'sentiment_acc': 0.7935871743486974, 'category_f1': {'precision': 0.5130111524163569, 'recall': 0.5531062124248497, 'fscore': 0.532304725168756}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.34426229508196726, 'precision': 0.31343283582089554, 'recall': 0.38181818181818183}, 'anecdotes/miscellaneous': {'f1': 0.6847457627118645, 'precision': 0.6917808219178082, 'recall': 0.6778523489932886}, 'food': {'f1': 0.5798816568047338, 'precision': 0.550561797752809, 'recall': 0.6125}, 'price': {'f1': 0.39215686274509803, 'precision': 0.3508771929824561, 'recall': 0.4444444444444444}, 'service': {'f1': 0.4000000000000001, 'precision': 0.4, 'recall': 0.4}}, 'acsc_metrics': {'ambience': {'acc': 0.8181818181818182}, 'anecdotes/miscellaneous': {'acc': 0.6845637583892618}, 'food': {'acc': 0.8625}, 'price': {'acc': 0.8444444444444444}, 'service': {'acc': 0.8111111111111111}}, 'polarity_metrics': {'negative': {'f1': 0.73992673992674, 'precision': 0.6917808219178082, 'recall': 0.7952755905511811}, 'neutral': {'f1': 0.5689655172413793, 'precision': 0.673469387755102, 'recall': 0.4925373134328358}, 'positive': {'f1': 0.8604269293924466, 'precision': 0.8618421052631579, 'recall': 0.8590163934426229}}, 'merge_micro_f1': 0.42430086788813887}}
allennlp_callback.py-39-2021-03-18 02:19:48,207-epoch: 24 data_type: test result: {'sentiment_acc': 0.8232271325796505, 'category_f1': {'precision': 0.8853046594982079, 'recall': 0.7615621788283659, 'fscore': 0.8187845303867404}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7632850241545894, 'precision': 0.7745098039215687, 'recall': 0.7523809523809524}, 'anecdotes/miscellaneous': {'f1': 0.7645569620253164, 'precision': 0.8579545454545454, 'recall': 0.6894977168949772}, 'food': {'f1': 0.8559782608695653, 'precision': 0.9431137724550899, 'recall': 0.7835820895522388}, 'price': {'f1': 0.7857142857142857, 'precision': 0.9166666666666666, 'recall': 0.6875}, 'service': {'f1': 0.8493975903614458, 'precision': 0.8545454545454545, 'recall': 0.844311377245509}}, 'acsc_metrics': {'ambience': {'acc': 0.8380952380952381}, 'anecdotes/miscellaneous': {'acc': 0.7351598173515982}, 'food': {'acc': 0.845771144278607}, 'price': {'acc': 0.825}, 'service': {'acc': 0.874251497005988}}, 'polarity_metrics': {'negative': {'f1': 0.7433628318584071, 'precision': 0.7304347826086957, 'recall': 0.7567567567567568}, 'neutral': {'f1': 0.4313725490196078, 'precision': 0.559322033898305, 'recall': 0.35106382978723405}, 'positive': {'f1': 0.8948545861297539, 'precision': 0.8771929824561403, 'recall': 0.91324200913242}}, 'merge_micro_f1': 0.6961325966850829}}
allennlp_callback.py-39-2021-03-18 02:19:50,376-epoch: 24 data_type: hard_test result: {'sentiment_acc': 0.6981132075471698, 'category_f1': {'precision': 0.9411764705882353, 'recall': 0.6037735849056604, 'fscore': 0.735632183908046}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.5454545454545454, 'precision': 1.0, 'recall': 0.375}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.8421052631578948, 'precision': 1.0, 'recall': 0.7272727272727273}, 'price': {'f1': 0.923076923076923, 'precision': 1.0, 'recall': 0.8571428571428571}, 'service': {'f1': 0.7368421052631577, 'precision': 0.7777777777777778, 'recall': 0.7}}, 'acsc_metrics': {'ambience': {'acc': 0.75}, 'anecdotes/miscellaneous': {'acc': 0.16666666666666666}, 'food': {'acc': 0.6363636363636364}, 'price': {'acc': 1.0}, 'service': {'acc': 0.9}}, 'polarity_metrics': {'negative': {'f1': 0.75, 'precision': 0.6428571428571429, 'recall': 0.9}, 'neutral': {'f1': 0.375, 'precision': 0.75, 'recall': 0.25}, 'positive': {'f1': 0.7619047619047619, 'precision': 0.7619047619047619, 'recall': 0.7619047619047619}}, 'merge_micro_f1': 0.5977011494252874}}
allennlp_callback.py-39-2021-03-18 02:24:54,292-epoch: 25 data_type: train result: {'sentiment_acc': 0.9896769896769897, 'category_f1': {'precision': 0.9502219403931516, 'recall': 0.998001998001998, 'fscore': 0.9735260678902062}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.9865067466266867, 'precision': 0.9733727810650887, 'recall': 1.0}, 'anecdotes/miscellaneous': {'f1': 0.9822361546499477, 'precision': 0.9650924024640657, 'recall': 1.0}, 'food': {'f1': 0.9598470363288719, 'precision': 0.9227941176470589, 'recall': 1.0}, 'price': {'f1': 0.9866666666666666, 'precision': 0.9736842105263158, 'recall': 1.0}, 'service': {'f1': 0.969760166840459, 'precision': 0.9528688524590164, 'recall': 0.9872611464968153}}, 'acsc_metrics': {'ambience': {'acc': 1.0}, 'anecdotes/miscellaneous': {'acc': 0.9872340425531915}, 'food': {'acc': 0.9810756972111554}, 'price': {'acc': 1.0}, 'service': {'acc': 1.0}}, 'polarity_metrics': {'negative': {'f1': 0.9872340425531915, 'precision': 0.997134670487106, 'recall': 0.9775280898876404}, 'neutral': {'f1': 0.9804372842347527, 'precision': 0.9770642201834863, 'recall': 0.9838337182448037}, 'positive': {'f1': 0.9927555674805474, 'precision': 0.9898341359015517, 'recall': 0.9956942949407965}}, 'merge_micro_f1': 0.9634562286828}}
allennlp_callback.py-39-2021-03-18 02:25:18,398-epoch: 25 data_type: dev result: {'sentiment_acc': 0.7635270541082164, 'category_f1': {'precision': 0.5283018867924528, 'recall': 0.6733466933867736, 'fscore': 0.5920704845814977}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.4186046511627907, 'precision': 0.36486486486486486, 'recall': 0.4909090909090909}, 'anecdotes/miscellaneous': {'f1': 0.6871165644171779, 'precision': 0.632768361581921, 'recall': 0.7516778523489933}, 'food': {'f1': 0.6666666666666665, 'precision': 0.559322033898305, 'recall': 0.825}, 'price': {'f1': 0.38461538461538464, 'precision': 0.3389830508474576, 'recall': 0.4444444444444444}, 'service': {'f1': 0.5, 'precision': 0.5, 'recall': 0.5}}, 'acsc_metrics': {'ambience': {'acc': 0.8181818181818182}, 'anecdotes/miscellaneous': {'acc': 0.6375838926174496}, 'food': {'acc': 0.8125}, 'price': {'acc': 0.8444444444444444}, 'service': {'acc': 0.8111111111111111}}, 'polarity_metrics': {'negative': {'f1': 0.7020408163265306, 'precision': 0.7288135593220338, 'recall': 0.6771653543307087}, 'neutral': {'f1': 0.5174825174825174, 'precision': 0.4868421052631579, 'recall': 0.5522388059701493}, 'positive': {'f1': 0.8459016393442624, 'precision': 0.8459016393442623, 'recall': 0.8459016393442623}}, 'merge_micro_f1': 0.4581497797356828}}
allennlp_callback.py-39-2021-03-18 02:25:53,589-epoch: 25 data_type: test result: {'sentiment_acc': 0.7903391572456321, 'category_f1': {'precision': 0.8409090909090909, 'recall': 0.8746145940390545, 'fscore': 0.8574307304785894}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7802690582959643, 'precision': 0.7372881355932204, 'recall': 0.8285714285714286}, 'anecdotes/miscellaneous': {'f1': 0.7456140350877192, 'precision': 0.7172995780590717, 'recall': 0.776255707762557}, 'food': {'f1': 0.9334945586457074, 'precision': 0.908235294117647, 'recall': 0.9601990049751243}, 'price': {'f1': 0.8609271523178808, 'precision': 0.9154929577464789, 'recall': 0.8125}, 'service': {'f1': 0.8719512195121951, 'precision': 0.8881987577639752, 'recall': 0.8562874251497006}}, 'acsc_metrics': {'ambience': {'acc': 0.8095238095238095}, 'anecdotes/miscellaneous': {'acc': 0.7031963470319634}, 'food': {'acc': 0.7935323383084577}, 'price': {'acc': 0.825}, 'service': {'acc': 0.8682634730538922}}, 'polarity_metrics': {'negative': {'f1': 0.6965174129353233, 'precision': 0.7777777777777778, 'recall': 0.6306306306306306}, 'neutral': {'f1': 0.4059405940594059, 'precision': 0.37962962962962965, 'recall': 0.43617021276595747}, 'positive': {'f1': 0.8763040238450075, 'precision': 0.8583941605839416, 'recall': 0.8949771689497716}}, 'merge_micro_f1': 0.6821158690176322}}
allennlp_callback.py-39-2021-03-18 02:25:56,091-epoch: 25 data_type: hard_test result: {'sentiment_acc': 0.7358490566037735, 'category_f1': {'precision': 0.925, 'recall': 0.6981132075471698, 'fscore': 0.7956989247311829}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.5454545454545454, 'precision': 1.0, 'recall': 0.375}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.9767441860465117, 'precision': 1.0, 'recall': 0.9545454545454546}, 'price': {'f1': 0.923076923076923, 'precision': 1.0, 'recall': 0.8571428571428571}, 'service': {'f1': 0.7777777777777777, 'precision': 0.875, 'recall': 0.7}}, 'acsc_metrics': {'ambience': {'acc': 0.75}, 'anecdotes/miscellaneous': {'acc': 0.16666666666666666}, 'food': {'acc': 0.7727272727272727}, 'price': {'acc': 1.0}, 'service': {'acc': 0.8}}, 'polarity_metrics': {'negative': {'f1': 0.8205128205128205, 'precision': 0.8421052631578947, 'recall': 0.8}, 'neutral': {'f1': 0.5263157894736842, 'precision': 0.7142857142857143, 'recall': 0.4166666666666667}, 'positive': {'f1': 0.75, 'precision': 0.6666666666666666, 'recall': 0.8571428571428571}}, 'merge_micro_f1': 0.6236559139784946}}
allennlp_callback.py-39-2021-03-18 02:31:05,705-epoch: 26 data_type: train result: {'sentiment_acc': 0.9663669663669664, 'category_f1': {'precision': 0.9591977869986169, 'recall': 0.9237429237429238, 'fscore': 0.9411365564037321}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.9601226993865031, 'precision': 0.9690402476780186, 'recall': 0.9513677811550152}, 'anecdotes/miscellaneous': {'f1': 0.957622454595487, 'precision': 0.992018244013683, 'recall': 0.925531914893617}, 'food': {'f1': 0.9623103279490944, 'precision': 0.9461020211742059, 'recall': 0.9790836653386454}, 'price': {'f1': 0.8134831460674158, 'precision': 0.9731182795698925, 'recall': 0.6988416988416989}, 'service': {'f1': 0.9104477611940298, 'precision': 0.9143468950749465, 'recall': 0.9065817409766455}}, 'acsc_metrics': {'ambience': {'acc': 0.9908814589665653}, 'anecdotes/miscellaneous': {'acc': 0.9531914893617022}, 'food': {'acc': 0.9810756972111554}, 'price': {'acc': 0.9806949806949807}, 'service': {'acc': 0.9363057324840764}}, 'polarity_metrics': {'negative': {'f1': 0.9563380281690141, 'precision': 0.9590395480225988, 'recall': 0.9536516853932584}, 'neutral': {'f1': 0.9316909294512878, 'precision': 0.9043478260869565, 'recall': 0.9607390300230947}, 'positive': {'f1': 0.9786081776333604, 'precision': 0.9847411444141689, 'recall': 0.972551130247578}}, 'merge_micro_f1': 0.9095843935538592}}
allennlp_callback.py-39-2021-03-18 02:31:31,012-epoch: 26 data_type: dev result: {'sentiment_acc': 0.7775551102204409, 'category_f1': {'precision': 0.5569395017793595, 'recall': 0.627254509018036, 'fscore': 0.590009425070688}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.43749999999999994, 'precision': 0.3835616438356164, 'recall': 0.509090909090909}, 'anecdotes/miscellaneous': {'f1': 0.6735395189003436, 'precision': 0.6901408450704225, 'recall': 0.6577181208053692}, 'food': {'f1': 0.6541554959785523, 'precision': 0.5727699530516432, 'recall': 0.7625}, 'price': {'f1': 0.3376623376623376, 'precision': 0.40625, 'recall': 0.28888888888888886}, 'service': {'f1': 0.5416666666666666, 'precision': 0.5098039215686274, 'recall': 0.5777777777777777}}, 'acsc_metrics': {'ambience': {'acc': 0.8181818181818182}, 'anecdotes/miscellaneous': {'acc': 0.6711409395973155}, 'food': {'acc': 0.8375}, 'price': {'acc': 0.8444444444444444}, 'service': {'acc': 0.7888888888888889}}, 'polarity_metrics': {'negative': {'f1': 0.7309236947791163, 'precision': 0.7459016393442623, 'recall': 0.7165354330708661}, 'neutral': {'f1': 0.49333333333333335, 'precision': 0.4457831325301205, 'recall': 0.5522388059701493}, 'positive': {'f1': 0.8681135225375626, 'precision': 0.8843537414965986, 'recall': 0.8524590163934426}}, 'merge_micro_f1': 0.47502356267672}}
allennlp_callback.py-39-2021-03-18 02:32:08,194-epoch: 26 data_type: test result: {'sentiment_acc': 0.7913669064748201, 'category_f1': {'precision': 0.8722415795586528, 'recall': 0.7718396711202467, 'fscore': 0.8189749182115594}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7200000000000001, 'precision': 0.7578947368421053, 'recall': 0.6857142857142857}, 'anecdotes/miscellaneous': {'f1': 0.7474747474747473, 'precision': 0.8361581920903954, 'recall': 0.6757990867579908}, 'food': {'f1': 0.8978562421185373, 'precision': 0.9104859335038363, 'recall': 0.8855721393034826}, 'price': {'f1': 0.6153846153846154, 'precision': 0.972972972972973, 'recall': 0.45}, 'service': {'f1': 0.8475609756097561, 'precision': 0.8633540372670807, 'recall': 0.8323353293413174}}, 'acsc_metrics': {'ambience': {'acc': 0.8095238095238095}, 'anecdotes/miscellaneous': {'acc': 0.6986301369863014}, 'food': {'acc': 0.8034825870646766}, 'price': {'acc': 0.8}, 'service': {'acc': 0.8682634730538922}}, 'polarity_metrics': {'negative': {'f1': 0.687960687960688, 'precision': 0.7567567567567568, 'recall': 0.6306306306306306}, 'neutral': {'f1': 0.4019607843137255, 'precision': 0.37272727272727274, 'recall': 0.43617021276595747}, 'positive': {'f1': 0.8823970037453184, 'precision': 0.8687315634218289, 'recall': 0.8964992389649924}}, 'merge_micro_f1': 0.6782988004362049}}
allennlp_callback.py-39-2021-03-18 02:32:09,982-epoch: 26 data_type: hard_test result: {'sentiment_acc': 0.7169811320754716, 'category_f1': {'precision': 0.9666666666666667, 'recall': 0.5471698113207547, 'fscore': 0.6987951807228916}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.5454545454545454, 'precision': 1.0, 'recall': 0.375}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.8717948717948718, 'precision': 1.0, 'recall': 0.7727272727272727}, 'price': {'f1': 0.6, 'precision': 1.0, 'recall': 0.42857142857142855}, 'service': {'f1': 0.7058823529411764, 'precision': 0.8571428571428571, 'recall': 0.6}}, 'acsc_metrics': {'ambience': {'acc': 0.75}, 'anecdotes/miscellaneous': {'acc': 0.16666666666666666}, 'food': {'acc': 0.7272727272727273}, 'price': {'acc': 0.8571428571428571}, 'service': {'acc': 0.9}}, 'polarity_metrics': {'negative': {'f1': 0.761904761904762, 'precision': 0.7272727272727273, 'recall': 0.8}, 'neutral': {'f1': 0.5714285714285715, 'precision': 0.6666666666666666, 'recall': 0.5}, 'positive': {'f1': 0.7441860465116279, 'precision': 0.7272727272727273, 'recall': 0.7619047619047619}}, 'merge_micro_f1': 0.5542168674698795}}
allennlp_callback.py-39-2021-03-18 02:37:11,877-epoch: 27 data_type: train result: {'sentiment_acc': 0.9713619713619713, 'category_f1': {'precision': 0.9593253968253969, 'recall': 0.9660339660339661, 'fscore': 0.9626679940268792}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.9553158705701079, 'precision': 0.96875, 'recall': 0.9422492401215805}, 'anecdotes/miscellaneous': {'f1': 0.9749333333333332, 'precision': 0.9775401069518717, 'recall': 0.9723404255319149}, 'food': {'f1': 0.9801390268123138, 'precision': 0.9772277227722772, 'recall': 0.9830677290836654}, 'price': {'f1': 0.8908145580589255, 'precision': 0.8081761006289309, 'recall': 0.9922779922779923}, 'service': {'f1': 0.9495614035087719, 'precision': 0.981859410430839, 'recall': 0.9193205944798302}}, 'acsc_metrics': {'ambience': {'acc': 0.9878419452887538}, 'anecdotes/miscellaneous': {'acc': 0.9617021276595744}, 'food': {'acc': 0.9760956175298805}, 'price': {'acc': 0.9652509652509652}, 'service': {'acc': 0.9723991507430998}}, 'polarity_metrics': {'negative': {'f1': 0.9524506217995611, 'precision': 0.9938931297709923, 'recall': 0.9143258426966292}, 'neutral': {'f1': 0.967741935483871, 'precision': 0.9655172413793104, 'recall': 0.9699769053117783}, 'positive': {'f1': 0.9790506496950411, 'precision': 0.9649764767381077, 'recall': 0.9935414424111948}}, 'merge_micro_f1': 0.9354571096731374}}
allennlp_callback.py-39-2021-03-18 02:37:37,007-epoch: 27 data_type: dev result: {'sentiment_acc': 0.781563126252505, 'category_f1': {'precision': 0.5418118466898955, 'recall': 0.6232464929859719, 'fscore': 0.5796831314072692}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.38983050847457623, 'precision': 0.36507936507936506, 'recall': 0.41818181818181815}, 'anecdotes/miscellaneous': {'f1': 0.6905537459283387, 'precision': 0.6708860759493671, 'recall': 0.7114093959731543}, 'food': {'f1': 0.6330532212885155, 'precision': 0.5736040609137056, 'recall': 0.70625}, 'price': {'f1': 0.5691056910569106, 'precision': 0.44871794871794873, 'recall': 0.7777777777777778}, 'service': {'f1': 0.40476190476190477, 'precision': 0.4358974358974359, 'recall': 0.37777777777777777}}, 'acsc_metrics': {'ambience': {'acc': 0.8181818181818182}, 'anecdotes/miscellaneous': {'acc': 0.6644295302013423}, 'food': {'acc': 0.8375}, 'price': {'acc': 0.8444444444444444}, 'service': {'acc': 0.8222222222222222}}, 'polarity_metrics': {'negative': {'f1': 0.6978723404255319, 'precision': 0.7592592592592593, 'recall': 0.6456692913385826}, 'neutral': {'f1': 0.5230769230769231, 'precision': 0.5396825396825397, 'recall': 0.5074626865671642}, 'positive': {'f1': 0.8657187993680885, 'precision': 0.8353658536585366, 'recall': 0.898360655737705}}, 'merge_micro_f1': 0.47530288909599255}}
allennlp_callback.py-39-2021-03-18 02:38:14,196-epoch: 27 data_type: test result: {'sentiment_acc': 0.789311408016444, 'category_f1': {'precision': 0.8682766190998902, 'recall': 0.8129496402877698, 'fscore': 0.8397027600849257}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7523809523809524, 'precision': 0.7523809523809524, 'recall': 0.7523809523809524}, 'anecdotes/miscellaneous': {'f1': 0.7714285714285715, 'precision': 0.8059701492537313, 'recall': 0.7397260273972602}, 'food': {'f1': 0.8929032258064515, 'precision': 0.9276139410187667, 'recall': 0.8606965174129353}, 'price': {'f1': 0.9047619047619048, 'precision': 0.8636363636363636, 'recall': 0.95}, 'service': {'f1': 0.8231511254019293, 'precision': 0.8888888888888888, 'recall': 0.7664670658682635}}, 'acsc_metrics': {'ambience': {'acc': 0.8285714285714286}, 'anecdotes/miscellaneous': {'acc': 0.684931506849315}, 'food': {'acc': 0.8208955223880597}, 'price': {'acc': 0.7375}, 'service': {'acc': 0.8502994011976048}}, 'polarity_metrics': {'negative': {'f1': 0.6631299734748011, 'precision': 0.8064516129032258, 'recall': 0.5630630630630631}, 'neutral': {'f1': 0.4043715846994535, 'precision': 0.4157303370786517, 'recall': 0.39361702127659576}, 'positive': {'f1': 0.8744588744588745, 'precision': 0.831275720164609, 'recall': 0.9223744292237442}}, 'merge_micro_f1': 0.6836518046709129}}
allennlp_callback.py-39-2021-03-18 02:38:16,278-epoch: 27 data_type: hard_test result: {'sentiment_acc': 0.6792452830188679, 'category_f1': {'precision': 0.8857142857142857, 'recall': 0.5849056603773585, 'fscore': 0.7045454545454545}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.5454545454545454, 'precision': 1.0, 'recall': 0.375}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.8717948717948718, 'precision': 1.0, 'recall': 0.7727272727272727}, 'price': {'f1': 0.8750000000000001, 'precision': 0.7777777777777778, 'recall': 1.0}, 'service': {'f1': 0.5333333333333333, 'precision': 0.8, 'recall': 0.4}}, 'acsc_metrics': {'ambience': {'acc': 0.75}, 'anecdotes/miscellaneous': {'acc': 0.3333333333333333}, 'food': {'acc': 0.6818181818181818}, 'price': {'acc': 0.7142857142857143}, 'service': {'acc': 0.8}}, 'polarity_metrics': {'negative': {'f1': 0.6842105263157895, 'precision': 0.7222222222222222, 'recall': 0.65}, 'neutral': {'f1': 0.5263157894736842, 'precision': 0.7142857142857143, 'recall': 0.4166666666666667}, 'positive': {'f1': 0.7346938775510204, 'precision': 0.6428571428571429, 'recall': 0.8571428571428571}}, 'merge_micro_f1': 0.5454545454545454}}
allennlp_callback.py-39-2021-03-18 02:43:27,057-epoch: 28 data_type: train result: {'sentiment_acc': 0.994005994005994, 'category_f1': {'precision': 0.9745033112582782, 'recall': 0.98001998001998, 'fscore': 0.9772538602025569}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.9579579579579579, 'precision': 0.9465875370919882, 'recall': 0.9696048632218845}, 'anecdotes/miscellaneous': {'f1': 0.9839400428265526, 'precision': 0.990301724137931, 'recall': 0.9776595744680852}, 'food': {'f1': 0.9826474962816064, 'precision': 0.9782823297137216, 'recall': 0.9870517928286853}, 'price': {'f1': 0.9748549323017408, 'precision': 0.9767441860465116, 'recall': 0.972972972972973}, 'service': {'f1': 0.9675392670157068, 'precision': 0.9545454545454546, 'recall': 0.9808917197452229}}, 'acsc_metrics': {'ambience': {'acc': 0.9969604863221885}, 'anecdotes/miscellaneous': {'acc': 0.9904255319148936}, 'food': {'acc': 0.9940239043824701}, 'price': {'acc': 0.9922779922779923}, 'service': {'acc': 1.0}}, 'polarity_metrics': {'negative': {'f1': 0.992226148409894, 'precision': 0.9985775248933144, 'recall': 0.9859550561797753}, 'neutral': {'f1': 0.9895470383275261, 'precision': 0.9953271028037384, 'recall': 0.9838337182448037}, 'positive': {'f1': 0.9957104557640751, 'precision': 0.9919871794871795, 'recall': 0.9994617868675996}}, 'merge_micro_f1': 0.9716088328075709}}
allennlp_callback.py-39-2021-03-18 02:43:55,174-epoch: 28 data_type: dev result: {'sentiment_acc': 0.7935871743486974, 'category_f1': {'precision': 0.5342960288808665, 'recall': 0.593186372745491, 'fscore': 0.5622032288698955}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.40322580645161293, 'precision': 0.36231884057971014, 'recall': 0.45454545454545453}, 'anecdotes/miscellaneous': {'f1': 0.664451827242525, 'precision': 0.6578947368421053, 'recall': 0.6711409395973155}, 'food': {'f1': 0.6153846153846153, 'precision': 0.5654450261780105, 'recall': 0.675}, 'price': {'f1': 0.3655913978494624, 'precision': 0.3541666666666667, 'recall': 0.37777777777777777}, 'service': {'f1': 0.5, 'precision': 0.48936170212765956, 'recall': 0.5111111111111111}}, 'acsc_metrics': {'ambience': {'acc': 0.8181818181818182}, 'anecdotes/miscellaneous': {'acc': 0.697986577181208}, 'food': {'acc': 0.84375}, 'price': {'acc': 0.8888888888888888}, 'service': {'acc': 0.8}}, 'polarity_metrics': {'negative': {'f1': 0.7537313432835822, 'precision': 0.7163120567375887, 'recall': 0.7952755905511811}, 'neutral': {'f1': 0.5864661654135339, 'precision': 0.5909090909090909, 'recall': 0.582089552238806}, 'positive': {'f1': 0.8576214405360134, 'precision': 0.8767123287671232, 'recall': 0.839344262295082}}, 'merge_micro_f1': 0.4577397910731244}}
allennlp_callback.py-39-2021-03-18 02:44:37,305-epoch: 28 data_type: test result: {'sentiment_acc': 0.802672147995889, 'category_f1': {'precision': 0.8680089485458613, 'recall': 0.7975334018499486, 'fscore': 0.8312801285484734}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7407407407407407, 'precision': 0.7207207207207207, 'recall': 0.7619047619047619}, 'anecdotes/miscellaneous': {'f1': 0.7554479418886199, 'precision': 0.8041237113402062, 'recall': 0.7123287671232876}, 'food': {'f1': 0.8877284595300261, 'precision': 0.9340659340659341, 'recall': 0.845771144278607}, 'price': {'f1': 0.7769784172661871, 'precision': 0.9152542372881356, 'recall': 0.675}, 'service': {'f1': 0.8768768768768769, 'precision': 0.8795180722891566, 'recall': 0.874251497005988}}, 'acsc_metrics': {'ambience': {'acc': 0.8285714285714286}, 'anecdotes/miscellaneous': {'acc': 0.7031963470319634}, 'food': {'acc': 0.818407960199005}, 'price': {'acc': 0.8}, 'service': {'acc': 0.8802395209580839}}, 'polarity_metrics': {'negative': {'f1': 0.7083333333333334, 'precision': 0.7285714285714285, 'recall': 0.6891891891891891}, 'neutral': {'f1': 0.43820224719101125, 'precision': 0.4642857142857143, 'recall': 0.4148936170212766}, 'positive': {'f1': 0.8817365269461078, 'precision': 0.8674521354933726, 'recall': 0.8964992389649924}}, 'merge_micro_f1': 0.6823781467595073}}
allennlp_callback.py-39-2021-03-18 02:44:40,293-epoch: 28 data_type: hard_test result: {'sentiment_acc': 0.6415094339622641, 'category_f1': {'precision': 0.9375, 'recall': 0.5660377358490566, 'fscore': 0.7058823529411765}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.4, 'precision': 1.0, 'recall': 0.25}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.8108108108108109, 'precision': 1.0, 'recall': 0.6818181818181818}, 'price': {'f1': 0.923076923076923, 'precision': 1.0, 'recall': 0.8571428571428571}, 'service': {'f1': 0.7777777777777777, 'precision': 0.875, 'recall': 0.7}}, 'acsc_metrics': {'ambience': {'acc': 0.625}, 'anecdotes/miscellaneous': {'acc': 0.16666666666666666}, 'food': {'acc': 0.5909090909090909}, 'price': {'acc': 0.8571428571428571}, 'service': {'acc': 0.9}}, 'polarity_metrics': {'negative': {'f1': 0.723404255319149, 'precision': 0.6296296296296297, 'recall': 0.85}, 'neutral': {'f1': 0.3157894736842105, 'precision': 0.42857142857142855, 'recall': 0.25}, 'positive': {'f1': 0.7, 'precision': 0.7368421052631579, 'recall': 0.6666666666666666}}, 'merge_micro_f1': 0.5176470588235295}}
allennlp_callback.py-39-2021-03-18 02:49:47,573-epoch: 29 data_type: train result: {'sentiment_acc': 0.9973359973359973, 'category_f1': {'precision': 0.986785596299967, 'recall': 0.9946719946719946, 'fscore': 0.9907131011608624}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.9877300613496933, 'precision': 0.9969040247678018, 'recall': 0.9787234042553191}, 'anecdotes/miscellaneous': {'f1': 0.996282527881041, 'precision': 0.9946977730646872, 'recall': 0.997872340425532}, 'food': {'f1': 0.9886643666830951, 'precision': 0.9785365853658536, 'recall': 0.999003984063745}, 'price': {'f1': 0.9790476190476192, 'precision': 0.9661654135338346, 'recall': 0.9922779922779923}, 'service': {'f1': 0.9925611052072264, 'precision': 0.9936170212765958, 'recall': 0.9915074309978769}}, 'acsc_metrics': {'ambience': {'acc': 1.0}, 'anecdotes/miscellaneous': {'acc': 0.9936170212765958}, 'food': {'acc': 0.999003984063745}, 'price': {'acc': 0.9961389961389961}, 'service': {'acc': 1.0}}, 'polarity_metrics': {'negative': {'f1': 0.995774647887324, 'precision': 0.998587570621469, 'recall': 0.9929775280898876}, 'neutral': {'f1': 0.9953917050691244, 'precision': 0.993103448275862, 'recall': 0.9976905311778291}, 'positive': {'f1': 0.9983862291554598, 'precision': 0.9978494623655914, 'recall': 0.9989235737351991}}, 'merge_micro_f1': 0.9880597014925374}}
allennlp_callback.py-39-2021-03-18 02:50:15,493-epoch: 29 data_type: dev result: {'sentiment_acc': 0.7755511022044088, 'category_f1': {'precision': 0.5258467023172906, 'recall': 0.591182364729459, 'fscore': 0.5566037735849056}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.38655462184873945, 'precision': 0.359375, 'recall': 0.41818181818181815}, 'anecdotes/miscellaneous': {'f1': 0.7100977198697068, 'precision': 0.689873417721519, 'recall': 0.7315436241610739}, 'food': {'f1': 0.6094182825484764, 'precision': 0.5472636815920398, 'recall': 0.6875}, 'price': {'f1': 0.4077669902912621, 'precision': 0.3620689655172414, 'recall': 0.4666666666666667}, 'service': {'f1': 0.3764705882352941, 'precision': 0.4, 'recall': 0.35555555555555557}}, 'acsc_metrics': {'ambience': {'acc': 0.8181818181818182}, 'anecdotes/miscellaneous': {'acc': 0.6778523489932886}, 'food': {'acc': 0.83125}, 'price': {'acc': 0.8222222222222222}, 'service': {'acc': 0.7888888888888889}}, 'polarity_metrics': {'negative': {'f1': 0.7230769230769232, 'precision': 0.706766917293233, 'recall': 0.7401574803149606}, 'neutral': {'f1': 0.5343511450381679, 'precision': 0.546875, 'recall': 0.5223880597014925}, 'positive': {'f1': 0.8500823723228995, 'precision': 0.8543046357615894, 'recall': 0.8459016393442623}}, 'merge_micro_f1': 0.4377358490566038}}
allennlp_callback.py-39-2021-03-18 02:50:51,611-epoch: 29 data_type: test result: {'sentiment_acc': 0.802672147995889, 'category_f1': {'precision': 0.8701298701298701, 'recall': 0.8263103802672148, 'fscore': 0.8476541908276225}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7751196172248804, 'precision': 0.7788461538461539, 'recall': 0.7714285714285715}, 'anecdotes/miscellaneous': {'f1': 0.7971360381861575, 'precision': 0.835, 'recall': 0.7625570776255708}, 'food': {'f1': 0.9088639200998752, 'precision': 0.9122807017543859, 'recall': 0.9054726368159204}, 'price': {'f1': 0.8137931034482759, 'precision': 0.9076923076923077, 'recall': 0.7375}, 'service': {'f1': 0.8235294117647058, 'precision': 0.8525641025641025, 'recall': 0.7964071856287425}}, 'acsc_metrics': {'ambience': {'acc': 0.819047619047619}, 'anecdotes/miscellaneous': {'acc': 0.7168949771689498}, 'food': {'acc': 0.818407960199005}, 'price': {'acc': 0.8125}, 'service': {'acc': 0.8622754491017964}}, 'polarity_metrics': {'negative': {'f1': 0.7092198581560283, 'precision': 0.746268656716418, 'recall': 0.6756756756756757}, 'neutral': {'f1': 0.4, 'precision': 0.4186046511627907, 'recall': 0.3829787234042553}, 'positive': {'f1': 0.8860759493670884, 'precision': 0.8673469387755102, 'recall': 0.9056316590563166}}, 'merge_micro_f1': 0.694781233526621}}
allennlp_callback.py-39-2021-03-18 02:50:53,454-epoch: 29 data_type: hard_test result: {'sentiment_acc': 0.6037735849056604, 'category_f1': {'precision': 0.9459459459459459, 'recall': 0.660377358490566, 'fscore': 0.7777777777777778}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.6153846153846154, 'precision': 0.8, 'recall': 0.5}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.9268292682926829, 'precision': 1.0, 'recall': 0.8636363636363636}, 'price': {'f1': 0.923076923076923, 'precision': 1.0, 'recall': 0.8571428571428571}, 'service': {'f1': 0.7058823529411764, 'precision': 0.8571428571428571, 'recall': 0.6}}, 'acsc_metrics': {'ambience': {'acc': 0.625}, 'anecdotes/miscellaneous': {'acc': 0.16666666666666666}, 'food': {'acc': 0.5454545454545454}, 'price': {'acc': 0.8571428571428571}, 'service': {'acc': 0.8}}, 'polarity_metrics': {'negative': {'f1': 0.6956521739130435, 'precision': 0.6153846153846154, 'recall': 0.8}, 'neutral': {'f1': 0.3157894736842105, 'precision': 0.42857142857142855, 'recall': 0.25}, 'positive': {'f1': 0.6341463414634146, 'precision': 0.65, 'recall': 0.6190476190476191}}, 'merge_micro_f1': 0.5555555555555556}}
allennlp_callback.py-39-2021-03-18 02:56:04,067-epoch: 30 data_type: train result: {'sentiment_acc': 0.9976689976689976, 'category_f1': {'precision': 0.9930509596293845, 'recall': 0.9993339993339994, 'fscore': 0.996182572614108}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.996969696969697, 'precision': 0.9939577039274925, 'recall': 1.0}, 'anecdotes/miscellaneous': {'f1': 0.9984067976633033, 'precision': 0.9968186638388123, 'recall': 1.0}, 'food': {'f1': 0.9955379276152703, 'precision': 0.9911154985192497, 'recall': 1.0}, 'price': {'f1': 0.9885496183206107, 'precision': 0.9773584905660377, 'recall': 1.0}, 'service': {'f1': 0.9968119022316685, 'precision': 0.997872340425532, 'recall': 0.9957537154989384}}, 'acsc_metrics': {'ambience': {'acc': 1.0}, 'anecdotes/miscellaneous': {'acc': 0.9968085106382979}, 'food': {'acc': 0.9960159362549801}, 'price': {'acc': 1.0}, 'service': {'acc': 1.0}}, 'polarity_metrics': {'negative': {'f1': 0.9971870604781997, 'precision': 0.9985915492957746, 'recall': 0.9957865168539326}, 'neutral': {'f1': 0.9953596287703016, 'precision': 1.0, 'recall': 0.9907621247113164}, 'positive': {'f1': 0.9983879634605051, 'precision': 0.9967811158798283, 'recall': 1.0}}, 'merge_micro_f1': 0.9938589211618256}}
allennlp_callback.py-39-2021-03-18 02:56:28,806-epoch: 30 data_type: dev result: {'sentiment_acc': 0.7775551102204409, 'category_f1': {'precision': 0.5207956600361664, 'recall': 0.5771543086172345, 'fscore': 0.5475285171102662}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.4, 'precision': 0.35714285714285715, 'recall': 0.45454545454545453}, 'anecdotes/miscellaneous': {'f1': 0.7142857142857143, 'precision': 0.6918238993710691, 'recall': 0.738255033557047}, 'food': {'f1': 0.5977011494252874, 'precision': 0.5531914893617021, 'recall': 0.65}, 'price': {'f1': 0.39215686274509803, 'precision': 0.3508771929824561, 'recall': 0.4444444444444444}, 'service': {'f1': 0.3431952662721894, 'precision': 0.3670886075949367, 'recall': 0.32222222222222224}}, 'acsc_metrics': {'ambience': {'acc': 0.8181818181818182}, 'anecdotes/miscellaneous': {'acc': 0.6711409395973155}, 'food': {'acc': 0.85}, 'price': {'acc': 0.8}, 'service': {'acc': 0.7888888888888889}}, 'polarity_metrics': {'negative': {'f1': 0.7181467181467182, 'precision': 0.7045454545454546, 'recall': 0.7322834645669292}, 'neutral': {'f1': 0.549618320610687, 'precision': 0.5625, 'recall': 0.5373134328358209}, 'positive': {'f1': 0.8519736842105262, 'precision': 0.8547854785478548, 'recall': 0.8491803278688524}}, 'merge_micro_f1': 0.4258555133079848}}
allennlp_callback.py-39-2021-03-18 02:57:06,790-epoch: 30 data_type: test result: {'sentiment_acc': 0.8129496402877698, 'category_f1': {'precision': 0.8627027027027027, 'recall': 0.8201438848920863, 'fscore': 0.8408851422550053}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7706422018348624, 'precision': 0.7433628318584071, 'recall': 0.8}, 'anecdotes/miscellaneous': {'f1': 0.7801418439716311, 'precision': 0.8088235294117647, 'recall': 0.7534246575342466}, 'food': {'f1': 0.8962868117797695, 'precision': 0.9234828496042217, 'recall': 0.8706467661691543}, 'price': {'f1': 0.8299319727891157, 'precision': 0.9104477611940298, 'recall': 0.7625}, 'service': {'f1': 0.8389057750759878, 'precision': 0.8518518518518519, 'recall': 0.8263473053892215}}, 'acsc_metrics': {'ambience': {'acc': 0.8095238095238095}, 'anecdotes/miscellaneous': {'acc': 0.7168949771689498}, 'food': {'acc': 0.835820895522388}, 'price': {'acc': 0.8125}, 'service': {'acc': 0.8862275449101796}}, 'polarity_metrics': {'negative': {'f1': 0.7234042553191488, 'precision': 0.7611940298507462, 'recall': 0.6891891891891891}, 'neutral': {'f1': 0.4166666666666667, 'precision': 0.47297297297297297, 'recall': 0.3723404255319149}, 'positive': {'f1': 0.8900369003690036, 'precision': 0.8638968481375359, 'recall': 0.9178082191780822}}, 'merge_micro_f1': 0.702845100105374}}
allennlp_callback.py-39-2021-03-18 02:57:08,886-epoch: 30 data_type: hard_test result: {'sentiment_acc': 0.6415094339622641, 'category_f1': {'precision': 0.9090909090909091, 'recall': 0.5660377358490566, 'fscore': 0.6976744186046512}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.6153846153846154, 'precision': 0.8, 'recall': 0.5}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.7777777777777778, 'precision': 1.0, 'recall': 0.6363636363636364}, 'price': {'f1': 0.8571428571428571, 'precision': 0.8571428571428571, 'recall': 0.8571428571428571}, 'service': {'f1': 0.7058823529411764, 'precision': 0.8571428571428571, 'recall': 0.6}}, 'acsc_metrics': {'ambience': {'acc': 0.625}, 'anecdotes/miscellaneous': {'acc': 0.16666666666666666}, 'food': {'acc': 0.5909090909090909}, 'price': {'acc': 0.8571428571428571}, 'service': {'acc': 0.9}}, 'polarity_metrics': {'negative': {'f1': 0.7391304347826088, 'precision': 0.6538461538461539, 'recall': 0.85}, 'neutral': {'f1': 0.23529411764705882, 'precision': 0.4, 'recall': 0.16666666666666666}, 'positive': {'f1': 0.6976744186046512, 'precision': 0.6818181818181818, 'recall': 0.7142857142857143}}, 'merge_micro_f1': 0.5581395348837209}}
allennlp_callback.py-39-2021-03-18 03:02:24,634-epoch: 31 data_type: train result: {'sentiment_acc': 0.999000999000999, 'category_f1': {'precision': 0.9966799468791501, 'recall': 0.9996669996669997, 'fscore': 0.998171238570241}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.9984825493171471, 'precision': 0.996969696969697, 'recall': 1.0}, 'anecdotes/miscellaneous': {'f1': 1.0, 'precision': 1.0, 'recall': 1.0}, 'food': {'f1': 0.9965226030799802, 'precision': 0.9940535183349851, 'recall': 0.999003984063745}, 'price': {'f1': 0.9942418426103647, 'precision': 0.9885496183206107, 'recall': 1.0}, 'service': {'f1': 1.0, 'precision': 1.0, 'recall': 1.0}}, 'acsc_metrics': {'ambience': {'acc': 1.0}, 'anecdotes/miscellaneous': {'acc': 0.997872340425532}, 'food': {'acc': 0.999003984063745}, 'price': {'acc': 1.0}, 'service': {'acc': 1.0}}, 'polarity_metrics': {'negative': {'f1': 0.9985935302390998, 'precision': 1.0, 'recall': 0.9971910112359551}, 'neutral': {'f1': 0.9976958525345622, 'precision': 0.9954022988505747, 'recall': 1.0}, 'positive': {'f1': 0.9994617868675996, 'precision': 0.9994617868675996, 'recall': 0.9994617868675996}}, 'merge_micro_f1': 0.9971737323358271}}
allennlp_callback.py-39-2021-03-18 03:02:53,383-epoch: 31 data_type: dev result: {'sentiment_acc': 0.7715430861723447, 'category_f1': {'precision': 0.5109090909090909, 'recall': 0.56312625250501, 'fscore': 0.5357483317445185}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.3739837398373984, 'precision': 0.3382352941176471, 'recall': 0.41818181818181815}, 'anecdotes/miscellaneous': {'f1': 0.6903225806451613, 'precision': 0.6645962732919255, 'recall': 0.7181208053691275}, 'food': {'f1': 0.5930232558139535, 'precision': 0.5543478260869565, 'recall': 0.6375}, 'price': {'f1': 0.3762376237623763, 'precision': 0.3392857142857143, 'recall': 0.4222222222222222}, 'service': {'f1': 0.3508771929824561, 'precision': 0.37037037037037035, 'recall': 0.3333333333333333}}, 'acsc_metrics': {'ambience': {'acc': 0.8181818181818182}, 'anecdotes/miscellaneous': {'acc': 0.6510067114093959}, 'food': {'acc': 0.85}, 'price': {'acc': 0.8}, 'service': {'acc': 0.7888888888888889}}, 'polarity_metrics': {'negative': {'f1': 0.7209302325581396, 'precision': 0.7099236641221374, 'recall': 0.7322834645669292}, 'neutral': {'f1': 0.5255474452554744, 'precision': 0.5142857142857142, 'recall': 0.5373134328358209}, 'positive': {'f1': 0.8490878938640133, 'precision': 0.8590604026845637, 'recall': 0.839344262295082}}, 'merge_micro_f1': 0.42135367016205916}}
allennlp_callback.py-39-2021-03-18 03:03:31,002-epoch: 31 data_type: test result: {'sentiment_acc': 0.7995889003083247, 'category_f1': {'precision': 0.8727876106194691, 'recall': 0.8108941418293937, 'fscore': 0.8407032498668088}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7641509433962264, 'precision': 0.7570093457943925, 'recall': 0.7714285714285715}, 'anecdotes/miscellaneous': {'f1': 0.7962085308056872, 'precision': 0.8275862068965517, 'recall': 0.7671232876712328}, 'food': {'f1': 0.8894668400520156, 'precision': 0.9318801089918256, 'recall': 0.8507462686567164}, 'price': {'f1': 0.84, 'precision': 0.9, 'recall': 0.7875}, 'service': {'f1': 0.8333333333333334, 'precision': 0.8598726114649682, 'recall': 0.8083832335329342}}, 'acsc_metrics': {'ambience': {'acc': 0.780952380952381}, 'anecdotes/miscellaneous': {'acc': 0.7123287671232876}, 'food': {'acc': 0.8233830845771144}, 'price': {'acc': 0.8}, 'service': {'acc': 0.8682634730538922}}, 'polarity_metrics': {'negative': {'f1': 0.7096774193548386, 'precision': 0.7264150943396226, 'recall': 0.6936936936936937}, 'neutral': {'f1': 0.4171122994652407, 'precision': 0.41935483870967744, 'recall': 0.4148936170212766}, 'positive': {'f1': 0.8830188679245283, 'precision': 0.875748502994012, 'recall': 0.8904109589041096}}, 'merge_micro_f1': 0.6947256259989345}}
allennlp_callback.py-39-2021-03-18 03:03:32,600-epoch: 31 data_type: hard_test result: {'sentiment_acc': 0.6037735849056604, 'category_f1': {'precision': 0.9117647058823529, 'recall': 0.5849056603773585, 'fscore': 0.7126436781609194}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.6153846153846154, 'precision': 0.8, 'recall': 0.5}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.8108108108108109, 'precision': 1.0, 'recall': 0.6818181818181818}, 'price': {'f1': 0.8571428571428571, 'precision': 0.8571428571428571, 'recall': 0.8571428571428571}, 'service': {'f1': 0.7058823529411764, 'precision': 0.8571428571428571, 'recall': 0.6}}, 'acsc_metrics': {'ambience': {'acc': 0.625}, 'anecdotes/miscellaneous': {'acc': 0.16666666666666666}, 'food': {'acc': 0.5454545454545454}, 'price': {'acc': 0.8571428571428571}, 'service': {'acc': 0.8}}, 'polarity_metrics': {'negative': {'f1': 0.6808510638297872, 'precision': 0.5925925925925926, 'recall': 0.8}, 'neutral': {'f1': 0.3157894736842105, 'precision': 0.42857142857142855, 'recall': 0.25}, 'positive': {'f1': 0.6500000000000001, 'precision': 0.6842105263157895, 'recall': 0.6190476190476191}}, 'merge_micro_f1': 0.5287356321839081}}
allennlp_callback.py-39-2021-03-18 03:08:39,239-epoch: 32 data_type: train result: {'sentiment_acc': 0.999000999000999, 'category_f1': {'precision': 0.9986684420772304, 'recall': 0.999000999000999, 'fscore': 0.998834692858332}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 1.0, 'precision': 1.0, 'recall': 1.0}, 'anecdotes/miscellaneous': {'f1': 1.0, 'precision': 1.0, 'recall': 1.0}, 'food': {'f1': 0.9970149253731343, 'precision': 0.9960238568588469, 'recall': 0.99800796812749}, 'price': {'f1': 1.0, 'precision': 1.0, 'recall': 1.0}, 'service': {'f1': 0.9989373007438895, 'precision': 1.0, 'recall': 0.9978768577494692}}, 'acsc_metrics': {'ambience': {'acc': 1.0}, 'anecdotes/miscellaneous': {'acc': 0.9989361702127659}, 'food': {'acc': 0.99800796812749}, 'price': {'acc': 1.0}, 'service': {'acc': 1.0}}, 'polarity_metrics': {'negative': {'f1': 0.999298245614035, 'precision': 0.9985974754558204, 'recall': 1.0}, 'neutral': {'f1': 0.996523754345307, 'precision': 1.0, 'recall': 0.9930715935334873}, 'positive': {'f1': 0.9994620763851533, 'precision': 0.9989247311827957, 'recall': 1.0}}, 'merge_micro_f1': 0.9978358581654736}}
allennlp_callback.py-39-2021-03-18 03:09:04,085-epoch: 32 data_type: dev result: {'sentiment_acc': 0.7775551102204409, 'category_f1': {'precision': 0.5083798882681564, 'recall': 0.5470941883767535, 'fscore': 0.5270270270270271}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.3666666666666667, 'precision': 0.3384615384615385, 'recall': 0.4}, 'anecdotes/miscellaneous': {'f1': 0.7012987012987013, 'precision': 0.6792452830188679, 'recall': 0.7248322147651006}, 'food': {'f1': 0.5747800586510264, 'precision': 0.5414364640883977, 'recall': 0.6125}, 'price': {'f1': 0.3092783505154639, 'precision': 0.28846153846153844, 'recall': 0.3333333333333333}, 'service': {'f1': 0.35294117647058826, 'precision': 0.375, 'recall': 0.3333333333333333}}, 'acsc_metrics': {'ambience': {'acc': 0.8181818181818182}, 'anecdotes/miscellaneous': {'acc': 0.6711409395973155}, 'food': {'acc': 0.85}, 'price': {'acc': 0.8}, 'service': {'acc': 0.7888888888888889}}, 'polarity_metrics': {'negative': {'f1': 0.7300380228136881, 'precision': 0.7058823529411765, 'recall': 0.7559055118110236}, 'neutral': {'f1': 0.5426356589147286, 'precision': 0.5645161290322581, 'recall': 0.5223880597014925}, 'positive': {'f1': 0.8481848184818482, 'precision': 0.8538205980066446, 'recall': 0.8426229508196721}}, 'merge_micro_f1': 0.4092664092664093}}
allennlp_callback.py-39-2021-03-18 03:09:45,592-epoch: 32 data_type: test result: {'sentiment_acc': 0.7985611510791367, 'category_f1': {'precision': 0.8734739178690344, 'recall': 0.8088386433710175, 'fscore': 0.83991462113127}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7632850241545894, 'precision': 0.7745098039215687, 'recall': 0.7523809523809524}, 'anecdotes/miscellaneous': {'f1': 0.7904761904761904, 'precision': 0.8258706467661692, 'recall': 0.7579908675799086}, 'food': {'f1': 0.8985879332477535, 'precision': 0.9283819628647215, 'recall': 0.8706467661691543}, 'price': {'f1': 0.7746478873239436, 'precision': 0.8870967741935484, 'recall': 0.6875}, 'service': {'f1': 0.8404907975460123, 'precision': 0.8616352201257862, 'recall': 0.8203592814371258}}, 'acsc_metrics': {'ambience': {'acc': 0.7904761904761904}, 'anecdotes/miscellaneous': {'acc': 0.7031963470319634}, 'food': {'acc': 0.8208955223880597}, 'price': {'acc': 0.8125}, 'service': {'acc': 0.8682634730538922}}, 'polarity_metrics': {'negative': {'f1': 0.7037037037037036, 'precision': 0.7238095238095238, 'recall': 0.6846846846846847}, 'neutral': {'f1': 0.39766081871345027, 'precision': 0.44155844155844154, 'recall': 0.3617021276595745}, 'positive': {'f1': 0.8801191362620997, 'precision': 0.8615160349854227, 'recall': 0.8995433789954338}}, 'merge_micro_f1': 0.6862326574172892}}
allennlp_callback.py-39-2021-03-18 03:09:47,371-epoch: 32 data_type: hard_test result: {'sentiment_acc': 0.6037735849056604, 'category_f1': {'precision': 0.9393939393939394, 'recall': 0.5849056603773585, 'fscore': 0.7209302325581395}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.5454545454545454, 'precision': 1.0, 'recall': 0.375}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.8421052631578948, 'precision': 1.0, 'recall': 0.7272727272727273}, 'price': {'f1': 0.8571428571428571, 'precision': 0.8571428571428571, 'recall': 0.8571428571428571}, 'service': {'f1': 0.7058823529411764, 'precision': 0.8571428571428571, 'recall': 0.6}}, 'acsc_metrics': {'ambience': {'acc': 0.625}, 'anecdotes/miscellaneous': {'acc': 0.16666666666666666}, 'food': {'acc': 0.5454545454545454}, 'price': {'acc': 0.8571428571428571}, 'service': {'acc': 0.8}}, 'polarity_metrics': {'negative': {'f1': 0.6956521739130435, 'precision': 0.6153846153846154, 'recall': 0.8}, 'neutral': {'f1': 0.23529411764705882, 'precision': 0.4, 'recall': 0.16666666666666666}, 'positive': {'f1': 0.6511627906976744, 'precision': 0.6363636363636364, 'recall': 0.6666666666666666}}, 'merge_micro_f1': 0.5348837209302327}}
allennlp_callback.py-39-2021-03-18 03:15:01,176-epoch: 33 data_type: train result: {'sentiment_acc': 0.9993339993339994, 'category_f1': {'precision': 0.9983377659574468, 'recall': 1.0, 'fscore': 0.9991681916486441}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 1.0, 'precision': 1.0, 'recall': 1.0}, 'anecdotes/miscellaneous': {'f1': 1.0, 'precision': 1.0, 'recall': 1.0}, 'food': {'f1': 0.9975161450571287, 'precision': 0.9950445986124876, 'recall': 1.0}, 'price': {'f1': 1.0, 'precision': 1.0, 'recall': 1.0}, 'service': {'f1': 1.0, 'precision': 1.0, 'recall': 1.0}}, 'acsc_metrics': {'ambience': {'acc': 1.0}, 'anecdotes/miscellaneous': {'acc': 0.9989361702127659}, 'food': {'acc': 0.999003984063745}, 'price': {'acc': 1.0}, 'service': {'acc': 1.0}}, 'polarity_metrics': {'negative': {'f1': 0.9985955056179775, 'precision': 0.9985955056179775, 'recall': 0.9985955056179775}, 'neutral': {'f1': 0.9988439306358381, 'precision': 1.0, 'recall': 0.9976905311778291}, 'positive': {'f1': 0.9997309658326607, 'precision': 0.9994620763851533, 'recall': 1.0}}, 'merge_micro_f1': 0.9985027449675595}}
allennlp_callback.py-39-2021-03-18 03:15:29,606-epoch: 33 data_type: dev result: {'sentiment_acc': 0.7675350701402806, 'category_f1': {'precision': 0.5102803738317757, 'recall': 0.5470941883767535, 'fscore': 0.528046421663443}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.3620689655172414, 'precision': 0.3442622950819672, 'recall': 0.38181818181818183}, 'anecdotes/miscellaneous': {'f1': 0.6990291262135923, 'precision': 0.675, 'recall': 0.7248322147651006}, 'food': {'f1': 0.5855072463768116, 'precision': 0.5459459459459459, 'recall': 0.63125}, 'price': {'f1': 0.2947368421052632, 'precision': 0.28, 'recall': 0.3111111111111111}, 'service': {'f1': 0.3431952662721894, 'precision': 0.3670886075949367, 'recall': 0.32222222222222224}}, 'acsc_metrics': {'ambience': {'acc': 0.8181818181818182}, 'anecdotes/miscellaneous': {'acc': 0.6442953020134228}, 'food': {'acc': 0.84375}, 'price': {'acc': 0.8}, 'service': {'acc': 0.7888888888888889}}, 'polarity_metrics': {'negative': {'f1': 0.7109375, 'precision': 0.7054263565891473, 'recall': 0.7165354330708661}, 'neutral': {'f1': 0.5179856115107914, 'precision': 0.5, 'recall': 0.5373134328358209}, 'positive': {'f1': 0.8490878938640133, 'precision': 0.8590604026845637, 'recall': 0.839344262295082}}, 'merge_micro_f1': 0.4081237911025145}}
allennlp_callback.py-39-2021-03-18 03:16:11,406-epoch: 33 data_type: test result: {'sentiment_acc': 0.7965056526207606, 'category_f1': {'precision': 0.8738839285714286, 'recall': 0.8047276464542652, 'fscore': 0.8378812199036918}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.7450980392156863, 'precision': 0.7676767676767676, 'recall': 0.7238095238095238}, 'anecdotes/miscellaneous': {'f1': 0.7838479809976246, 'precision': 0.8168316831683168, 'recall': 0.7534246575342466}, 'food': {'f1': 0.9083969465648856, 'precision': 0.9296875, 'recall': 0.8880597014925373}, 'price': {'f1': 0.7801418439716312, 'precision': 0.9016393442622951, 'recall': 0.6875}, 'service': {'f1': 0.8201892744479495, 'precision': 0.8666666666666667, 'recall': 0.7784431137724551}}, 'acsc_metrics': {'ambience': {'acc': 0.7904761904761904}, 'anecdotes/miscellaneous': {'acc': 0.7031963470319634}, 'food': {'acc': 0.818407960199005}, 'price': {'acc': 0.8125}, 'service': {'acc': 0.8622754491017964}}, 'polarity_metrics': {'negative': {'f1': 0.705607476635514, 'precision': 0.7330097087378641, 'recall': 0.6801801801801802}, 'neutral': {'f1': 0.39999999999999997, 'precision': 0.3958333333333333, 'recall': 0.40425531914893614}, 'positive': {'f1': 0.8825301204819277, 'precision': 0.8733233979135618, 'recall': 0.8919330289193302}}, 'merge_micro_f1': 0.6773675762439807}}
allennlp_callback.py-39-2021-03-18 03:16:13,183-epoch: 33 data_type: hard_test result: {'sentiment_acc': 0.6226415094339622, 'category_f1': {'precision': 0.96875, 'recall': 0.5849056603773585, 'fscore': 0.7294117647058823}, 'other_metrics': {'acd_metrics': {'ambience': {'f1': 0.4, 'precision': 1.0, 'recall': 0.25}, 'anecdotes/miscellaneous': {'f1': 0.0, 'precision': 0.0, 'recall': 0.0}, 'food': {'f1': 0.8717948717948718, 'precision': 1.0, 'recall': 0.7727272727272727}, 'price': {'f1': 0.923076923076923, 'precision': 1.0, 'recall': 0.8571428571428571}, 'service': {'f1': 0.7058823529411764, 'precision': 0.8571428571428571, 'recall': 0.6}}, 'acsc_metrics': {'ambience': {'acc': 0.625}, 'anecdotes/miscellaneous': {'acc': 0.16666666666666666}, 'food': {'acc': 0.5909090909090909}, 'price': {'acc': 0.8571428571428571}, 'service': {'acc': 0.8}}, 'polarity_metrics': {'negative': {'f1': 0.6956521739130435, 'precision': 0.6153846153846154, 'recall': 0.8}, 'neutral': {'f1': 0.3157894736842105, 'precision': 0.42857142857142855, 'recall': 0.25}, 'positive': {'f1': 0.6829268292682926, 'precision': 0.7, 'recall': 0.6666666666666666}}, 'merge_micro_f1': 0.5176470588235295}}
acd_and_sc_train_templates_pytorch.py-602-2021-03-18 03:19:19,942-metrics: {'best_epoch': 24, 'peak_cpu_memory_MB': 4519.444, 'peak_gpu_0_memory_MB': 0, 'training_duration': '2:54:57.497790', 'training_start_epoch': 0, 'training_epochs': 33, 'epoch': 33, 'training_accuracy': 0.9445828144458281, 'training_category_f1': 1.0, 'training_loss': 0.0359014063491486, 'training_cpu_memory_MB': 4436.424, 'training_gpu_0_memory_MB': 0, 'validation_loss': 0, 'validation_accuracy': 0.7675350701402806, 'validation_category_f1': 0.528046421663443, 'validation_merge_micro_f1': 0.4081237911025145, 'best_validation_loss': 0, 'best_validation_accuracy': 0.7935871743486974, 'best_validation_category_f1': 0.532304725168756, 'best_validation_merge_micro_f1': 0.42430086788813887}
acd_and_sc_train_templates_pytorch.py-88-2021-03-18 14:53:56,223-len_count_list: [[2, 16], [3, 37], [4, 75], [5, 119], [6, 172], [7, 167], [8, 178], [9, 192], [10, 207], [11, 218], [12, 205], [13, 203], [14, 191], [15, 170], [16, 163], [17, 169], [18, 136], [19, 149], [20, 115], [21, 94], [22, 99], [23, 86], [24, 79], [25, 74], [26, 67], [27, 57], [28, 48], [29, 35], [30, 47], [31, 37], [32, 22], [33, 23], [34, 25], [35, 11], [36, 19], [37, 11], [38, 7], [39, 9], [40, 10], [41, 6], [42, 8], [43, 5], [44, 5], [45, 3], [46, 2], [47, 6], [48, 3], [49, 4], [50, 3], [52, 1], [53, 1], [54, 1], [56, 1], [57, 3], [60, 1], [61, 1], [66, 1], [69, 1], [70, 1], [79, 1]]
acd_and_sc_train_templates_pytorch.py-191-2021-03-18 14:53:56,304-n_trainable_params: 3160592, n_nontrainable_params: 1507800
acd_and_sc_train_templates_pytorch.py-192-2021-03-18 14:53:56,304-> training arguments:
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,304->>> current_dataset: SemEval-2014-Task-4-REST-DevSplits
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,304->>> hard_test: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,304->>> task_name: acd_and_sc
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,304->>> data_type: constituency
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,304->>> model_name: scam
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,305->>> timestamp: 1571400646
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,305->>> train: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,305->>> evaluate: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,305->>> error_analysis: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,305->>> predict: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,305->>> epochs: 100
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,305->>> batch_size: 32
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,305->>> patience: 10
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,305->>> visualize_attention: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,305->>> embedding_filepath: glove.840B.300d.txt
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,305->>> embed_size: 300
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,305->>> seed: 776
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,305->>> repeat: 1
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,306->>> device: cpu
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,306->>> gpu_id: 0
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,306->>> lstm_layer_category_classifier: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,306->>> position: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,306->>> aspect_position: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,306->>> position_embeddings_dim: 64
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,306->>> only_acd: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,306->>> only_sc: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,306->>> debug: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,306->>> early_stopping_by_batch: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,306->>> share_sentiment_classifier: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,306->>> attention_lamda: 1
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,307->>> sparse_reg: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,307->>> orthogonal_reg: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,307->>> early_stopping: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,307->>> bert: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,307->>> only_bert: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,307->>> concat_cls_vector: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,307->>> pair: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,307->>> fixed: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,307->>> max_len: 128
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,307->>> lstm_layer_num_in_bert: 0
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,307->>> dropout_in_bert: 0.5
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,307->>> learning_rate_in_bert: 2e-05
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,307->>> l2_in_bert: 1e-05
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,307->>> acd_init_weight: 1
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,308->>> bilstm_hidden_size_in_bert: 0
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,308->>> dropout_after_cls: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,308->>> acd_sc_mode: multi-multi
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,308->>> joint_type: joint
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,308->>> acd_warmup: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,308->>> acd_warmup_epochs: 100
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,308->>> acd_warmup_patience: 10
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,308->>> pipeline: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,308->>> pipeline_with_acd: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,308->>> lstm_or_fc_after_embedding_layer: lstm
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,308->>> lstm_layer_num_in_lstm: 3
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,308->>> sentence_encoder_for_sentiment: bilstm
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,308->>> bert_file_path: bert-base-uncased.tar.gz
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,309->>> bert_vocab_file_path: Dvocab.txt
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,309->>> savefig_dir: 
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,309->>> frozen_all_acsc_parameter_while_pretrain_acd: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,309->>> interactive_loss_lamda: 1
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,309->>> attention_warmup_init: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,309->>> attention_warmup: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,309->>> acd_encoder_mode_for_sentiment_attention_warmup: same
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,309->>> gnn_for_sentiment_attention_warmup: gat
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,309->>> acd_sc_encoder_mode: same
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,309->>> acd_encoder_mode: mixed
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,309->>> sentiment_encoder_with_own_gnn: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,309->>> aspect_graph: gat
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,309->>> sentiment_graph: gat
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,309->>> aspect_graph_with_dotted_line: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,310->>> constituency_tree: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,310->>> performance_of_different_lengths: 30,40,50,60,100000
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,310->>> gat_visualization: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,310->>> model_name_complete: model_name.scam.repeat.1
acd_and_sc_train_templates_pytorch.py-580-2021-03-18 14:53:56,312-validation_metric: +accuracy
acd_and_sc_train_templates_pytorch.py-191-2021-03-18 14:53:56,316-n_trainable_params: 3160592, n_nontrainable_params: 1507800
acd_and_sc_train_templates_pytorch.py-192-2021-03-18 14:53:56,316-> training arguments:
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,316->>> current_dataset: SemEval-2014-Task-4-REST-DevSplits
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,316->>> hard_test: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,316->>> task_name: acd_and_sc
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,316->>> data_type: constituency
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,317->>> model_name: scam
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,317->>> timestamp: 1571400646
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,317->>> train: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,317->>> evaluate: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,317->>> error_analysis: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,317->>> predict: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,317->>> epochs: 100
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,317->>> batch_size: 32
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,317->>> patience: 10
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,317->>> visualize_attention: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,317->>> embedding_filepath: glove.840B.300d.txt
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,317->>> embed_size: 300
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,317->>> seed: 776
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,317->>> repeat: 1
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,318->>> device: cpu
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,318->>> gpu_id: 0
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,318->>> lstm_layer_category_classifier: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,318->>> position: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,318->>> aspect_position: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,318->>> position_embeddings_dim: 64
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,318->>> only_acd: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,318->>> only_sc: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,318->>> debug: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,318->>> early_stopping_by_batch: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,318->>> share_sentiment_classifier: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,318->>> attention_lamda: 1
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,318->>> sparse_reg: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,318->>> orthogonal_reg: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,319->>> early_stopping: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,319->>> bert: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,319->>> only_bert: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,319->>> concat_cls_vector: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,319->>> pair: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,319->>> fixed: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,319->>> max_len: 128
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,319->>> lstm_layer_num_in_bert: 0
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,319->>> dropout_in_bert: 0.5
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,319->>> learning_rate_in_bert: 2e-05
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,319->>> l2_in_bert: 1e-05
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,319->>> acd_init_weight: 1
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,319->>> bilstm_hidden_size_in_bert: 0
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,320->>> dropout_after_cls: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,320->>> acd_sc_mode: multi-multi
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,320->>> joint_type: joint
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,320->>> acd_warmup: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,320->>> acd_warmup_epochs: 100
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,320->>> acd_warmup_patience: 10
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,320->>> pipeline: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,320->>> pipeline_with_acd: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,320->>> lstm_or_fc_after_embedding_layer: lstm
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,320->>> lstm_layer_num_in_lstm: 3
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,320->>> sentence_encoder_for_sentiment: bilstm
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,320->>> bert_file_path: bert-base-uncased.tar.gz
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,320->>> bert_vocab_file_path: Dvocab.txt
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,320->>> savefig_dir: 
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,320->>> frozen_all_acsc_parameter_while_pretrain_acd: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,321->>> interactive_loss_lamda: 1
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,321->>> attention_warmup_init: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,321->>> attention_warmup: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,321->>> acd_encoder_mode_for_sentiment_attention_warmup: same
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,321->>> gnn_for_sentiment_attention_warmup: gat
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,321->>> acd_sc_encoder_mode: same
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,321->>> acd_encoder_mode: mixed
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,321->>> sentiment_encoder_with_own_gnn: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,321->>> aspect_graph: gat
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,321->>> sentiment_graph: gat
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,321->>> aspect_graph_with_dotted_line: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,321->>> constituency_tree: True
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,321->>> performance_of_different_lengths: 30,40,50,60,100000
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,321->>> gat_visualization: False
acd_and_sc_train_templates_pytorch.py-194-2021-03-18 14:53:56,322->>> model_name_complete: model_name.scam.repeat.1
